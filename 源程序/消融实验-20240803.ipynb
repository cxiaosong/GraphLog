{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31b118d",
   "metadata": {},
   "source": [
    "# 准备参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089c11c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATcAAAD8CAYAAAASeuPxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA9LElEQVR4nO2dd3gc1b33P7+ZbVpJVnOTbdlykW252xjjgCFgumkhoTnkAo4TJwQIBEhCKsmb5N4kN5DkJhBKqIFQQgmQEIqBgAlgY2zj3pvcLatrtdrVzHn/2F15Lavsqm3R+ejZRzNnzsz8zplzvnP6iFIKjUajSTeMRBug0Wg0PYEWN41Gk5ZocdNoNGmJFjeNRpOWaHHTaDRpiRY3jUaTlvSYuInIeSKySUS2isgdPXUfjUajaQ3piXFuImICm4GzgT3Ax8B8pdT6br+ZRqPRtEJPldxmAVuVUtuVUgHgaeCSHrqXRqPRHIejh647FCiL2t8DnNSW5/75pioucvaQKelBUFnU2G6qmjLwNzmxmwykScDuRSMElENhOG3cjiZyHA3kGn6cYvbI7RqVRZWdQU0wg4BlYgdMxAJ6c1KNEQ6zw8brDJLj8NHPCOLQzdW9zs6yIOUVlsTqv6fErUNEZBGwCGD4UAfLXi9KlClJz+M1/fntprPwNJl8pqCC2fk7mO7dyQxXOQNNb6/ZUWP7+TSQxTt1paysKmJHRT4YNl8uWcLCnN3dJnKNKshdRybx9PYZ9FPCif0PMbnfPs7KXssEp59+hgcAG4VBzGm9UxyyfHzcOJBl9aNZeqSYvZU5mJ4Ad45/mQu8/h69t+ZYZp1b1rGnKHpK3PYC0Wo1LOzWjFLqAeABgJlTPce9i4PKYlmj8D+7L2DdlmG494dMVQLSy9NhA3k2/UdVcOe4Vzgroxa39E4p01I2d1eW8ODaU7hp8rtcmr2OQWZGlI8M7HAxxkCaM7vdTUWbltfKMtyc4glyimc1Rv81HLJ8PFc7kXs3n8bBkZ/yw/5rMaVrJZpGFeTGPaez/EARN497h89nbSfLcDfbA8cLW3eHNzrc/c0MLvDWcb53FQxYxe6mBp6pPoFbll1F/cwXuSKrulvu3VWCymJ1wOLOXZcck18SQSS/fLfkdS7MPNJr+aUlPdWh4CDUoXAmIVH7GPiiUmpda/5nTvWoSMktqCze87v48ZZL2LezPxPHl7FgyH84y3uw2+2MlbUBN48ePpXFa0txZzdyy6S3u7Wk0hqNKsgPD87irT1jeXnaQwww3RgtqkJ2uE4acW+531VsbAyMNq8bcd/T1Mjlny5k3vD1/GjAik4nZp8dYNHuc9hdm8dzE54gL1xCa8u2CN0Z3ujrtRbuiNv2YJAvLF/EV8Z/wE15W3o0LbSHpWyW+B38dPtF7Nw6iImlR/OLiWD1ah0+xNqAm8fLT+HN9RNwZwb41qS3WNCvrMtxNOvcMpZ/6o+5qN4j4gYgIvOA3wEm8LBS6hdt+Y2IW6Sk8viWkzijaAs/HfRu81s7EbQsufhUgIeqJvLE9hOZPnAv9xa902NvpWfrcvj5+nk8MvUxJrl6turVHexrauTiFYuYP/oTvt9/U6eu8cNDk/lXWSmLpz2K10j+Ntj3/R5uWjmf/532XMKqqHdXjOKxrScxq3A3vx7yZnOVHej2knxHtJdfpg7cx++GvUGOkdHOFdonacQtHiLi9mjNQH699hzum/EEM10BnGK2+jbtrjd1Z7Cx2dUU4Jbtl9PfU8eDw9/qdoFrVEFmfXwNFxWv5fv9P+nWa/ckD1WXcM+603h+1gNMdMWXiFcH/Fy98sv8aOI/mRcupbf1nBOdBiI2WChu3XM2qw4P4e3pj3Up43aGR2sG8qs15/LHGX/lM+6G5vySLHklwo4mPzdsvYocVwNPjvpXp/NLvOKW+JCHebney89XzOPvJ97PSe4gpkgoASmFpRR2+A9o3m7rL6is4/5Htrv6ZynFMNPJ70b9jbWHC/ll+VQs1b1dlk/UFFFb5eXb/Zd1ydZY4ioWP7H+XdlvPQ6Hzf8dmktQWXGF+YHDnyXTHeBc74Fmt+60uaNz4r0mgInwq6GvU7krj5fqiuIOc1eI5JcXTryfz7gbjskvERujt23suPNAW/7jvc4Ih4v7xjzF7po8fnzoxG7PL22RFOLWhM1PN17If01cxjDT2SxoVlSpMtqto1/Ef/T/eK/R3rUBhplObhu3mEc/Ppm9lq9b4+N3G+dy4aTVzW0mnfkBBDma2Voei2xH/LR078zPLQYzCsv4YO9I9jQ1xBzeRhXk7V0lnDt0Q6dtaC2c0b8gVrvnd3S8rXt6xGR06T7+uO0MKu3eqZpWWr7m/DLCEeo4aJn2W263th9rPmorD8RyjaCysJSi0HTxw7H/5NlPZrI52DvxlBTiVmM7aWh0cWnOima3eN/Mkbdp9LlBWn/DRNPZtzfAhZl7yCrwcUfZRfjsQLfFh39TDl/r/16XGoPXBpyc8O43mLL4BtYHTe4+MpMZz32LG3afz2GridkfLmLa6zdx9qfXcOmmy5n22k184M8GwO5CU8W8/DXUHcjisO2OuSRjKYV/bxbnZ6/u9H2jBWdtwM0J713P9MU3cv2ueTxUNZFq2+KWPecwf8sXOG/Nl5j+4i386MBp2Ep1OryRey4c9j5HthRQZdMrpZI3Gwq7lF8ADloB3vH3a/VYLOdHnm1HfqM7pOZmVDCgsJpv7/xCt+aXtkgKcatq8jIop5YRDtUsShD/G9ynguxosqiym3iubji37z2LKruJivDPpyz2WRZBLMrtAD4VPKbEEs/bO2LjbaWL+fDTEvZb3fOwDln1YECOYTVnukgGjOf3y7J52EdcnFm6kffrx/Hy/Z/FUehj5T8mcO+RUyn6rcHQoRW4Hsln645BGLUO/t/WiwiGM2dn7gkwynUIs87EZ7vxqdjixMbGWSuMcvo7fe9oG/637Hzscjdnlm5kXNZB7ll8Di/VTuT97aPZvG8QTX8byJCSw7z5yon4lY0h0qX7TnXvxVVlUGs7aaJnq6aWsnm5fHqn8kuF3cSXd17A8oCLx6tmcfOLC9hnWfiUFc4/Fjuajm7X2k3ss0L/a+0mDltNQKhGsM+yqLYDMeWT6P8/H/d3NqwcwR4r2KPxBAkcxBuNL+DitIFbAQiGE1swzjegheKarZezcX0Rl5+8lOfenc3gD+D3d/h4fs10VJPB12e9y33/OYMVF/yOK9Zex7fHvMFZ3oNx3yuas7zb+UWdyZrAYIY5KrvcuRBQCmUeLUl0plRhofh20b+4rf4K3t0xmjX9CkPXrvRg5SnyHD58Qzy8MvFPzJp5Gwtnvcsz22Z0uU/NVgoThSiwEBpjjFcLhdjgRLpUaoRQ+rmj6FVuqb+Sd3eM5vapi3HUC/dtnINh2mRmNCJfqONboxbzww+/hEXXSqoApihQ4FcOgirQo+O6mrBYuX8ol41ZBcSXXx6o+AxH7izmywuv47OjtjL4I5t5xm1IoZ/nT76PhWsXcGRXHoWjD3N50Qru+fR01D4PVo7VPBPmV2c8y+sVk3ln+UQc/Rt4ZNajTHC2Xc005NhnOsFVCQo+9g9nhONAj8ZVUpTclCWMdB/GVqr5LRovQaXYtGo43sI6zsxeR/6YCqrGGJyXsxrveg/TS3YxPWMnQxcLhghH1gxgg39Ic8THel9D5Di/Tf0sPvUNx6+aOmV7NANMN6ZfCKrOt38BvFw9g8Mb+5PxURazB+3EdkNGmQPlVAxyVmM5Q+15ygQDhSEKUxQ2XWt3O2D1w/LYWMo4pn2mPUwEywNlltGle0d4sfqE5rAHlUnpnO3kPpPNtaVLcZg2phEunTq6Fl7C524P5tOUqbB6KTv5ajxx5xdDhHP6raGi1M0l41aT76qnKUO45PRlDHrBzW/2n0Pgzf54C+s4snQwj22djWuNl1mf2UTGLieDiioRj8WPVl7C8qemkFFYh2NtFn86MLfd+7Z8cZiA3a+J1b6ibskv7ZEUJTcUeI1GoPNvUY8Y3HLOv/j9a+fzzaarmFS4nzWZBQww67Ey4OrBSyl2VqEEPvTn4q44NlHEet9W/ZkKn+2KuaTSHm5xEhwa4MXaKXwpZ02nrmEAF+esYNA5NUzN2MVkVw033/Jvnqo+Aa/ZyOyMHfDjv2MDv/ncXxjnPMScKZswsTGIbbpqJBu39Pte7XiMggAusTBjzHhOMbEGN7LEV8KV2RubrxmrLUcJPZuLc1bQ/5xaZmTsZLKrhkyjkd/3H81X8lYwJysUzsGmj59d+Vc8UTa2da/WJMvm6MyI54/MRA1rwIzT2s5gKQVNRtz5xVaKYkcd/v5Q7DlCeVMWRyYLPxz4PhfbJ+JrcpG7vYnD/XIIDLTIcwWpHtfINwsXM39YCV8ctpZ/MImDB3Ip3BLkUFYOyoThGRUd3jtSejNEQo9IoMk2uiW/tEdyiJup2NE4ECtjX6cvEVSKBzafgu1SDM2vZlrOHtb7x/Jk5WwaCyw8RgCvKA7OFm74xwK8FmSZ/rga7SO9l2Z42o9F6M1v1JkMc1XGXFLpiMunfsKDG07hS7M7J242UOJsoCRKHDMN4St5RxugL8raBsDJntCYsgKzvvncWO/RkoBSvFk2jvFDD+CUJpwxzvt0YDJj5G7+vn8an8/a0CyK8Sb9iP9Q2I92TuwL5HHqlz8GYLyzPuwqnB5Obx3dp73jAaVYvKaUS6avwilWj48rM0VwZgU6lV8MoLG4kXvWncZFY9bSVBBqc64ZbvLVAau489Ji8Dfh7t/A6Jwj1Pg8OMXCmdNIjtlAfoYP11CLsvm52PUhoZqWuavNPBTJJxEBDiobCxC/yUBXbbfll7ZICnFzOS0+rhrBwtxVx7y148EU4dulb1A7LoPTvZvxisXwq8sZ6zrIaedvZLKrHJcIj116L6v8IyhyHmG863Dz+XYM94yuikTYHuyHERBK3AfaOi1ubil4n2dXzORt3zA+m3H8ZGGLUPG+bTtDx6P9tecW2Y6mo4kyrdmwvHEwNbVe5o9ajkeaYp5lYIrBlwZ/xPdWXcr6YA4TnPHN12wrTJH9K3M+wQAsFds5Hd0jmmdrpoDA3H7ryZQmnOKKy/Z4cWAyYmBFp/KLR4S/nPYge5vyGO86yNmnr8UGvnP9M8zN2MXQzz7CGn8R2YafyZ4y/IOdDDIDPDDrLwxx1HKydwsWgmecxRJfCT7bxURX2+k+Op9E8tc+y41ZZzDOs78LsRAbSSFuuU4f6w4UUjEcco3w+JlOXOds704gJHSWOro/gqNjrkY5fIzK2tC8H4x+eYjE9DYxw/5MEX6x8wKySyvwGo14je6ZXzjQ9HLFjOXcvfUsJkz8C/nG8W0THcVPtJi1PCfyv+VxOJoIO7p+9Lkm4FNw9/azGVxQzYkZO8g3Arglq4OrHOWcjAruK6jgnr1z+V3xi82ZtTUbW6O1cEZwS+vu7Z3Tktbs2Ge5+fPGkzlr0gYKzDryDavH55iaYnDB4DXcv/7UmPNLpCRsKcUoh48SZyg/DDAqsRTMzdiFKcIEZ3WLF0tjyD1cyso2A83X+3xbeaiN+0ds/GXZPJyja8k367otv7RFUnQo5BoNiCjerh/bLUXV9q7RXiKI9d4Rf+sD2WzdXMjXS5aQKQGcHb7/Y8MUg9v6/4ccj5+f77kg3L4T/y/S2R7ZJ2ofQnFht+InlmtbUX79Cu7ceyGHa7L41qjFeCTIEEd8c4K9hos/jX6GrRX9+f3h05qvHexk2FsLc1d+wajr2UCjgu9uvYyczAYuyl9JthEg3+ydedAXZa2NK79EBtRGi1zL89rNM3H4be/8fZaLlRuKuXbcUjIlgEd6tmyVFOLmFpNzRm7kwW2n4FOhjNPVH63tR4+ujnJv6dbeNSP+/Ap+uv0ihowsZ6zrAPlGAK/RfVWSgWYmD5WEpqws3DKfCsvZXK2y1LFVrPZoFjIVJUytbMeLTeiNfdhysWjrfFbsG8YPp7zKUEclY5z+TnXxj3Rm8dS0h3lj93i+U3bRMWGOh+j4iQ5ndxBUcMBy8/mVX6U+4OJboxcz1FHFCIfqtaV9RjuzOpVfAlElvNbSdGv5xmrlnM7kRb+CH+38HAOGVXGSd1u4ZN+z8ZU0E+c/fG0oF2y6CIdh89MRLzHIDByX6eLvPTueaDXvqHesreNBBb8+eDYf7ivm+6WvUeI6yDinTVY7S/R0lkYVZN6GL7B910BOnbCZOblbKXYepsCsJ9/o+VHephwrDBW2iyo7g52BAXxUM5q3NownN7+OH5f+kyGOSoodAQqMjC6t67YjWMfnV32FmtoMzh+3npnZOyh2lpNv+siWnh0+0BqHbTdVlpctgcG8VT6elRuKGTtmP98p/heZEmCU089AM7NXbbKUzbyNF2MaNj8pfonBZuNxfjpKx7HQXfnllwfO5YM9xXx/0muMd+3vVH5J6VVBfHaAizZeRm2jmzvHvcJkV3miTTuOsiYvP9lxCQdrs7hx3LuMd+9jvLO+RxN3owryRE0Rv9s4F1+9u3mZbaWOdq03t93Gsh29355/jvcjhmo+7nBaXFyyhvNyVpNt+BnlCJDXRWGLUGf7+b+KqTy24SSagiaoFuGN2BZNDPZ3ahsQUSDgzWxk/uhPOClzKwWGj5FOmyxxd0uY4yWSX+oDLu4oeY0Z3dip1V1E8sv+6n58eeyHzPRu73R+SWlxAyi36vnB/rN4c9kUxk7YwzeK3qHYWYGJwq9MrC4uK21G5YiOrmWi8IiFheBXJr/Zex4frx3NgKJKbh7zNkXOIxSZdQx3eHslcVdaPt5uGMwG/xCqmzKot9xYSjBFYSnBVgaGdPye/uDZ6cy8bA2uVjoqWhK5dmQ7wwjQz+FnkLOaEvcBsg0/BUYjg0xHt5dcLWWz3/LxfkMRm/yF1DR5aLBdzfbESoPlZOULkzj58pWdsqOfw0+Oo4GBzhrGug7gkSCDzAaGONwJW2U2QqXl4wcH5vLa0qmMLt3HTcPfYrijEqfYCc8v/7f/bP6zeiwDiiq5dcxihjgrGWrWUdzJ/JLy4hZhVWMj1356HXU7crA9Nrhs3HtcxJB3j0VB9m5F7XDhmGfTsiTTCk1ehSpqwPI5cJY7obier076Dydm7MBrNHZLFawzNKogtXaAartzgn/VQ7fy8II/4DU6N7/PiU2mYeMVIcfw9MoqtI0qSIXViF+BXxkE42gurrVdfOWxG3lmwd2dvr9HLDyiyBaDfoYnISW19tgQ8HH16gVUb83D9trgDOcXRXwf1OnG/KKGN7BoypLm/FJkNjLQ7HxBIF5xS4qhIK0xze3mvZkPs3Wqydv1pWysK2TGZ3ZxgmcnfhX727LG9nDH49dx/3X3YqvYI9UQm7JgAYsrJ9DfVceJWdsZ7KjGI0FyjQCDTIMco3fbWSK4xYnbdNK/k5piOxWTXMEeaSPsKdzipNDRuVJStd2A7VBMcaVOeOOl1OXlnRmPsmmyg3frxzfnl2meXQRV7Nm8u/OLC4sco5EhDiHHiH1oUHeQtOIGkGNkMM1lM8m1EV/OGurD0zVy4yhx1No12E7FDFd8a0gZGMxw7eWSzL1U2aHqm0eEbMOFg94vrXU3ybBCa29hdrFqlirkGBmc4LKZGpVfbCDfiD2b90R+MXAn5BsTSS1uEBrzZWLgNp3kdeoKfpTQpVJKVt/RAU2Ko/PLUZLEDI1Go+letLhpNJq0RItbH6XlcusaTbqR9G1u3YGnXBjz7+uOcUtkE/OcUdt4ZPiSBFrQtzoUNH2TtBe3DHHxnW88g99Ojo/8+mw3j/1uHqVDJifMBndVwm6t0fQaaS9uphhcnX0k0WY0U2f7+ePweaz98h8SakdPrzum0SSatBe3ZCOygF8ixv1oNH0J3fCi0WiOwVMujH38ev7pS+0ZHVrcepm+Mlpek5pkGR6Wf+cPmKPr2OQfkmhzuoQWN41GcwxOMUNLPKU4XWpzE5GdQC2hBTeblFIzRSQfeAYoBnYCVyilKrtmZmqzLVhHtiF4xTz6kRll06AC+JWFR8yUmsSeCvjsAEEsvOI65kMljSqIzw6SZSRmvmMy06iCWErhjlr+O6gsyq0G8s3EL+8UL93RoXCGUip6Vck7gLeUUr8UkTvC+9/thvukLAcsL5f9+kbqihSln9mBq1a4fNu5rFo9CmxYdund9O56CenPQSvA3FdvJWtwHdMH78FVK1y1Yy7Ldw3HNG2WnXI/OZKRaDOThkYVZMHOc/ho20imFZfh35/JU56ZPLVzJqUFB7i36I0+KW4tuQQ4Pbz9GPBv+ri4zXIrqk4IMOwVk8qlI8jFovzukQxxwOCbt9G/l5eo7gsMd3jJG1pN5qO57FCl5GKx9zclDHQJF/7gHXIMLWzRuMXJ1YM+ZO0LpRx+cRRDAcijocAg//rtKVmz6GqbmwLeEJFPRGRR2G2QUiryUcIDwKDWThSRRSKyXESWHz7SmQ/5pQ5OMbn1pDdpyD8a3UqE2iKT+4pfSqBl6YspBj8u/SdVY46teh46Eb6Z/2mCrEpuzsvwoeZUoQyQ8CK29cOE7w98N8GWdY6uitscpdQM4HzgBhE5LfqgCi3z22rLpFLqAaXUTKXUzAEF6d/2cU3ORqrm+oms/2c7hdnzV+pSWw9yvrcS16nlWK5QD3VjP4Przvl3SpZCegNTDJ6f8SD1g02UCMqEkjO29/rHb7qLLombUmpv+P8h4EVgFnBQRAoBwv8PddXIdCDHyOAbU9/FNzAU5eVThEUDUvONmCq4xcn/K32ZuqHhOD/R5uqc5Qm2KrkZ7cjAOK8cZYA/z+BXI15ItEmdptPiJiKZIpId2QbOAdYCLwPXhr1dC+h6V5gb8jZRNckm6BWGnLSPaS49QaSnOS/DB6dV4htgcN5JnzLSqbtu2sMUgzvH/4Pq0SYVpwQY4+ydD033BF3JXYOAFyX0FWsH8Fel1Gsi8jHwrIgsBHYBV3TVyCdrC1hRN6Krl0kKRo7bT/nmYQzPqOM7B2Ym2pxOYUW9E91GE5fmfMJsT/tNCx/5LV6sPoEmu/eHVo4tOMzG/DwaLQe37Z/R6/fPcjTy9bylFDraF9ZkSeeNthPfEJu8glruOHBios3BYdhcmvNJ3Ocl7devohnz5PXkjD+C07Sx4/ysW6IxRGEraf4f7/Fkp7w8m4KCOj6e8Wy7/k745AqqqjIpyK/rJctapzfjOXKvig0F3D7vFb6eu7dd/8mazhOdNiNpTL53V3p8/SoaRwO8O/1xMlJsJYvoj8hYyj7uozKWSt0FIyNh+e/ycTy0+mTWBRqY6Gp7eIXP7+br09/j1rwtzef3Zvh7836RuIncb5LvWlbWDWdH5qZ2q8XJks5b+/hRItPqr46U8tDqk/HGaUNKTb9KtS9OWcpuThSRzBWdSFItPBFaZl5lC0fs9seNiShM1DFhTkT4e/qe0XETfa+A7aA6xjUFE5kuWnsBR8KSSLuULTTFKVcpUXKD0FJBqVrSaWl3qoYjQmftt5Dmc3s7DnrrvtHXt5SNHTUSyohhvmai03lb906kTXZksYk4W9BSRtyAYxJKKmAgzTZHb7e2n4qEwhB/W4yN6rXwG2H7op9D9H5P39OIip9Y262SIV20Fm/JYFc8pIy42Uol9sMHnSA6MbRMGKmWUFoj3jDYymg+p7fCn6h4by2csZTckiWdp0N6TRlxA/3FpmTDwMBS8bWD9IVn2DKE8Q5I6AtxFA/xprEIqdmirUkKdCbUJDMpI25WChaLNZp40em8+0ipaqmVBAOONZ3HUoZ+hjGg46h7SJmSG4ApgilJ0Nqq0fQgOp13DylTcrPRb7RkpDNDQTRto9P58XQ2jaWMuIFuwE51bKRPPkMV57zMvhhHPUFKVUs1Go0mVlKm5GYphZUGnxtLNzozBqkv9QjG+51anc6Pp0+MczMR/VHjFMZSBhaqTz3Hzgh5X4qfniRlSm7Qt9746Yx+ju2j46d7SBlxswjPu0sxKmybV+omsb1hAHNzNjDHc5BaW/FU9QlUNnn5r/wPWeEfzijXISY7fSzx96fYWcEIh0qJ8Ma7iGFPhskQodyyeKJqFgcD/RjhOcLX81ZjK4URHloRub+F4nXfcOZ5y3rMngjxBLk30rkhcsw9InGzKejAhc1Ip90cZ7ZSLPH3542qyeQ6fVydu5TBZuvXannd7qKzC2WmjLilImWWwWUfXk+wOrQO/StqOvef/Qi3rr6C+gOZSNDgpYFTsPdl4BlZy5sz7+eH6y7hujFLWZCzFui5BJOO2ErxUMXJvPTsHBpK/QwZWMWi3E+b49CIGju2KZjBL9acz9kn3Y+zj1QBI/HQMj1F9m/ffAVnDt7EN/OPfkSnVtl8a+mVqCNuEMXmGQO5f8Qrx51riBBUNg9VTeOMrPWUOIK9EKL2STlxS6XM/u1tl6HKvPz+0sewMdjSOIg/7j0T/45snrr0HiyE/y07n08bilAf57BlWhZKCYOc1cdkRE37RKeJ1dVD8Y1o4s+nPMYoRzV3HjyDN7aPJ7gnk+Ip+9ixvpAzZq0j2+nHvz+Tk5+5jaKp+/nruCfxJNHiod2ZztcEvSz4YAGOXR5u/Nyr/HX3TMo/HcjQGft5fPwTLNj8RXasL0RswVm4nh8dOIPX3puOZ2Qtv5/6NCi4/LSPeG1XKYcbsnjVV8QP3/oCuG3+9Nm/cOOy+dhBg4KCOtyP5nHf2afz3Dn3MMrR1C32d5bkeZppRhDF3uocFs17g3drxvODh6/hTys+y7oVxZx56qeMczYyydXIuH4HcWYEaZzs49e7z6fR76TEdaD5DZsqQp5oDBGCKNZtHUr/pSZff24RtcrBq+/MpLHehZ1hc/DNYcycsZV/fzCJt3aPxX3YZMyMMnavLWRp4+BEB6FHCKL48ocLMAzF9Z/7Fy/sm8bBrf254OyPqXplCHPfv5HtOwZx8sxNKIdim28Aa++cgpVtkftkFisaivF+msErz58M7+Qxq/8u/vjTy8GA/GVO/rz/VIY/bDJtZBlfHf0+wQyD35zxDIPMQKKDnnriliqZ3VKK+moPm+sH87X+S/CcXA6A0Sg0WKHlpp+qGcMuXz75OfXcPu1N9jw/kmCDkyFmI4aILr3FSUApjBoHc7/5Ifd84c/sa8rBNbKW3895itwhNUy6eCMjvBWYQ3zUVXo5++KPuWjwauzcIIPN6kSbfwzdlc79SmHu8PCzE17iyuy17D6YzxknruPsnLWYAYVji5cTS7eT62xAgsKy/cOpGeFA3Ba5N+1mc/1gfENtfnbNE9SMa+LZD2fhzzXAbWFcdISBnjp2XuzgoZEvsbuxgPJz/czNOEBmEpSCU6Zammpjtp0inFiyk1X3TeHSwqnYDvjC5z5k16h8dv5qPKeNmYTMqcRpWpw7bCPnZm7iHjfk9a8lU4ykFvHIcI546ampWpG4alRQ/M8gL1efzN9Gz+Br05cAUOysoLoyk8+XfsJvtpzDGaO28O/Xp/HBf2ayJEMY9/ldjHL66clVIq2YV+HtXgxAmfA/987nx7PrUbaw9t7JrMyczMArd1OxazDb/jKWnY2gpitmFpaxc7sXf4GHiVP2886+EoYsUfyg5ks4nYqzzl7JqvemEdjsxjG4itVHhjD7hM1AqEkgd4mHH4ybyy8Gv93NIYmflBE3SL0u8ntHvMz2H7tY5R/B6d4t5IdfZpvuzqDK9jLDVc6upgyGOBrwiPCH6+9jkFmXEuHsrI09GbYcQ/j8H94AoMCs40RPGWedsI4hpsW9c55ggusI/zP+BQaY9Vx99YdUWFl4pZEZ7qputc1EjruWGcfA3O6MI48IL86/m7frxzPOvY/xrko+mj2U4Y4Kxjj9+MYoPji5iBmePdTaTkY4giy952PqbTezPXu5NOcTPv7ZKJzSxOezN2MA2/9nCesbh3KSZyc+5SDXCGAh/HnU87x563BOzdgFGAlPxyklbqlIiSNISdZWiErwE5x+LBoAYUxUiWGqKzWELRmJCMoXszcBEYEQcoxQr91MdwUgzHDVYqEoMn3g9IX9pjeDTJv5/daHS9sGZ2fsB0Lh9opwlncPJsIAowkQTnZXNMffOGcjY5zrw/EbosQRZJRjR/h6TVjh9GsA53p3YyZJa1fKiJsViuuUI4g67lG3FDC7+f/xfpMdKw6LOzuNpiNsjsZdsPmDJkfjNfqukbi3UK0e7w6Oe0HFMU6rJ9N5JMythbetl2qw2V0d57e9c7ozTuNJY9GkjLhB6rW7wbGZrOV+a8cIu7U8li70RJiMFts2tJmJo20wWnFLBno6jlpev600aXBsfLZ13ZZpODodJ5KUErdUo7VMF01rwhZ9LF0FrrvpKI7aErqeKrklG/GkI7vFdrTAdeQ/2dJryohbMkVarLRMKC3d4OjChBYct/pqUKmkX5E1nqkxnZ1G01lai//WCIafQTLEdU+k847SUXv3bCu9Roi+bk/FY49NvxKRh4ELgUNKqUlht3zgGaAY2AlcoZSqFBEBfg/MA3zAdUqpFZ2yrBXSfYXS1sKXbmFO5vAki209YUdPhS2Z02wsJfJHgfNauN0BvKWUKgHeCu8DnA+UhH+LgD91j5k098hoNOmMTufdR4clN6XUeyJS3ML5EuD08PZjwL+B74bdH1dKKeAjEckVkUKl1P7uMDbdu+xTkXirDH3xGca7zHhfjKP26Gy1tLNtqYOiBOsAMCi8PRSIXkNmT9jtOERkkYgsF5Hlh4/ox6nRaLqXLncUhUtpcVeylVIPKKVmKqVmDigwO/Qf6/QVTfLS2fFKfQmdzruPzvaWHoxUN0WkEDgUdt8LFEX5GxZ26xZSscdUcyx97Rl2Rs77Whz1FJ19lb4MXBvevhZ4Kcr9GgkxG6jurvY2jSYV0UKVOGIZCvIUoc6D/iKyB7gT+CXwrIgsBHYBV4S9v0poGMhWQkNBFnSXofrjv8lJPFXN3h7nlkzEvipI342jtuix6VdKqfltHDqzFb8KuKFTlsSAlRzDZzRdoC8+Q0Wcq4L0wTjqCXQLr0ajSUtSZvqVhej2iyTEVhJzX7mt9DPsCJ3Oj6e3x7n1On25vSZZaU48+tF0Gzqddx8pUXJTArdtuxynoQf7JhOGKLbsH4gYHRfdDEPxyubJrKsu7AXLkgNDFLYSZE8GxqiO40in89bZemAAEkebZYSUELcL5y1lRUUR+2v6JdqULmPZBs63c7DPqky0Kd1CVqafucM2d+jvOxNe56/7TkrIM0x0nGePr+CknO0d+kumdN64OpfGwib6DahLtClkh9PYq3EWalNC3O4qXEHdoA9YHzSxe2g1196ixvZwy8qv8vCUJxJtSrdgIZgonGIBrjb9XdOvnKuzX2FVoAm/cmD24nLqVbaXW1cuTHicOztoTUumdP7Fmq9y1rhNLBzwXkLtiOa1ONNMSogbQJbhYZY70VZ0nTq7FsutmO3peMpZatFxeEwxOMHdtgD2FNV2VZLEubdDH8mSzj3eAKWZ+5Mgzo6SGec6caldDEpV9Dim3kfHeZ9Di5umb6A7IfscWtw0aU9nPiCtSX20uGk0mrREi5sm7bFQus0tTuJdPTgZ0eKm0WiOozODZpMNLW4ajeY40qHkljLj3NIJV60w4Z5vJNqMlMBoAiMITRldu463Cibc+w1dPY0RRxDyJyd+dkJX0OLWy2QZHtbccm+izUgZfnWkhMc3z2L9Z55MtCmaFENXSzUaTVqixU2j0aQlWtw0Gk1aosVNo9GkJVrcNBpNWqLFTaPRpCVa3DQaTVqixU2j0aQlWtw0Gk1aosVNo9GkJVrcNBpNWqLFTaPRpCUdipuIPCwih0RkbZTbT0Rkr4isCv/mRR37nohsFZFNInJuTxmu0Wg07RFLye1R4LxW3H+rlJoW/r0KICITgKuAieFz7hWR5Pk2mEaj6TN0KG5KqfeAihivdwnwtFKqUSm1A9gKzOqCfZo+yI7g0XXELGUQ5+cqNRqga+u53Sgi1wDLgduUUpXAUOCjKD97wm4aTUxYyuYHey7iwzUlDCiqpKo2g6bDGVyx/Uw+2TmcMYWHeXrss+SZHX/gWNO36WyHwp+A0cA0YD9wV7wXEJFFIrJcRJYfPmJ10gxNumGKwazcHQz4yMTxeAH9X/RSuEQou7eEAf9yUx900c/wJNpMTQrQKXFTSh1USllKKRt4kKNVz71AUZTXYWG31q7xgFJqplJq5oAC3SynOcq1/dZTfnoAM6gwAwojqHA0KGpGGjxc+hdM0Z38mo7pVCoRkcKo3UuBSE/qy8BVIuIWkZFACbCsayZq+hp5ppcvz/gPvgGh5ClKoUwoPnMnY52ZCbZOkyp02OYmIk8BpwP9RWQPcCdwuohMI/S5jZ3A1wCUUutE5FlgPdAE3KCU0nVOTdx8t2Adj5SejvcwKIS6QpO/j/kboMVNExsdiptSan4rzg+14/8XwC+6YpRG4xSTey58hJ8uW4AZUEyav57+phY2TezoxgtN0nJmho8Dp1uUTzb5ZuGbiTZHk2KkxKf9Rr72FTK2uUDQ353sQ4iCrEDo26ULHrwZZXDs89fpIS0RBZFvQttuxdIv302WuOO+TkqIm3uPkw+vv6tTAdRoNKnLxD/fyIf+XKa6jsR9bkqIWwRbv6Y1mj6HhRDsxHkp0+ZmKy1sGo0mdlKm5GahsLETbYZGo0kRUkbcACxdetNo+hTShSyfMuJmgy65pTEGxjHPN7LflntH52s0KSNullJYXZFxTdISVDY/OfBZbhv4FvmmyUt1RfzPmvOxmgwWTvqAB1fPAcDlDnLtuKW8sncy3xn9GnMzKggqm/8tn80efy6/Gvoq2YYDS3c8aUihDgU9hyt9qbUVHz0wI7xt8d/PXo5SkOltxGsEsCtdeFdkkOEOsPjQeGreHMzPNl2IhWKfJbx2/ym8v7yUIGhhSzf6QrUUdI9puvJqfSnVJZBrGNQqG89h4ewL13Jz/yVkGyYvl07BKFU8PfZZzl9zDWpOFdWrC/BNsbhw8S0Yc/xgCTmGia0UhohOK+lCFxYqTZmSmyY9MUR4Yd90+k8+BEC2GFz21bf5+xuzuXX3JdhKsW3vAEr6HcZCcehwP04dtp3sXfCtsosxPBb9shuYMGJ/s6BpYdNACombbipOPywUtlLs2FTIBUPXNVcpF+Yup9/EI6w7OJh9lqAsYYi7isOW4M1upNS7n5qRsPLf47hp+jtUlWdx/sC1ukqqOYaUqpbqxJt+WChy1pm8/eIpPDlnLplliuy9TfinO7lzwZMcsLLIzvMxKaOM7U35ZHkameDZQ/aECoKWyUVZa/lDzXmcl7mBlnUYnV7SgL7Q5mYpsPWHQtKS17/3v+0eX3zCn5u3X578GACvTX847CJ8eNldgISHC2lB04TQ1VKNRpOWpEzJDfQMBY1GEzspI24WottQNBpNzKRMtdRSusFNo9HETsqU3EC3u2k0fY6+0FtqR3XzW7p2mtaYUYX0yLM2RT/3dCfy3LvrOaeGuCkJrcapE3efwA4/Z4Oo0rrSJfd0x+7m/J0S4tY4qIn5D9yK6NTdZ1ASWsvLCIAZgGBWoi3S9DZKwLTB7GTdNCXEbcfFD1Bn+1kR8ABgq5TpB9F0AUNsHjhwOh9sHcXDpzyaaHM0PYwhdnPejmwb4RJNZ3J8SogbgFucTHb69HCQPsbwjAo+dg9nkqs20aZoEoAZHgKWbaTpp/0g9AXyPNObaDM0vUyOowGHw9Jfm9fEja7faTSatESLm0ajSUu0uGk0mrSkQ3ETkSIReUdE1ovIOhG5OeyeLyJvisiW8P+8sLuIyP+JyFYRWS0iM3o6EBqNRtOSWEpuTcBtSqkJwGzgBhGZANwBvKWUKgHeCu8DnA+UhH+LgD91u9UajUbTAR2Km1Jqv1JqRXi7FtgADAUuAR4Le3sM+Fx4+xLgcRXiIyBXRAq723CNRqNpj7ja3ESkGJgOLAUGKaX2hw8dAAaFt4cCZVGn7Qm7tbzWIhFZLiLLDx/RH+7TaDTdS8ziJiJZwPPALUqpmuhjSilFnPP3lVIPKKVmKqVmDigw4zlVo9FoOiQmcRMRJyFhe1Ip9ULY+WCkuhn+fyjsvhcoijp9WNhNo9Foeo1YeksFeAjYoJS6O+rQy8C14e1rgZei3K8J95rOBqqjqq8ajUbTK8Qy/eoU4L+ANSKyKuz2feCXwLMishDYBVwRPvYqMA/YCviABd1psEaj0cRCh+KmlHqftj9qf2Yr/hVwQxft0mg0mi6hZyhoNJq0RIubRqNJS7S4aTSatESLm0ajSUu0uGk0mrREi5tGo0lLtLhpNJq0RIubRqNJS7S4aTSatESLm0ajSUu0uGk0mrREi5tGo0lLtLhpNJq0RIubRqNJS7S4aTSatESLm0ajSUu0uGk0mrREi5tGo0lLtLhpNJq0RIubRqNJS7S4aTSatESLm0ajSUu0uGk0mrREi5sm6ZG2vpqr0bSDFjdN0qNUoi3QpCJa3DQaTVqixU2j0aQlWtw0SUed7eeTxgA+O3CMe6Xl460GkzrbnyDLNKmEoyMPIlIEPA4MAhTwgFLq9yLyE+CrwOGw1+8rpV4Nn/M9YCFgAd9USr3eA7Zr0hQLxQ0bvsjhzf0BcJcbjG5YgLnHQ/bEI7w7/fEEW6hJBToUN6AJuE0ptUJEsoFPROTN8LHfKqV+E+1ZRCYAVwETgSHAYhEZq5SyutNwTfqSY2Rw1pBN/OulAXgP24CF2uCibqhw3UVLyTI8iTZRkwJ0WC1VSu1XSq0Ib9cCG4Ch7ZxyCfC0UqpRKbUD2ArM6g5jNX2HbxZ8RHWJCtUVwr2l9bN9fCN3R0Lt0qQOcbW5iUgxMB1YGna6UURWi8jDIpIXdhsKlEWdtof2xVCjOY6BZianzlmHMkP79YNMnvzMg5iim4k1sRFzShGRLOB54BalVA3wJ2A0MA3YD9wVz41FZJGILBeR5YeP6Bqr5nh+N+wNKsea2E4h53P7OMFlJtokTQoRk7iJiJOQsD2plHoBQCl1UCllKaVs4EGOVj33AkVRpw8Lux2DUuoBpdRMpdTMAQU60WqOJ0vcDDmzjMPThbtKntWlNk1cdJhaRESAh4ANSqm7o9wLo7xdCqwNb78MXCUibhEZCZQAy7rPZE06YymbQ1Y9dx6eyLyNF7N1SyFKYP5HX+WybWdxT1UR1XYDlrITbaomyYmlt/QU4L+ANSKyKuz2fWC+iEwj1Ny7E/gagFJqnYg8C6wn1NN6g+4p1XREpeXjpfpifr9pLjU7cnENrWfi4P3cNGcxhtjUWR7eOTSWu1ecxV2V8xhZup9bi9/gDE8NXsOVaPM1SYioJJi4N3OqRy17vahjj5q0ZF2ggQXrrqEh4GRe8XpuKHifoab3OH82CgNhY7CR3xw4h2V7hzMst5qnxz5LXiv+NenFrHPLWP6pP+ZlFGIpuWk0PcbL9V5u/+QarpmwjKtzljPMkYGBt1nIbI5/+Y53urmv6G02Dba459AZzFn2VZ474UFKXVrgNEfRLbSahPF0bR7fX3Mpvz7hBb5dsIZhjgxs7GZBO/r/aPtaE1ZY+AxKnU5+O+RdFo77kCtXLeQjv2790BxFi5smIawO+PnF+vO5fcKbnO+tBI6KmN3iL9qt5XFThEW567m4eC3Xr7macqs+MQHSJB1a3DS9TqMK8oUPvs6Cko+4LGs3AJZSWOH2XyuqHTjibrVoG47eNzC4rWApQ3OquXzjF3VPqgbQ4qZJAD87PIPsrAYW5q7DwDhOzKL/R9OawEXc3OLgwVF/Y+eOgTxdN0ALnEaLm6Z32d1Ux4vbpvDNse9gIsdVQaP/WhO4oLKwlGrVf7bhYO6UDfxm49lU2g0JCJ0mmdDipulVPvYPwbYNprnL2vRjoQgqmyBWs3C94+/HTXvmElQ2dSpIUNlY4Q4HK6pHdV7+amrrMnjfP6jHw6JJbrS4aXoNS9ksqR1LVkYjI50hcWr5Cyqb2/eexYlLvsGngYxm4bp56XyynX5O/2Qhpz1yOye9fz3lloWFwkSaz5/m3odh2CypGUujCiY4xJpEosVN02vYKNZVFTIy90hoX6njfn5ls3hNKeqghworC1sp1gacSJmHgO2gYX0uCz//BqrMy12HzsJW6piSW75h4HI3sdefS4XVmKigapIALW6aXsPG5lBtFkUZlW368YjBnXNexs5panb73f6z8YyrJsMIwEgfC3JWkzvhCMsPhWa1RIQRwBAh19vAQV82VbZO3n0Z/fQ1vUq9z02Rp6JZjFoj2zj6jQS/slm6aRTzitdjYRD0Owgqha0gz9OAEfVR04jIZTiDBCyTRqVXm+nLaHHT9CqZ3kbK/PltHvcrm+8s/QK5K1zcvuxylviHYjgtTsnaTI6jgZyVbk5dchM16wpYWLSkVZFsCDpxmRYWQlCv2dBn0XNLNb2GgUH/rHp2+fKPaSdrSUF+HXWnB8nPaOT9mrHYTQYz3IeY6DrEtqsGsKliIPOnvsvpGfuIlq5Ix0JFnZcphft6PkCapEaLm6bXcIrJjPwy3tozluAIhSnHL/BgivDS5EeaRevr2y9nbukmXCK4BO4u+ieBYQqXSHO14+hwXcVByyAYcDDYU4NTbJyil0Pqq2hx0/Qqs7O28Yp/EruaMhjlbP37o6YIJqEZCD8d8RIDzABwVAgjLWkt5yDYwCf+ImxbmODdh0d0lbQvo9vcNL3KiZ59GIbiE3/xMfNGW/sBFDmCOOG4+aWtzT8NKMUbFZPI8AYY7TpIrk7dfRr9+DW9ynBHFl8a+zF/XHc6PkVz9dNq5QcQUOpYP5G5p+FjkeMWcNBy8Z/VY/mvkmV4JEie/r5pn0aLm6bXuSV/DSKK+46cCoSEKdZBG61VNE1CVdJvbb6SAUWVnOTdxmDTh1P0UJC+jBY3Ta/jNVz8beaD/GPbJJ6rnQS0LlpWG9uRSVV21P4vDpzNkTov3x/7L7xGI4Wm7kjo62hx0ySEia4Mfjv9WR7YMIenaqZiQ/OPFv9b/qITrU8Jvz54Ju/uHMNNpf9msFlNsSOgPxqj0b2lmsRxnreRqsn/5Ofr5vHJoBHcOuR1BhiNR0tpAlaL4XCmhATOUrAlmMd/b7+Aw7WZfH3iEiZ49jLC0UCBob+loNElN02CuSq7kiemP8ym8oFc+c71PFV9ArXKQRDBr+SY4R5BhHrboMp28PP95/G1t66jyTa4c/I/mJ2xjWJHHQNNr/54swbQJTdNEjDN7eY/JzzOfVXj+cP7Z/Hn4OlkF9UwIq+SUVnlmNhYGKypHMK+yhyadmRhF/q5ec6bTPPsJtvwU+QIMtDMSnRQNEmEFjdNUuA1XNyct5UvXbCGF2rH8vLBqeyoyGftriGhz34L5OXVUTroABdMWsNo1yE8EmSQ2cAA00GWkZnoIGiSDC1umqTBFIOBZiYLcnYyv99WKiyLatuJXzkIhAeLeCRIpjSRb1jkGC7coquhmtZJii/Oi8hhoB4oT7QtLeiPtilWktEubVPsJKNdLW0aoZQaEOvJSSFuACKyXCk1M9F2RKNtip1ktEvbFDvJaFdXbdLleY1Gk5ZocdNoNGlJMonbA4k2oBW0TbGTjHZpm2InGe3qkk1J0+am0Wg03Ukyldw0Go2m20i4uInIeSKySUS2isgdCbRjp4isEZFVIrI87JYvIm+KyJbw/7xesONhETkkImuj3Fq1Q0L8XzjuVovIjF606ScisjccX6tEZF7Use+FbdokIuf2kE1FIvKOiKwXkXUicnPYPdFx1ZZdCYsvEfGIyDIR+TRs00/D7iNFZGn43s+IhNZkFxF3eH9r+HhxL9r0qIjsiIqnaWH3+J+fUiphP0JLcW0DRgEu4FNgQoJs2Qn0b+H2a+CO8PYdwK96wY7TgBnA2o7sAOYB/yK0BvdsYGkv2vQT4PZW/E4IP0c3MDL8fM0esKkQmBHezgY2h++d6Lhqy66ExVc4zFnhbSewNBwHzwJXhd3vA64Pb38DuC+8fRXwTA/EU1s2PQpc1or/uJ9foktus4CtSqntSqkA8DRwSYJtiuYS4LHw9mPA53r6hkqp94CKGO24BHhchfgIyBWRwl6yqS0uAZ5WSjUqpXYAWwk95+62ab9SakV4uxbYAAwl8XHVll1t0ePxFQ5zXXjXGf4pYC7wXNi9ZVxF4vA54EyRVr7m0zM2tUXczy/R4jYUKIva30P7CaEnUcAbIvKJiCwKuw1SSu0Pbx8ABiXGtDbtSHT83RiuIjwcVWXvdZvC1abphN7+SRNXLeyCBMaXiJgisgo4BLxJqIRYpZRqauW+zTaFj1cDBT1tk1IqEk+/CMfTb0XE3dKmVuxtlUSLWzIxRyk1AzgfuEFETos+qEJl44R3LSeLHcCfgNHANGA/cFcijBCRLOB54BalVE30sUTGVSt2JTS+lFKWUmoaMIxQyXB8b96/NVraJCKTgO8Rsu1EIB/4bmevn2hx2wsURe0PC7v1OkqpveH/h4AXCSWAg5Gib/j/oUTY1o4dCYs/pdTBcOK0gQc5WpXqNZtExElIQJ5USr0Qdk54XLVmVzLEV9iOKuAd4DOEqnaRxTOi79tsU/h4DnCkF2w6L1ytV0qpRuARuhBPiRa3j4GScK+Ni1Dj5cu9bYSIZIpIdmQbOAdYG7bl2rC3a4GXetu2MG3Z8TJwTbgnaTZQHVUl61FatHdcSii+IjZdFe5xGwmUAMt64P4CPARsUErdHXUooXHVll2JjC8RGSAiueHtDOBsQm2B7wCXhb21jKtIHF4GvB0uBfe0TRujXkxCqA0wOp7ie37d3QvSiV6TeYR6lLYBP0iQDaMI9Vh9CqyL2EGoneEtYAuwGMjvBVueIlRtCRJqV1jYlh2Eeo7uCcfdGmBmL9r0l/A9V4cTXmGU/x+EbdoEnN9DNs0hVOVcDawK/+YlQVy1ZVfC4guYAqwM33st8OOodL+MUCfG3wB32N0T3t8aPj6qF216OxxPa4EnONqjGvfz0zMUNBpNWpLoaqlGo9H0CFrcNBpNWqLFTaPRpCVa3DQaTVqixU2j0aQlWtw0Gk1aosVNo9GkJVrcNBpNWvL/AX8OHnm8um6SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "img = Image.open(\"D:\\\\研究生\\\\研究内容\\\\研究生开题\\\\工作空间\\\\最终版实验程序和数据备份\\\\examination_2\\\\model-2.jpg\")\n",
    "# img.show() # 会调用系统的显示窗口\n",
    "plt.figure('image')\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bec1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作者：常晓松\n",
    "# 作用：日志等级预测\n",
    "# 时间： 2024/7/9 19:31\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import diags\n",
    "import scipy\n",
    "import stellargraph as sg\n",
    "from playsound import playsound\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import *\n",
    "from keras import Model, Input\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from numpy import sort\n",
    "from random import random\n",
    "from stellargraph.layer.gcn import GraphConvolution, GatherIndices\n",
    "from keras.layers import Lambda\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.spatial.distance import pdist\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "import pymysql\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split  # 出留法\n",
    "from keras.layers import Convolution1D, MaxPooling1D, LSTM\n",
    "import random\n",
    "import numpy\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "def get_ave_and_standard_error(data_list):\n",
    "\n",
    "    SD_P = numpy.std(data_list, ddof=0)\n",
    "    n = len(data_list)\n",
    "\n",
    "    ave = mean(data_list)\n",
    "    standard_error = SD_P / math.sqrt(n)\n",
    "    return round(ave,3),round(standard_error,3)\n",
    "\n",
    "def get_connection():\n",
    "    global conn\n",
    "    host = \"127.0.0.1\"\n",
    "    user = \"root\"\n",
    "    password = 'chang123'\n",
    "    db = 'predict_log_final'\n",
    "    conn = pymysql.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        db=db,\n",
    "        charset='utf8',\n",
    "        # autocommit=True,    # 如果插入数据，， 是否自动提交? 和conn.commit()功能一致。\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "def shuffle_data(node, semantic_vec, syntatic_vec,leaf_method_name_vec,lable, seed):\n",
    "    c = list(zip(node, semantic_vec, syntatic_vec,leaf_method_name_vec,lable))  # 将a,b整体作为一个zip,每个元素一一对应后打乱\n",
    "    random.seed(seed)\n",
    "    random.shuffle(c)  # 打乱c\n",
    "    node[:], semantic_vec[:], syntatic_vec[:],leaf_method_name_vec[:],lable[:] = zip(*c)  # 将打乱的c解开\n",
    "    return node, semantic_vec, syntatic_vec,leaf_method_name_vec,lable\n",
    "\n",
    "\n",
    "\n",
    "def get_method_name_emb(seq):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select method_name_emb from data_model_1 where seq=( select methodSeq from data_model_2 where seq=\" + str(seq) + \")\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    emb=oneRow[0]\n",
    "    cur.close()\n",
    "    return eval(emb)\n",
    "\n",
    "\n",
    "\n",
    "def get_positive_data_from_db(vec_length):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select vectorStruct,logNum from data_model_2 where logNum<>0 and vectorStruct is not null;\"\n",
    "    result = cur.execute(sqli)\n",
    "    methods_padding = np.empty(shape=[0, vec_length], dtype=int)\n",
    "    labels = []\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        methods_padding = string_vector_to_int(methods_padding, oneRow)\n",
    "        labels.append(oneRow[1])\n",
    "    return methods_padding, labels\n",
    "\n",
    "\n",
    "\n",
    "def add_nagitive_data(methods_padding, lables):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select vectorStruct,logNum from data_model_2 where logNum=0 and vectorStruct is not null limit \" + str(\n",
    "        len(methods_padding)) + \";\"\n",
    "    result = cur.execute(sqli)\n",
    "    for i in range(1, len(methods_padding)):\n",
    "        oneRow = cur.fetchone()\n",
    "        methods_padding = string_vector_to_int(methods_padding, oneRow)\n",
    "        lables.append(oneRow[1])\n",
    "    return methods_padding, lables\n",
    "\n",
    "\n",
    "def string_vector_to_int(methods_padding, oneRow):\n",
    "    methods_padding = np.append(methods_padding, [list(map(int, oneRow[0][1:len(oneRow[0]) - 1].split(\",\")))], axis=0)\n",
    "    return methods_padding\n",
    "\n",
    "\n",
    "def float_revert_int(value_list):\n",
    "    revert_value_list = []\n",
    "    for i in value_list:\n",
    "        revert_value_list.append(int(np.round(i)))\n",
    "    value_list = revert_value_list\n",
    "    return revert_value_list\n",
    "\n",
    "\n",
    "def draw_confusion_matrix(y_true, y_pred, dic_lables):\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "    labels = []\n",
    "    for key in dic_lables:\n",
    "        labels.append(key)\n",
    "    print_message(type(labels))\n",
    "    # 小数转整数\n",
    "    y_pred_int = float_revert_int(y_pred)\n",
    "    sns.set()\n",
    "    f, ax = plt.subplots()\n",
    "    C2 = confusion_matrix(y_true,y_pred,  labels=labels)\n",
    "    print_message(C2)  # 打印出来看看\n",
    "    sns.heatmap(C2, annot=True, fmt='.20g', ax=ax, cmap=\"YlGnBu\")  # 画热力图\n",
    "    ax.set_title('confusion matrix')  # 标题\n",
    "    ax.set_xlabel('true')  # x轴\n",
    "    ax.set_ylabel('predict')  # y轴\n",
    "    save_pic(plt, '混淆矩阵')\n",
    "    # plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def save_pic(plt, file_name):\n",
    "    # 创建目录\n",
    "    dirs = 'C:\\\\Users\\\\chang\\\\Desktop\\\\日志工作空间\\\\实验图片\\\\'\n",
    "    t = time.strftime('%Y-%m-%d-%H', time.localtime(int(time.time())))\n",
    "    dirs = dirs + t\n",
    "    file = dirs + '\\\\' + file_name + '.png'\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    # 保存图片\n",
    "    plt.savefig(file)  # 保存图片\n",
    "\n",
    "\n",
    "def plot_value(y_true, y_pred):\n",
    "    balance_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision = sk.metrics.precision_score(y_true, y_pred)\n",
    "    recall = sk.metrics.recall_score(y_true, y_pred)\n",
    "    f1_value = sk.metrics.f1_score(y_true, y_pred)\n",
    "    return balance_accuracy, precision, recall, f1_value\n",
    "\n",
    "\n",
    "def autolabel(rects, ax):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "def plot_graphs(history_loc, type_loc):\n",
    "    plt.cla()\n",
    "\n",
    "    plt.plot(history_loc.history[type_loc])\n",
    "    plt.plot(history_loc.history['val_' + type_loc])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(type_loc)\n",
    "    plt.legend([type_loc, 'val_' + type_loc])\n",
    "    save_pic(plt, type_loc)\n",
    "    # plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def str_revert_to_list(methods_padding, max_len):\n",
    "    methods_padding_list = np.empty(shape=[0, max_len], dtype=int)\n",
    "\n",
    "    for methods_padding_one in methods_padding:\n",
    "        list1 = methods_padding_one.split(',')\n",
    "        list1 = list(map(int, list1))\n",
    "        methods_padding_list = np.append(methods_padding_list, [np.array(list1)], axis=0)  # 添加整行元素，axis=1添加整列元素\n",
    "    return methods_padding_list\n",
    "\n",
    "\n",
    "def get_object(save_absolute_path):\n",
    "    summer_load = None\n",
    "    with open(save_absolute_path, 'rb') as f:\n",
    "        summer_load = pickle.load(f)  # read file and build object\n",
    "    return summer_load\n",
    "\n",
    "\n",
    "def save_object(summer_save, save_absolute_path):\n",
    "    summer_save = pickle.dumps(summer_save)\n",
    "    with open(save_absolute_path, 'wb') as f:  # open file with write-mode\n",
    "        f.write(summer_save)  # serialize and save object\n",
    "\n",
    "\n",
    "def get_parent(vec_chair):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select parentId from data_model_2 where seq=\" + str(vec_chair[len(vec_chair) - 1]) + \";\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    if oneRow[0] == 0:\n",
    "        return vec_chair\n",
    "    else:\n",
    "        vec_chair.append(oneRow[0])\n",
    "        return get_parent(vec_chair)\n",
    "\n",
    "\n",
    "def get_chair_vec(vec_chair, i,maxlen):\n",
    "    cur = conn.cursor()\n",
    "    one_leaf_list = []\n",
    "    leaf = True\n",
    "    message_type = i\n",
    "    for one_leaf in vec_chair:\n",
    "        sqli = \"select a.vectorSemantic,REPLACE(REPLACE(vectorStruct,'[',''),']',''),logNum ,REPLACE(REPLACE(syntacticMessage,'[',''),']','') \" \\\n",
    "               \"from data_model_2 a  where seq=\" + str(one_leaf) + \";\"\n",
    "        result = cur.execute(sqli)\n",
    "        oneRow = cur.fetchone()\n",
    "        vectorSemantic = oneRow[0]\n",
    "        vectorStruct = oneRow[1]\n",
    "        logNum = oneRow[2]\n",
    "        syntacticMessage = oneRow[3]\n",
    "        if vectorSemantic != \"[]\":\n",
    "            if message_type == 1 or message_type == 3:\n",
    "                if vectorSemantic is not None :\n",
    "                    if len(vectorSemantic)==0:\n",
    "                        vectorSemantic='0'\n",
    "                    list_semantic = vectorSemantic.split(',')\n",
    "                    list_semantic = list(map(float, list_semantic))\n",
    "                    list_semantic_tmp = pad_sequences([list_semantic], maxlen=maxlen,\n",
    "                                                      padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "                    list_semantic = list_semantic_tmp[0]\n",
    "                    one_leaf_list.extend(list_semantic)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "            if message_type == 2:\n",
    "                if syntacticMessage is not None :\n",
    "                    if len(syntacticMessage)==0:\n",
    "                        syntacticMessage='0'\n",
    "                    list_syntactic = syntacticMessage.split(',')\n",
    "                    list_syntactic = list(map(float, list_syntactic))\n",
    "                    list_syntactic_tmp = pad_sequences([list_syntactic], maxlen=maxlen,\n",
    "                                                       padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "                    list_syntactic = list_syntactic_tmp[0]\n",
    "                    one_leaf_list.extend(list_syntactic)\n",
    "    cur.close()\n",
    "    return one_leaf_list\n",
    "\n",
    "\n",
    "def print_message(message):\n",
    "    t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(int(time.time())))\n",
    "    print('时间：' + t, end=\" \", flush=True)\n",
    "    print(message, flush=True)\n",
    "\n",
    "\n",
    "\n",
    "def repetition_list(a):\n",
    "    repetition = 6\n",
    "    c = []\n",
    "    for a_one in a:\n",
    "        b = []\n",
    "        for a_single in a_one:\n",
    "            for i in range(repetition):\n",
    "                b.append(a_single)\n",
    "        c.append(b)\n",
    "    return c\n",
    "\n",
    "\n",
    "def overlap_channel(a, b):\n",
    "    element_num = 0\n",
    "    height = 0\n",
    "    width = 0\n",
    "    if len(a.shape) == 2:\n",
    "        element_num = a.shape[0]\n",
    "        height = 1\n",
    "        width = a.shape[1]\n",
    "    elif len(a.shape) == 3:\n",
    "        element_num = a.shape[0]\n",
    "        height = a.shape[1]\n",
    "        width = a.shape[2]\n",
    "    else:\n",
    "        raise Exception(print_message('通道叠加 不支持这类维度:' + str(a.shape)))\n",
    "\n",
    "    a = a.reshape(element_num, 1, height, width)\n",
    "    b = b.reshape(element_num, 1, height, width)\n",
    "    a = a.transpose(1, 0, 2, 3)\n",
    "    b = b.transpose(1, 0, 2, 3)\n",
    "    c = np.vstack((a, b))\n",
    "    c = c.transpose(1, 2, 3, 0)\n",
    "    return c\n",
    "\n",
    "\n",
    "def check_list_exist(A, B):\n",
    "    exist = False\n",
    "    for row in A:\n",
    "        if (row == B).all():\n",
    "            exist = True\n",
    "    return exist\n",
    "\n",
    "\n",
    "def get_convert_data(A, B):\n",
    "    C = []\n",
    "    for A_one in A:\n",
    "        for B_one in B:\n",
    "            if (np.array(A_one) == np.array(B_one)).all():\n",
    "                C.append(A_one)\n",
    "    return C\n",
    "\n",
    "\n",
    "def cosDist(A, B):\n",
    "    return pdist(np.vstack([A, B]), 'cosine')  # np.sqrt(sum(np.power((A - B), 2)))\n",
    "\n",
    "\n",
    "def get_normal_data(data_point, data_lable):\n",
    "    neigh_num_a = 5\n",
    "    diff_rate_b = 1\n",
    "    can_stop = 0.99\n",
    "\n",
    "    # print_message(len(data_point))\n",
    "\n",
    "    abnormal_old = []\n",
    "    abnormal_new_ = []\n",
    "    for i in range(0, 100):\n",
    "        for one_point, one_lable in zip(data_point, data_lable):\n",
    "            neighbour_point = []\n",
    "            neighbour_lable = []\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(data_point, data_lable):\n",
    "                if check_list_exist(abnormal_new_, one_point_neighbour) or (one_point_neighbour == one_point).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    this_point_distance = cosDist(one_point_neighbour, one_point)\n",
    "                    if len(neighbour_point) >= neigh_num_a \\\n",
    "                            and this_point_distance < neighbour_point[neigh_num_a - 1] \\\n",
    "                            or len(neighbour_point) < neigh_num_a:\n",
    "                        if len(neighbour_point) < neigh_num_a:\n",
    "                            neighbour_point.append(this_point_distance)\n",
    "                            neighbour_lable.append(one_lable_neighbour)\n",
    "                        else:\n",
    "                            neighbour_point[neigh_num_a - 1] = (this_point_distance)\n",
    "                            neighbour_lable[neigh_num_a - 1] = (one_lable_neighbour)\n",
    "                        # 按照距离排序\n",
    "                        zipped = zip(neighbour_point, neighbour_lable)\n",
    "                        sort_zipped = sorted(zipped)\n",
    "                        result = zip(*sort_zipped)\n",
    "                        neighbour_point, neighbour_lable = [list(x) for x in result]\n",
    "\n",
    "            same_num = 0\n",
    "            diff_num = 0\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(neighbour_point[0:neigh_num_a],\n",
    "                                                                neighbour_lable[0:neigh_num_a]):\n",
    "                if one_lable_neighbour != one_lable:\n",
    "                    diff_num += 1\n",
    "                else:\n",
    "                    same_num += 1\n",
    "            diff_rate = diff_num / (same_num + diff_num)\n",
    "            if diff_rate >= diff_rate_b:\n",
    "                abnormal_new_.append([one_point])\n",
    "        if max(len(abnormal_new_), len(abnormal_old)) != 0 and \\\n",
    "                len(get_convert_data(abnormal_new_, abnormal_old)) / max(len(abnormal_new_),\n",
    "                                                                         len(abnormal_old)) >= can_stop:\n",
    "            break\n",
    "        else:\n",
    "            abnormal_old = abnormal_new_[:]\n",
    "        # print_message('单次noise数据量：' + str(len(abnormal_old)))\n",
    "\n",
    "    data_point_clear = []\n",
    "    data_lable_clear = []\n",
    "\n",
    "    for one_point, one_lable in zip(data_point, data_lable):\n",
    "        if (not check_list_exist(abnormal_old, one_point)):\n",
    "            data_point_clear.append(one_point)\n",
    "            data_lable_clear.append(one_lable)\n",
    "    return data_point_clear, data_lable_clear, abnormal_old\n",
    "\n",
    "\n",
    "##仅支持0 - 1 分类\n",
    "def change_abnormal(data_point, data_lable):\n",
    "    neigh_num_a = 10\n",
    "    diff_rate_b = 1\n",
    "    can_stop = 0.99\n",
    "\n",
    "    # print_message(len(data_point))\n",
    "\n",
    "    abnormal_old = []\n",
    "    abnormal_new_ = []\n",
    "    for i in range(0, 100):\n",
    "        for one_point, one_lable in zip(data_point, data_lable):\n",
    "            neighbour_point = []\n",
    "            neighbour_lable = []\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(data_point, data_lable):\n",
    "                if check_list_exist(abnormal_new_, one_point_neighbour) or (one_point_neighbour == one_point).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    this_point_distance = cosDist(one_point_neighbour, one_point)\n",
    "                    if len(neighbour_point) >= neigh_num_a \\\n",
    "                            and this_point_distance < neighbour_point[neigh_num_a - 1] \\\n",
    "                            or len(neighbour_point) < neigh_num_a:\n",
    "                        if len(neighbour_point) < neigh_num_a:\n",
    "                            neighbour_point.append(this_point_distance)\n",
    "                            neighbour_lable.append(one_lable_neighbour)\n",
    "                        else:\n",
    "                            neighbour_point[neigh_num_a - 1] = (this_point_distance)\n",
    "                            neighbour_lable[neigh_num_a - 1] = (one_lable_neighbour)\n",
    "                        # 按照距离排序\n",
    "                        zipped = zip(neighbour_point, neighbour_lable)\n",
    "                        sort_zipped = sorted(zipped)\n",
    "                        result = zip(*sort_zipped)\n",
    "                        neighbour_point, neighbour_lable = [list(x) for x in result]\n",
    "\n",
    "            same_num = 0\n",
    "            diff_num = 0\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(neighbour_point[0:neigh_num_a],\n",
    "                                                                neighbour_lable[0:neigh_num_a]):\n",
    "                if one_lable_neighbour != one_lable:\n",
    "                    diff_num += 1\n",
    "                else:\n",
    "                    same_num += 1\n",
    "            diff_rate = diff_num / (same_num + diff_num)\n",
    "            if diff_rate >= diff_rate_b:\n",
    "                abnormal_new_.append([one_point])\n",
    "        if max(len(abnormal_new_), len(abnormal_old)) != 0 and \\\n",
    "                len(get_convert_data(abnormal_new_, abnormal_old)) / max(len(abnormal_new_),\n",
    "                                                                         len(abnormal_old)) >= can_stop:\n",
    "            break\n",
    "        else:\n",
    "            abnormal_old = abnormal_new_[:]\n",
    "        # print_message('单次noise数据量：' + str(len(abnormal_old)))\n",
    "\n",
    "    data_point_clear = []\n",
    "    data_lable_clear = []\n",
    "\n",
    "    for one_point, one_lable in zip(data_point, data_lable):\n",
    "        if (not check_list_exist(abnormal_old, one_point)):\n",
    "            data_point_clear.append(one_point)\n",
    "            data_lable_clear.append(one_lable)\n",
    "        else:\n",
    "            ##修改noise标签\n",
    "            data_point_clear.append(one_point)\n",
    "            if one_lable == 0:\n",
    "                data_lable_clear.append(1)\n",
    "            else:\n",
    "                data_lable_clear.append(0)\n",
    "    return data_point_clear, data_lable_clear, abnormal_old\n",
    "\n",
    "\n",
    "def clear_data(train_vec_tmp, train_labels, zero_index):\n",
    "    group_num = 300\n",
    "    train_vec_tmp_clear = []\n",
    "    train_labels_clear = []\n",
    "    abnormal_num = 0\n",
    "    for b in range(int(len(train_vec_tmp) / group_num) + 1):\n",
    "        begin = b * group_num + zero_index\n",
    "        end = (b + 1) * group_num\n",
    "        if end > len(train_vec_tmp):\n",
    "            end = len(train_vec_tmp)\n",
    "        train_vec_group_one, train_labels_group_one, abnormal_old = change_abnormal(train_vec_tmp[begin:end],\n",
    "                                                                                    train_labels[begin:end])\n",
    "        train_vec_tmp_clear.extend(train_vec_group_one)\n",
    "        train_labels_clear.extend(train_labels_group_one)\n",
    "        abnormal_num = abnormal_num + len(abnormal_old)\n",
    "    print_message('noise数据量：' + str(abnormal_num))\n",
    "    print_message('训练总数据量：' + str(len(train_vec_tmp)))\n",
    "    train_vec_tmp = train_vec_tmp_clear\n",
    "    train_labels = train_labels_clear\n",
    "    return train_vec_tmp, train_labels\n",
    "\n",
    "\n",
    "def play_title_message():\n",
    "    playsound('D:/python/log_predict/file/结束提示.mp3')\n",
    "\n",
    "\n",
    "def get_tree_model_together(struct_vec_length_one,semantic_vec_length_one,syntatic_vec_length_one\n",
    "                            ,num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        keras.layers.InputLayer(struct_vec_length_one)\n",
    "    ])\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(semantic_vec_length_one, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(syntatic_vec_length_one, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model3.output, model2.output])\n",
    "    x = Dropout(0.4)(model_together)\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    final_output = Dense(24, activation='relu')(x)\n",
    "    model_together = Model(inputs=[model3.input, model2.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "\n",
    "\n",
    "def seperate_data(struct_leaf_vec, semantic_leaf_vec, syntatic_leaf_vec,\n",
    "                  lable, leaf,seed):\n",
    "    struct_train_vec, struct_test_vec, \\\n",
    "    semantic_train_vec, semantic_test_vec, \\\n",
    "    syntatic_train_vec, syntatic_test_vec, \\\n",
    "    struct_train_labels, struct_test_labels, \\\n",
    "    struct_train_leaf, struct_test_leaf = train_test_split(\n",
    "        struct_leaf_vec, semantic_leaf_vec, syntatic_leaf_vec,\n",
    "        lable, leaf,\n",
    "        test_size=0.2,\n",
    "        random_state=seed)\n",
    "    struct_verify_vec, struct_test_vec, \\\n",
    "    semantic_verify_vec, semantic_test_vec, \\\n",
    "    syntatic_verify_vec, syntatic_test_vec, \\\n",
    "    struct_verify_labels, struct_test_labels, \\\n",
    "    struct_verify_leaf, struct_test_leaf = train_test_split(\n",
    "        struct_test_vec, semantic_test_vec, syntatic_test_vec,\n",
    "        struct_test_labels,\n",
    "        struct_test_leaf,\n",
    "        test_size=0.5,\n",
    "        random_state=seed)\n",
    "    return struct_train_vec, struct_test_vec, \\\n",
    "           semantic_train_vec, semantic_test_vec, \\\n",
    "           syntatic_train_vec, syntatic_test_vec, \\\n",
    "           struct_train_labels, struct_test_labels, \\\n",
    "           struct_verify_vec, semantic_verify_vec, \\\n",
    "           syntatic_verify_vec, struct_verify_labels, struct_verify_leaf\n",
    "\n",
    "def get_lable(conn, seq):\n",
    "    seq_db = 0\n",
    "    leaf = ''\n",
    "    lable = 0\n",
    "    sqli = \"select seq,leaf,CASE WHEN logNum > 0 THEN  1  ELSE 0 END from data_model_2 where seq=\" + seq\n",
    "    cur = conn.cursor()\n",
    "    result = cur.execute(sqli)\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        seq_db = oneRow[0]\n",
    "        leaf = oneRow[1]\n",
    "        lable = oneRow[2]\n",
    "    cur.close()\n",
    "    return seq_db, leaf, lable\n",
    "\n",
    "\n",
    "\n",
    "def get_feature_from_file(file_name, node_all, node_vec):\n",
    "    with open(file_name, 'r') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            one_row = line.split(' ')\n",
    "            if len(one_row) > 2:\n",
    "                node_all.append(one_row[0])\n",
    "                one_row[len(one_row) - 1] = one_row[len(one_row) - 1].replace('\\n', '')\n",
    "                one_row = list(map(float, one_row))\n",
    "                node_vec.append(list(map(float, one_row[1:])))\n",
    "            line = file.readline()\n",
    "    return node_all, node_vec\n",
    "\n",
    "\n",
    "def convert_metric_to_list(project_name, feature_type, percentage, metric_type, tool_type):\n",
    "    one_row = []\n",
    "    one_row.append(project_name)\n",
    "    one_row.append(feature_type)\n",
    "    one_row.append(percentage)\n",
    "    one_row.append(metric_type)\n",
    "    one_row.append(tool_type)\n",
    "    return one_row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_call_graph(graph_leng, num_word, embedding_dim):\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Convolution1D(input_shape=(graph_leng, 1), filters=32, kernel_size=5, strides=1, padding='same',\n",
    "                      activation='relu', name='call-message'),\n",
    "        MaxPooling1D(pool_size=2, strides=2, padding='same', ),\n",
    "        Convolution1D(64, 5, strides=1, padding='same', activation='relu'),\n",
    "        MaxPooling1D(2, 2, 'same'),\n",
    "        Flatten(),\n",
    "        Dense(69, activation='relu'),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model3\n",
    "\n",
    "def get_method_name_model(method_name_x, method_name_y, num_word, embedding_dim):\n",
    "\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        keras.layers.Masking(mask_value=0, input_shape=(method_name_x, method_name_y)),\n",
    "        LSTM(units=8,activation='sigmoid'),\n",
    "        #         LSTM(units=32, activation='tanh'),\n",
    "        #         Flatten(),\n",
    "        Dense(53, activation='relu'),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model1\n",
    "\n",
    "def get_tree_model_together(semantic_vec_length_chain,syntatic_vec_length_chain,num_word,embedding_dim):\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(semantic_vec_length_chain, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(syntatic_vec_length_chain, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        #         LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        #         Flatten(),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model3.output, model2.output])\n",
    "    x = Dropout(0.2)(model_together)\n",
    "    final_output = Dense(43, activation='relu')(x)\n",
    "    model_together = Model(inputs=[model3.input, model2.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "\n",
    "def get_method_name_call_graph_model(method_dem_x,method_dem_y,call_graph_len,num_word,embedding_dim):\n",
    "    method_name_model = get_method_name_model(method_dem_x,method_dem_y,  num_word, embedding_dim)\n",
    "    call_graph_model = get_call_graph(call_graph_len,  num_word, embedding_dim)\n",
    "    m_j_merged_a = keras.layers.concatenate([method_name_model.output,call_graph_model.output],axis=-1)\n",
    "    dense1_a = Dense(4,activation='tanh')(m_j_merged_a)\n",
    "    final_output = Dropout(0.2)(dense1_a)\n",
    "    model_together_final = Model(inputs=[ method_name_model.input,call_graph_model.input],outputs=final_output)\n",
    "    return model_together_final\n",
    "\n",
    "\n",
    "def get_featrue_and_label(project_name):\n",
    "    # 1-语法 2-语义 3-结合(本树+调用关系)\n",
    "    # 4-语法（新）5-纯调用关系 6-结合(本树)\n",
    "    select_model = 6\n",
    "    semantic_vec_length = 50\n",
    "    tree_deep=1\n",
    "    semantic_vec_length_chain =  semantic_vec_length* tree_deep\n",
    "    syntatic_vec_length_chain = 30 * tree_deep\n",
    "    # 获取每个叶子节点\n",
    "    node = []\n",
    "    lable = []\n",
    "    semantic_leaf_one_node = []\n",
    "    semantic_leaf_vec = []\n",
    "    syntatic_leaf_vec = []\n",
    "    theme_leaf_vec = []\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    sqli = \"select seq,(select method_name_emb from data_model_1 where seq=a.methodSeq),\" \\\n",
    "           \"CASE WHEN logNum > 0 THEN  1  ELSE 0 END \" \\\n",
    "           \"from  data_model_2 a where  methodSeq in \" \\\n",
    "           \"(select seq from data_model_1 where projectName='\"+project_name+\"') \"\n",
    "    result = cur.execute(sqli)\n",
    "    #  and methodSeq<59900\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        node.append(oneRow[0])\n",
    "        theme_leaf_vec.append(eval(oneRow[1]))\n",
    "        lable.append(oneRow[2])\n",
    "    print_message(\"获取节点特征\")\n",
    "    for one_leaf in node:\n",
    "        vec_chain = []\n",
    "        for i in range(1, 3):\n",
    "            if i == 1:\n",
    "                maxlen = semantic_vec_length_chain\n",
    "                vec_chain = []\n",
    "                vec_chain.append(one_leaf)\n",
    "#                 vec_chain = get_parent(vec_chain)\n",
    "            elif i==2:\n",
    "                maxlen = syntatic_vec_length_chain\n",
    "                vec_chain = []\n",
    "                vec_chain.append(one_leaf)\n",
    "                #vec_chain = get_parent(vec_chain)\n",
    "\n",
    "            # 根据list中的记录查询节点记录，组合向量\n",
    "            one_leaf_vec = get_chair_vec(vec_chain, i,maxlen//tree_deep)\n",
    "            # 划分为等长向量\n",
    "            one_leaf_vec_padding_tmp = pad_sequences([one_leaf_vec], maxlen=maxlen,\n",
    "                                                     padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "            one_leaf_vec_padding_tmp = one_leaf_vec_padding_tmp[0]\n",
    "\n",
    "            if i == 1:\n",
    "                semantic_leaf_vec.append(one_leaf_vec_padding_tmp)\n",
    "            elif i==2:\n",
    "                syntatic_leaf_vec.append(one_leaf_vec_padding_tmp)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return node,semantic_leaf_vec,syntatic_leaf_vec,theme_leaf_vec,lable\n",
    "\n",
    "\n",
    "def get_feature_from_file(file_name, node_all, node_vec):\n",
    "    with open(file_name, 'r') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            one_row = line.split(' ')\n",
    "            if len(one_row) > 2:\n",
    "                node_all.append(one_row[0])\n",
    "                one_row[len(one_row) - 1] = one_row[len(one_row) - 1].replace('\\n', '')\n",
    "                one_row = list(map(float, one_row))\n",
    "                node_vec.append(list(map(float, one_row[1:])))\n",
    "            line = file.readline()\n",
    "    return node_all, node_vec\n",
    "\n",
    "\n",
    "\n",
    "def build_data(node,semantic_vec,edge,lable):\n",
    "    features_matrix_tmp=[]\n",
    "    for one_node,feature_value in zip(node,semantic_vec):\n",
    "        features_matrix_tmp.append(feature_value)\n",
    "\n",
    "    features_matrix=np.array(features_matrix_tmp)\n",
    "    a = {}\n",
    "    index=0\n",
    "    for i in features_matrix:\n",
    "        a[index]=[int(i) for i in i.tolist()]\n",
    "        index+=1\n",
    "\n",
    "\n",
    "    max_feature = np.max([v for v_list in a.values() for v in v_list])\n",
    "    features_matrix = np.zeros(shape = (len(list(a.keys())), max_feature+1))\n",
    "\n",
    "    i = 0\n",
    "    for k, vs in tqdm(a.items()):\n",
    "        for v in vs:\n",
    "            features_matrix[i, v] = 1\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "    class data:\n",
    "        x=[]\n",
    "        y=[]\n",
    "        edge_index=[]\n",
    "        train_mask=[]\n",
    "        val_mask=[]\n",
    "        test_mask=[]\n",
    "\n",
    "    data=data()\n",
    "    node=node\n",
    "    label=lable\n",
    "    edge=edge\n",
    "    feature=[]\n",
    "\n",
    "    feature = features_matrix\n",
    "    # for i in node:\n",
    "    #     feature.append(seq_feature[i])\n",
    "\n",
    "    train_rate=0.6\n",
    "    val_rate=0.2\n",
    "    test_rate=0.2\n",
    "    number=0\n",
    "    node_new={}\n",
    "    for i in node:\n",
    "        node_new[i]=number\n",
    "        number=number+1\n",
    "    data.x=torch.Tensor(feature)\n",
    "    edge_new=[[],[]]\n",
    "    for i in edge:\n",
    "        edge_new[0].append(node_new[i[0]])\n",
    "        edge_new[1].append(node_new[i[1]])\n",
    "\n",
    "    data.edge_index=torch.LongTensor(edge_new)\n",
    "\n",
    "    data.y=torch.LongTensor(label)\n",
    "    train_num=int(len(node)*train_rate)\n",
    "    train_mask=[ False for i in node]\n",
    "    while train_num>0:\n",
    "        index=0\n",
    "        for i in node:\n",
    "            if train_num>0 and random.random()>=0.5 and not train_mask[index]:\n",
    "                train_mask[index]=True\n",
    "                train_num=train_num-1\n",
    "            index=index+1\n",
    "    data.train_mask=train_mask\n",
    "    test_num=int(len(node)*test_rate)\n",
    "\n",
    "    test_mask=[ False for i in node]\n",
    "    while test_num>0:\n",
    "        index=0\n",
    "        for i,j in zip(test_mask,train_mask):\n",
    "            if test_num>0 and not i and not j:\n",
    "                if random.random()>=0.5:\n",
    "                    test_mask[index]=True\n",
    "                    test_num=test_num-1\n",
    "            index=index+1\n",
    "\n",
    "    data.test_mask=test_mask\n",
    "    val_num=int(len(node)*val_rate)\n",
    "\n",
    "    val_mask=[ False for i in node]\n",
    "    while val_num>0:\n",
    "        index=0\n",
    "        for i,j,z in zip(val_mask,train_mask,test_mask):\n",
    "            if val_num>0 and not i and not j and not z:\n",
    "                if random.random()>=0.5:\n",
    "                    val_mask[index]=True\n",
    "                    val_num=val_num-1\n",
    "            index=index+1\n",
    "    data.val_mask=val_mask\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_node_indices(G, ids):\n",
    "    # find the indices of the nodes\n",
    "    node_ids = np.asarray(ids)\n",
    "    flat_node_ids = node_ids.reshape(-1)\n",
    "\n",
    "    flat_node_indices = G.node_ids_to_ilocs(flat_node_ids) # in-built function makes it really easy\n",
    "    # back to the original shape\n",
    "    node_indices = flat_node_indices.reshape(1, len(node_ids)) # add 1 extra dimension\n",
    "\n",
    "    return node_indices\n",
    "\n",
    "def splite_data(vec_x_sem,vec_x_syn,vec_x_graph,vec_theme,\n",
    "                vec_y,mask):\n",
    "    vec_sem=[]\n",
    "    vec_syn=[]\n",
    "    vec_graph=[]\n",
    "    vec_method_theme=[]\n",
    "    lable=[]\n",
    "    for i,j,h,w,g,z  in zip(vec_x_sem,vec_x_syn,\n",
    "                              vec_x_graph,vec_theme,\n",
    "                              vec_y,\n",
    "                              mask):\n",
    "        if z:\n",
    "            vec_sem.append(i)\n",
    "            vec_syn.append(j)\n",
    "            vec_graph.append(h)\n",
    "            vec_method_theme.append(w)\n",
    "            lable.append(g)\n",
    "    return vec_sem,vec_syn,vec_graph,vec_method_theme,lable\n",
    "def get_method_name_emb(seq):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select method_name_emb from data_model_1 where seq=( select methodSeq from data_model_2 where seq=\" + str(seq) + \")\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    emb=oneRow[0]\n",
    "    cur.close()\n",
    "    return eval(emb)\n",
    "\n",
    "\n",
    "# 得到多分枝模型\n",
    "def get_graph_model_4(graph_leng,syntactic_len,method_name_x, method_name_y,\n",
    "                      num_word,embedding_dim):\n",
    "#     model1 = tf.keras.models.Sequential([\n",
    "#         Input(shape=(semantic_vec,1)),\n",
    "#         (Flatten()),\n",
    "#         (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "#         (keras.layers.GlobalMaxPool1D()),\n",
    "#     ])\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_leng,1)),\n",
    "        Flatten(),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model4 = tf.keras.models.Sequential([\n",
    "        keras.layers.Masking(mask_value=0, input_shape=(method_name_x, method_name_y)),\n",
    "        LSTM(units=8, activation='tanh', return_sequences=True),\n",
    "        keras.layers.GlobalMaxPool1D(),\n",
    "        \n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([ model2.output, model3.output,model4.output])\n",
    "    model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "    x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "    x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model2.input, model3.input,model4.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "def get_graph_model_3(graph_leng,syntactic_len,\n",
    "                      num_word,embedding_dim):\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_leng,1)),\n",
    "        Flatten(),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([ model2.output, model3.output])\n",
    "\n",
    "    model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "    x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "    x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model2.input, model3.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "def get_graph_model_2(semantic_vec,syntactic_len,\n",
    "                      num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(semantic_vec,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model1.output, model3.output])\n",
    "    model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "\n",
    "    x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "    x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model1.input,model3.input], outputs=final_output)\n",
    "    return model_together\n",
    "def get_graph_model_1(semantic_vec,\n",
    "                      num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(semantic_vec,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        (Dropout(0.2)),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model1\n",
    "def get_graph_model_gcn(graph_vec_len,\n",
    "                        num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_vec_len,1)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        (Dropout(0.2)),\n",
    "        Dense(2, activation='softmax'),\n",
    "\n",
    "    ])\n",
    "    return model1\n",
    "def save_performance(content,ac,pr,re,f1):\n",
    "    f = open(\"print-message.txt\", mode='a')\n",
    "    f.write(str(datetime.datetime.now())+'\\t'+ content+' \\t'+str(ac)+\n",
    "            '\\t'+str(pr)+ '\\t'+str(re)+'\\t'+ str(f1)+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def convert_2_hot_vec(list):\n",
    "    key = []\n",
    "    for i in list:\n",
    "        tmp = [int(i) for i in i[::2]]\n",
    "        for j in tmp:\n",
    "            if j not in key:\n",
    "                key.append(j)\n",
    "        # print(i[1::2])\n",
    "    # 得到one-hot表格\n",
    "    key = sort(key)\n",
    "    feature_hot = []\n",
    "    for i in list:\n",
    "        key_tmp = i[::2]\n",
    "        num_tmp = i[1::2]\n",
    "        key_tmp = [int(i) for i in key_tmp]\n",
    "        num_tmp = [int(i) for i in num_tmp]\n",
    "        \n",
    "        feature_tmp = []\n",
    "        for j in key:\n",
    "            if j in key_tmp:\n",
    "                feature_tmp.append(num_tmp[key_tmp.index(j)])\n",
    "            else:\n",
    "                feature_tmp.append(0)\n",
    "        feature_hot.append(feature_tmp)\n",
    "    return key,feature_hot\n",
    "def plot_value(y_true, y_pred):\n",
    "    import sklearn as sk\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    balance_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision = sk.metrics.precision_score(y_true, y_pred)\n",
    "    recall = sk.metrics.recall_score(y_true, y_pred)\n",
    "    f1_value = sk.metrics.f1_score(y_true, y_pred)\n",
    "    return balance_accuracy, precision, recall, f1_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca1d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epoch = 100  # 训练周期\n",
    "feature_length = \"10\"\n",
    "one_project = 'dubbo-3.0'\n",
    "num_word=50000\n",
    "embedding_dim=12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e51ed",
   "metadata": {},
   "source": [
    "# 获取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e8401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间：2024-08-16 02:37:06 获取节点特征\n"
     ]
    }
   ],
   "source": [
    "\n",
    "node, semantic_vec, \\\n",
    "syntatic_vec,\\\n",
    "leaf_method_name_vec,lable = get_featrue_and_label(one_project)\n",
    "\n",
    "randomValue=int(random.random()*100)\n",
    "node, semantic_vec, \\\n",
    "syntatic_vec,\\\n",
    "leaf_method_name_vec,lable=shuffle_data(node, semantic_vec,\n",
    "                                        syntatic_vec,\n",
    "                                        leaf_method_name_vec,lable,randomValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32cec99d",
   "metadata": {},
   "source": [
    "# 特征矩阵化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94148add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# key, features_matrix_syn=convert_2_hot_vec(syntatic_vec)\n",
    "# index=0\n",
    "# features_syn={}\n",
    "# for i,j in zip(node,features_matrix_syn):\n",
    "#     features_syn[index]=j\n",
    "#     index+=1\n",
    "\n",
    "key, features_matrix_sem=convert_2_hot_vec(semantic_vec)\n",
    "index=0\n",
    "features_sem={}\n",
    "for i,j in zip(node,features_matrix_sem):\n",
    "    features_sem[index]=j\n",
    "    index+=1\n",
    "\n",
    "# features_matrix_com=[]\n",
    "# for i,j,k in zip(node, features_matrix_sem, features_matrix_syn):\n",
    "#     tmp=[]\n",
    "#     tmp.extend(j)\n",
    "#     tmp.extend(k)\n",
    "#     features_matrix_com.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a565397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number=0\n",
    "node_new={}\n",
    "for i in node:\n",
    "    node_new[i]=number\n",
    "    number=number+1\n",
    "\n",
    "edge=[]#[[28,16],[16,85],[85,46],[28,16],[16,85],[85,46],[16,85],[85,46]]\n",
    "\n",
    "conn = get_connection()\n",
    "\n",
    "cur = conn.cursor()\n",
    "sqli = \"select parentId,seq \" \\\n",
    "       \"from data_model_2 \" \\\n",
    "       \"where methodSeq in \" \\\n",
    "       \"(select seq from data_model_1 where projectName='\"+one_project+\"') \" \\\n",
    "       \"and parentId<>0  \" \\\n",
    "       \"union \" \\\n",
    "       \"select callBlockSeq,calledBlockSeq \" \\\n",
    "       \"from call_graph_data \" \\\n",
    "       \"where callMethodSeq in \" \\\n",
    "       \"(select seq from data_model_1 where projectName='\"+one_project+\"')   \"\n",
    "result = cur.execute(sqli)\n",
    "#  and methodSeq<59900  and callMethodSeq<59900 and calledMethodSeq<59900\n",
    "for i in range(result):\n",
    "    oneRow = cur.fetchone()\n",
    "    if oneRow[0] in node_new.keys() and oneRow[1] in node_new.keys():\n",
    "        edge_tmp=[]\n",
    "        edge_tmp.append(node_new[oneRow[0]])\n",
    "        edge_tmp.append(node_new[oneRow[1]])\n",
    "        edge.append(edge_tmp)\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04c392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da80a39b",
   "metadata": {},
   "source": [
    "# 构建图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3969850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "node_features = pd.DataFrame(features_matrix_sem, index = features_sem.keys()) # into dataframe for StellarGraph\n",
    "\n",
    "edge = pd.DataFrame(edge,columns=['source', 'target']) # 将第一维度数据转为为行，第二维度数据转化为列，即 3 行 2 列，并设置列标签\n",
    "edge.columns = ['source', 'target']\n",
    "G = sg.StellarGraph(node_features, edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f93dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d417ed3f",
   "metadata": {},
   "source": [
    "# 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0714d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "targets = pd.DataFrame(data=None,columns=['id','name','ml_target']) # into dataframe for StellarGraph\n",
    "index=0\n",
    "for key,label_tmp in zip(features_sem.keys(),lable):\n",
    "    targets.loc[index] = [key,'-',label_tmp]\n",
    "    index=index+1\n",
    "targets.index = targets.id\n",
    "targets = targets.loc[features_sem.keys(), :]\n",
    "targets.index = targets.id.astype(str)\n",
    "train_pages, test_pages = train_test_split(targets, test_size=0.4,random_state=73)\n",
    "val_pages, test_pages = train_test_split(test_pages, test_size=0.5,random_state=73)\n",
    "target_encoding = LabelBinarizer()\n",
    "train_targets = target_encoding.fit_transform(np.int64(train_pages['ml_target']))\n",
    "val_targets = target_encoding.transform(np.int64(val_pages['ml_target']))\n",
    "test_targets = target_encoding.transform(np.int64(test_pages['ml_target']))\n",
    "\n",
    "# 得到gcn特征\n",
    "\n",
    "# Get the adjacency matrix\n",
    "A = G.to_adjacency_matrix(weighted=False)\n",
    "# Add self-connections\n",
    "A_t = A + scipy.sparse.diags(np.ones(A.shape[0]) - A.diagonal())\n",
    "# Degree matrix to the power of -1/2\n",
    "D_t = scipy.sparse.diags(np.power(np.array(A.sum(1)), -0.5).flatten(), 0)\n",
    "# Normalise the Adjacency matrix\n",
    "A_norm = A.dot(D_t).transpose().dot(D_t).todense()\n",
    "A_input = np.expand_dims(A_norm, 0)\n",
    "# Get indices\n",
    "train_indices = get_node_indices(G, np.int64(train_pages.index))\n",
    "val_indices = get_node_indices(G, np.int64(val_pages.index))\n",
    "test_indices = get_node_indices(G, np.int64(test_pages.index))\n",
    "\n",
    "# Expand dimensions\n",
    "features_input = np.expand_dims(features_matrix_sem, 0)\n",
    "A_input = np.expand_dims(A_norm, 0)\n",
    "\n",
    "y_train = np.expand_dims(train_targets, 0)\n",
    "y_val = np.expand_dims(val_targets, 0)\n",
    "y_test = np.expand_dims(test_targets, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f430e6",
   "metadata": {},
   "source": [
    "# 得到图特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b6591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 49721, 2149)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(1, 49721, 2149)]   0           []                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (1, 49721, 2149)     0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(1, 49721, 49721)]  0           []                               \n",
      "                                                                                                  \n",
      " graph_convolution (GraphConvol  (1, 49721, 32)      68800       ['dropout[0][0]',                \n",
      " ution)                                                           'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (1, 49721, 32)       0           ['graph_convolution[0][0]']      \n",
      "                                                                                                  \n",
      " graph_convolution_1 (GraphConv  (1, 49721, 32)      1056        ['dropout_1[0][0]',              \n",
      " olution)                                                         'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(1, None)]          0           []                               \n",
      "                                                                                                  \n",
      " gather_indices (GatherIndices)  (1, None, 32)       0           ['graph_convolution_1[0][0]',    \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (1, None, 1)         33          ['gather_indices[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 69,889\n",
      "Trainable params: 69,889\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 - 1211s - loss: 0.7142 - accuracy: 0.7728 - val_loss: 0.5075 - val_accuracy: 0.9226 - 1211s/epoch - 1211s/step\n",
      "Epoch 2/100\n",
      "1/1 - 497s - loss: 0.5433 - accuracy: 0.9188 - val_loss: 0.4502 - val_accuracy: 0.9226 - 497s/epoch - 497s/step\n",
      "Epoch 3/100\n",
      "1/1 - 478s - loss: 0.4983 - accuracy: 0.9202 - val_loss: 0.4133 - val_accuracy: 0.9223 - 478s/epoch - 478s/step\n",
      "Epoch 4/100\n",
      "1/1 - 473s - loss: 0.4498 - accuracy: 0.9203 - val_loss: 0.3749 - val_accuracy: 0.9225 - 473s/epoch - 473s/step\n",
      "Epoch 5/100\n",
      "1/1 - 544s - loss: 0.4024 - accuracy: 0.9203 - val_loss: 0.3381 - val_accuracy: 0.9225 - 544s/epoch - 544s/step\n",
      "Epoch 6/100\n",
      "1/1 - 533s - loss: 0.3622 - accuracy: 0.9203 - val_loss: 0.3100 - val_accuracy: 0.9216 - 533s/epoch - 533s/step\n",
      "Epoch 7/100\n",
      "1/1 - 548s - loss: 0.3373 - accuracy: 0.9201 - val_loss: 0.2931 - val_accuracy: 0.9209 - 548s/epoch - 548s/step\n",
      "Epoch 8/100\n",
      "1/1 - 542s - loss: 0.3141 - accuracy: 0.9191 - val_loss: 0.2831 - val_accuracy: 0.9172 - 542s/epoch - 542s/step\n",
      "Epoch 9/100\n",
      "1/1 - 535s - loss: 0.3041 - accuracy: 0.9169 - val_loss: 0.2714 - val_accuracy: 0.9181 - 535s/epoch - 535s/step\n",
      "Epoch 10/100\n",
      "1/1 - 541s - loss: 0.2911 - accuracy: 0.9172 - val_loss: 0.2577 - val_accuracy: 0.9184 - 541s/epoch - 541s/step\n",
      "Epoch 11/100\n",
      "1/1 - 540s - loss: 0.2779 - accuracy: 0.9186 - val_loss: 0.2471 - val_accuracy: 0.9207 - 540s/epoch - 540s/step\n",
      "Epoch 12/100\n",
      "1/1 - 549s - loss: 0.2668 - accuracy: 0.9195 - val_loss: 0.2406 - val_accuracy: 0.9216 - 549s/epoch - 549s/step\n",
      "Epoch 13/100\n",
      "1/1 - 561s - loss: 0.2579 - accuracy: 0.9198 - val_loss: 0.2372 - val_accuracy: 0.9219 - 561s/epoch - 561s/step\n",
      "Epoch 14/100\n",
      "1/1 - 602s - loss: 0.2583 - accuracy: 0.9204 - val_loss: 0.2327 - val_accuracy: 0.9220 - 602s/epoch - 602s/step\n",
      "Epoch 15/100\n",
      "1/1 - 579s - loss: 0.2519 - accuracy: 0.9206 - val_loss: 0.2273 - val_accuracy: 0.9218 - 579s/epoch - 579s/step\n",
      "Epoch 16/100\n",
      "1/1 - 607s - loss: 0.2498 - accuracy: 0.9203 - val_loss: 0.2228 - val_accuracy: 0.9213 - 607s/epoch - 607s/step\n",
      "Epoch 17/100\n",
      "1/1 - 612s - loss: 0.2495 - accuracy: 0.9203 - val_loss: 0.2201 - val_accuracy: 0.9211 - 612s/epoch - 612s/step\n",
      "Epoch 18/100\n",
      "1/1 - 572s - loss: 0.2361 - accuracy: 0.9207 - val_loss: 0.2209 - val_accuracy: 0.9207 - 572s/epoch - 572s/step\n",
      "Epoch 19/100\n",
      "1/1 - 579s - loss: 0.2351 - accuracy: 0.9194 - val_loss: 0.2227 - val_accuracy: 0.9202 - 579s/epoch - 579s/step\n",
      "Epoch 20/100\n",
      "1/1 - 577s - loss: 0.2339 - accuracy: 0.9210 - val_loss: 0.2227 - val_accuracy: 0.9205 - 577s/epoch - 577s/step\n",
      "Epoch 21/100\n",
      "1/1 - 564s - loss: 0.2286 - accuracy: 0.9192 - val_loss: 0.2187 - val_accuracy: 0.9212 - 564s/epoch - 564s/step\n",
      "Epoch 22/100\n",
      "1/1 - 576s - loss: 0.2287 - accuracy: 0.9208 - val_loss: 0.2144 - val_accuracy: 0.9212 - 576s/epoch - 576s/step\n",
      "Epoch 23/100\n",
      "1/1 - 547s - loss: 0.2281 - accuracy: 0.9196 - val_loss: 0.2110 - val_accuracy: 0.9216 - 547s/epoch - 547s/step\n",
      "Epoch 24/100\n",
      "1/1 - 526s - loss: 0.2213 - accuracy: 0.9209 - val_loss: 0.2073 - val_accuracy: 0.9221 - 526s/epoch - 526s/step\n",
      "Epoch 25/100\n",
      "1/1 - 535s - loss: 0.2169 - accuracy: 0.9205 - val_loss: 0.2036 - val_accuracy: 0.9222 - 535s/epoch - 535s/step\n",
      "Epoch 26/100\n",
      "1/1 - 540s - loss: 0.2174 - accuracy: 0.9208 - val_loss: 0.2006 - val_accuracy: 0.9224 - 540s/epoch - 540s/step\n",
      "Epoch 27/100\n",
      "1/1 - 558s - loss: 0.2148 - accuracy: 0.9211 - val_loss: 0.1984 - val_accuracy: 0.9228 - 558s/epoch - 558s/step\n",
      "Epoch 28/100\n",
      "1/1 - 581s - loss: 0.2168 - accuracy: 0.9214 - val_loss: 0.1969 - val_accuracy: 0.9231 - 581s/epoch - 581s/step\n",
      "Epoch 29/100\n",
      "1/1 - 591s - loss: 0.2130 - accuracy: 0.9209 - val_loss: 0.1955 - val_accuracy: 0.9237 - 591s/epoch - 591s/step\n",
      "Epoch 30/100\n",
      "1/1 - 585s - loss: 0.2119 - accuracy: 0.9212 - val_loss: 0.1939 - val_accuracy: 0.9236 - 585s/epoch - 585s/step\n",
      "Epoch 31/100\n",
      "1/1 - 580s - loss: 0.2103 - accuracy: 0.9228 - val_loss: 0.1921 - val_accuracy: 0.9239 - 580s/epoch - 580s/step\n",
      "Epoch 32/100\n",
      "1/1 - 586s - loss: 0.2067 - accuracy: 0.9226 - val_loss: 0.1907 - val_accuracy: 0.9241 - 586s/epoch - 586s/step\n",
      "Epoch 33/100\n",
      "1/1 - 571s - loss: 0.2076 - accuracy: 0.9219 - val_loss: 0.1884 - val_accuracy: 0.9245 - 571s/epoch - 571s/step\n",
      "Epoch 34/100\n",
      "1/1 - 621s - loss: 0.2030 - accuracy: 0.9229 - val_loss: 0.1863 - val_accuracy: 0.9253 - 621s/epoch - 621s/step\n",
      "Epoch 35/100\n"
     ]
    }
   ],
   "source": [
    "# 得到GCN模型\n",
    "# Build the model\n",
    "\n",
    "kernel_initializer=\"glorot_uniform\"\n",
    "bias = True\n",
    "bias_initializer=\"zeros\"\n",
    "n_features = features_input.shape[2]\n",
    "n_nodes = features_input.shape[1]\n",
    "# Initialise input layers\n",
    "x_features = Input(batch_shape=(1, n_nodes, n_features))\n",
    "print(x_features.shape)\n",
    "x_indices = Input(batch_shape=(1, None), dtype=\"int32\")\n",
    "x_adjacency = Input(batch_shape=(1, n_nodes, n_nodes))\n",
    "x_inp = [x_features, x_indices, x_adjacency]\n",
    "\n",
    "x = Dropout(0.5)(x_features)\n",
    "x = GraphConvolution(32, activation='relu',\n",
    "                     use_bias=True,\n",
    "                     kernel_initializer=kernel_initializer,\n",
    "                     bias_initializer=bias_initializer)([x, x_adjacency])\n",
    "x = Dropout(0.5)(x)\n",
    "x = GraphConvolution(32, activation='relu',\n",
    "                     use_bias=True,\n",
    "                     kernel_initializer=kernel_initializer,\n",
    "                     bias_initializer=bias_initializer)([x, x_adjacency])\n",
    "# x=LSTM(units=32, activation='tanh', return_sequences=True)(x)\n",
    "# x=(keras.layers.GlobalMaxPool1D())(x)\n",
    "# x=Flatten()(x)\n",
    "x = GatherIndices(batch_dims=1)([x, x_indices])\n",
    "\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[x_features, x_indices, x_adjacency], outputs=output)\n",
    "\n",
    "# Print out the summary\n",
    "model.summary()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=losses.binary_crossentropy,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "history = model.fit(\n",
    "    x = [features_input, train_indices, A_input], # 3 inputs - features matrix, train indices, normalised adjacency matrix\n",
    "    y = y_train,\n",
    "    batch_size = 16,\n",
    "    epochs=num_epoch,\n",
    "    validation_data=([features_input, val_indices, A_input], y_val),\n",
    "    verbose=2,\n",
    "    shuffle=False,\n",
    "#     callbacks=[es_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c78b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\database_package\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "embedding_model = Model(inputs=x_inp, outputs=model.layers[-2].output)\n",
    "\n",
    "# Get indices of all nodes\n",
    "all_indices = get_node_indices(G, np.int64(targets.index))\n",
    "\n",
    "#Get embeddings\n",
    "emb = embedding_model.predict([features_input, all_indices, A_input])\n",
    "\n",
    "u = umap.UMAP(random_state=42)\n",
    "umap_embs = u.fit_transform(emb[0])\n",
    "\n",
    "#Plot the embeddingsembe\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.scatterplot(x = umap_embs[:, 0], y = umap_embs[:, 1], hue = targets['ml_target']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0b12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fb920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad93574f",
   "metadata": {},
   "source": [
    "# 得到特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ff8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_x_sem=semantic_vec\n",
    "vec_x_syn=syntatic_vec\n",
    "vec_x_graph=emb[0]\n",
    "vec_theme=leaf_method_name_vec\n",
    "vec_y=np.int64(targets['ml_target']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef45c1d",
   "metadata": {},
   "source": [
    "# 特征划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 数据集切分\n",
    "print('数据集切分正常')\n",
    "all_indices = get_node_indices(G, np.int64(targets.index))\n",
    "\n",
    "train_mask=[]\n",
    "val_mask=[]\n",
    "test_mask=[]\n",
    "for i in all_indices[0]:\n",
    "    if i in train_indices[0]:\n",
    "        train_mask.append(True)\n",
    "    else:\n",
    "        train_mask.append(False)\n",
    "    if i in val_indices[0]:\n",
    "        val_mask.append(True)\n",
    "    else:\n",
    "        val_mask.append(False)\n",
    "    if i in test_indices[0]:\n",
    "        test_mask.append(True)\n",
    "    else:\n",
    "        test_mask.append(False)\n",
    "train_vec_sem,train_vec_syn, \\\n",
    "train_vec_graph,train_vec_theme,train_lable=splite_data \\\n",
    "    (semantic_vec,syntatic_vec,vec_x_graph,leaf_method_name_vec,\n",
    "     lable,train_mask)\n",
    "val_vec_sem,val_vec_syn, \\\n",
    "val_vec_graph,val_vec_theme,val_lable=splite_data \\\n",
    "    (semantic_vec,syntatic_vec,vec_x_graph,leaf_method_name_vec,\n",
    "     lable,val_mask)\n",
    "test_vec_sem,test_vec_syn, \\\n",
    "test_vec_graph,test_vec_theme,test_lable=splite_data \\\n",
    "    (semantic_vec,syntatic_vec,vec_x_graph,leaf_method_name_vec,\n",
    "     lable,test_mask)\n",
    "train_lable = tf.keras.utils.to_categorical(train_lable, num_classes=2)\n",
    "test_lable = tf.keras.utils.to_categorical(test_lable, num_classes=2)\n",
    "val_lable = tf.keras.utils.to_categorical(val_lable, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09027bf",
   "metadata": {},
   "source": [
    "# 结构+语法+语义+主题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b039f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 模型构建\n",
    "print('模型构建')\n",
    "model_graph_4 = get_graph_model_4(len(train_vec_graph[0]),\n",
    "                                len(train_vec_syn[0]),\n",
    "                                len(train_vec_theme[0]),\n",
    "                                len(train_vec_theme[0][0]),num_word,embedding_dim)\n",
    "model_graph_4.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 训练多分枝模型\n",
    "# 模型训练\n",
    "#录入组合的特征，模型训练\n",
    "history_4 = model_graph_4.fit([np.array(train_vec_graph),\n",
    "                           np.array(train_vec_syn),np.array(train_vec_theme)],\n",
    "                              np.array(train_lable),\n",
    "                          validation_data=([np.array(val_vec_graph),\n",
    "                                            np.array(val_vec_syn),\n",
    "                                            np.array(val_vec_theme)],\n",
    "                                           np.array(val_lable)),\n",
    "                          epochs=num_epoch, verbose=2)\n",
    "\n",
    "# 输出结果\n",
    "# 模型评估\n",
    "predict_one_hot = model_graph_4.predict([np.array(test_vec_graph),\n",
    "                                       np.array(test_vec_syn),\n",
    "                                      np.array(test_vec_theme)])\n",
    "predict_number = [np.argmax(one_hot) for one_hot in predict_one_hot]\n",
    "verify_number = [np.argmax(one_hot) for one_hot in test_lable]\n",
    "\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance('主题+语法+语义+gcn', balance_accuracy, precision, recall, f1_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e2b10",
   "metadata": {},
   "source": [
    "# 结构+语法+语义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_graph_3 = get_graph_model_3(len(train_vec_graph[0]),\n",
    "                                len(train_vec_syn[0]),num_word,embedding_dim)\n",
    "model_graph_3.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 训练多分枝模型\n",
    "# 模型训练\n",
    "#录入组合的特征，模型训练\n",
    "history_3 = model_graph_3.fit([np.array(train_vec_graph),\n",
    "                           np.array(train_vec_syn)],np.array(train_lable),\n",
    "                          validation_data=([np.array(val_vec_graph),\n",
    "                                            np.array(val_vec_syn)],np.array(val_lable)),\n",
    "                          epochs=num_epoch, verbose=2)\n",
    "\n",
    "# 输出结果\n",
    "# 模型评估\n",
    "predict_one_hot = model_graph_3.predict([np.array(test_vec_graph),\n",
    "                                       np.array(test_vec_syn)])\n",
    "predict_number = [np.argmax(one_hot) for one_hot in predict_one_hot]\n",
    "verify_number = [np.argmax(one_hot) for one_hot in test_lable]\n",
    "\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance('语法+语义+gcn', balance_accuracy, precision, recall, f1_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b621a3",
   "metadata": {},
   "source": [
    "# 语法+语义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph_2 = get_graph_model_2(len(train_vec_sem[0]),\n",
    "                                  len(train_vec_syn[0]),num_word,embedding_dim)\n",
    "model_graph_2.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 训练多分枝模型\n",
    "# 模型训练\n",
    "#录入组合的特征，模型训练\n",
    "history_2 = model_graph_2.fit([np.array(train_vec_sem),\n",
    "                               np.array(train_vec_syn)],np.array(train_lable),\n",
    "                              validation_data=([np.array(val_vec_sem),\n",
    "                                                np.array(val_vec_syn)],np.array(val_lable)),\n",
    "                              epochs=num_epoch, verbose=2)\n",
    "\n",
    "# 输出结果\n",
    "# 模型评估\n",
    "predict_one_hot = model_graph_2.predict([np.array(test_vec_sem),\n",
    "                                         np.array(test_vec_syn)])\n",
    "predict_number = [np.argmax(one_hot) for one_hot in predict_one_hot]\n",
    "verify_number = [np.argmax(one_hot) for one_hot in test_lable]\n",
    "\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance('语法+语义', balance_accuracy, precision, recall, f1_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2434b",
   "metadata": {},
   "source": [
    "# 语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph_1 = get_graph_model_1(len(train_vec_syn[0]),num_word,embedding_dim)\n",
    "model_graph_1.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 模型训练\n",
    "#录入组合的特征，模型训练\n",
    "history_1 = model_graph_1.fit([np.array(train_vec_syn)],np.array(train_lable),\n",
    "                              validation_data=([np.array(val_vec_syn)],np.array(val_lable)),\n",
    "                              epochs=num_epoch, verbose=2)\n",
    "\n",
    "# 输出结果\n",
    "# 模型评估\n",
    "predict_one_hot = model_graph_1.predict([np.array(test_vec_syn)])\n",
    "predict_number = [np.argmax(one_hot) for one_hot in predict_one_hot]\n",
    "verify_number = [np.argmax(one_hot) for one_hot in test_lable]\n",
    "\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance('语法', balance_accuracy, precision, recall, f1_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_object(train_vec_sem,\"train_vec_sem\")\n",
    "# save_object(train_vec_syn,\"train_vec_syn\")\n",
    "# save_object(train_vec_graph,\"train_vec_graph\")\n",
    "# save_object(train_vec_theme,\"train_vec_theme\")\n",
    "# save_object(train_lable,\"train_lable\")\n",
    "# save_object(val_vec_sem,\"val_vec_sem\")\n",
    "# save_object(val_vec_syn,\"val_vec_syn\")\n",
    "# save_object(val_vec_graph,\"val_vec_graph\")\n",
    "# save_object(val_vec_theme,\"val_vec_theme\")\n",
    "# save_object(val_lable,\"val_lable\")\n",
    "# save_object(test_vec_sem,\"test_vec_sem\")\n",
    "# save_object(test_vec_syn,\"test_vec_syn\")\n",
    "# save_object(test_vec_graph,\"test_vec_graph\")\n",
    "# save_object(test_vec_theme,\"test_vec_theme\")\n",
    "# save_object(test_lable,\"test_lable\")\n",
    "# save_object(train_lable,\"train_lable\")\n",
    "# save_object(test_lable,\"test_lable\")\n",
    "# save_object(val_lable,\"val_lable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591350a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_sem=get_object(\"train_vec_sem\")\n",
    "# train_vec_syn=get_object(\"train_vec_syn\")\n",
    "# train_vec_graph=get_object(\"train_vec_graph\")\n",
    "# train_vec_theme=get_object(\"train_vec_theme\")\n",
    "# train_lable=get_object(\"train_lable\")\n",
    "# val_vec_sem=get_object(\"val_vec_sem\")\n",
    "# val_vec_syn=get_object(\"val_vec_syn\")\n",
    "# val_vec_graph=get_object(\"val_vec_graph\")\n",
    "# val_vec_theme=get_object(\"val_vec_theme\")\n",
    "# val_lable=get_object(\"val_lable\")\n",
    "# test_vec_sem=get_object(\"test_vec_sem\")\n",
    "# test_vec_syn=get_object(\"test_vec_syn\")\n",
    "# test_vec_graph=get_object(\"test_vec_graph\")\n",
    "# test_vec_theme=get_object(\"test_vec_theme\")\n",
    "# test_lable=get_object(\"test_lable\")\n",
    "# train_lable=get_object(\"train_lable\")\n",
    "# test_lable=get_object(\"test_lable\")\n",
    "# val_lable=get_object(\"val_lable\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b9f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
