{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27bec1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作者：常晓松\n",
    "# 作用：日志等级预测\n",
    "# 时间： 2024/7/9 19:31\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import diags\n",
    "import scipy\n",
    "import stellargraph as sg\n",
    "from playsound import playsound\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import *\n",
    "from keras import Model, Input\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from numpy import sort\n",
    "from random import random\n",
    "from stellargraph.layer.gcn import GraphConvolution, GatherIndices\n",
    "from keras.layers import Lambda\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.spatial.distance import pdist\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "import pymysql\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split  # 出留法\n",
    "from keras.layers import Convolution1D, MaxPooling1D, LSTM\n",
    "import random\n",
    "import numpy\n",
    "import math\n",
    "from statistics import mean\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def get_ave_and_standard_error(data_list):\n",
    "\n",
    "    SD_P = numpy.std(data_list, ddof=0)\n",
    "    n = len(data_list)\n",
    "\n",
    "    ave = mean(data_list)\n",
    "    standard_error = SD_P / math.sqrt(n)\n",
    "    return round(ave,3),round(standard_error,3)\n",
    "\n",
    "def get_connection():\n",
    "    global conn\n",
    "    host = \"127.0.0.1\"\n",
    "    user = \"root\"\n",
    "    password = 'chang123'\n",
    "    db = 'predict_log_final'\n",
    "    conn = pymysql.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        db=db,\n",
    "        charset='utf8',\n",
    "        # autocommit=True,    # 如果插入数据，， 是否自动提交? 和conn.commit()功能一致。\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "def shuffle_data(node, semantic_vec, syntatic_vec,leaf_method_name_vec,lable, seed):\n",
    "    c = list(zip(node, semantic_vec, syntatic_vec,leaf_method_name_vec,lable))  # 将a,b整体作为一个zip,每个元素一一对应后打乱\n",
    "    random.seed(seed)\n",
    "    random.shuffle(c)  # 打乱c\n",
    "    node[:], semantic_vec[:], syntatic_vec[:],leaf_method_name_vec[:],lable[:] = zip(*c)  # 将打乱的c解开\n",
    "    return node, semantic_vec, syntatic_vec,leaf_method_name_vec,lable\n",
    "\n",
    "\n",
    "\n",
    "def get_method_name_emb(seq):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select method_name_emb from data_model_1 where seq=( select methodSeq from data_model_2 where seq=\" + str(seq) + \")\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    emb=oneRow[0]\n",
    "    cur.close()\n",
    "    return eval(emb)\n",
    "\n",
    "\n",
    "\n",
    "def get_positive_data_from_db(vec_length):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select vectorStruct,logNum from data_model_2 where logNum<>0 and vectorStruct is not null;\"\n",
    "    result = cur.execute(sqli)\n",
    "    methods_padding = np.empty(shape=[0, vec_length], dtype=int)\n",
    "    labels = []\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        methods_padding = string_vector_to_int(methods_padding, oneRow)\n",
    "        labels.append(oneRow[1])\n",
    "    return methods_padding, labels\n",
    "\n",
    "\n",
    "\n",
    "def add_nagitive_data(methods_padding, lables):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select vectorStruct,logNum from data_model_2 where logNum=0 and vectorStruct is not null limit \" + str(\n",
    "        len(methods_padding)) + \";\"\n",
    "    result = cur.execute(sqli)\n",
    "    for i in range(1, len(methods_padding)):\n",
    "        oneRow = cur.fetchone()\n",
    "        methods_padding = string_vector_to_int(methods_padding, oneRow)\n",
    "        lables.append(oneRow[1])\n",
    "    return methods_padding, lables\n",
    "\n",
    "\n",
    "def string_vector_to_int(methods_padding, oneRow):\n",
    "    methods_padding = np.append(methods_padding, [list(map(int, oneRow[0][1:len(oneRow[0]) - 1].split(\",\")))], axis=0)\n",
    "    return methods_padding\n",
    "\n",
    "\n",
    "def float_revert_int(value_list):\n",
    "    revert_value_list = []\n",
    "    for i in value_list:\n",
    "        revert_value_list.append(int(np.round(i)))\n",
    "    value_list = revert_value_list\n",
    "    return revert_value_list\n",
    "\n",
    "\n",
    "def draw_confusion_matrix(y_true, y_pred, dic_lables):\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "    labels = []\n",
    "    for key in dic_lables:\n",
    "        labels.append(key)\n",
    "    print_message(type(labels))\n",
    "    # 小数转整数\n",
    "    y_pred_int = float_revert_int(y_pred)\n",
    "    sns.set()\n",
    "    f, ax = plt.subplots()\n",
    "    C2 = confusion_matrix(y_true,y_pred,  labels=labels)\n",
    "    print_message(C2)  # 打印出来看看\n",
    "    sns.heatmap(C2, annot=True, fmt='.20g', ax=ax, cmap=\"YlGnBu\")  # 画热力图\n",
    "    ax.set_title('confusion matrix')  # 标题\n",
    "    ax.set_xlabel('true')  # x轴\n",
    "    ax.set_ylabel('predict')  # y轴\n",
    "    save_pic(plt, '混淆矩阵')\n",
    "    # plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def save_pic(plt, file_name):\n",
    "    # 创建目录\n",
    "    dirs = 'C:\\\\Users\\\\chang\\\\Desktop\\\\日志工作空间\\\\实验图片\\\\'\n",
    "    t = time.strftime('%Y-%m-%d-%H', time.localtime(int(time.time())))\n",
    "    dirs = dirs + t\n",
    "    file = dirs + '\\\\' + file_name + '.png'\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    # 保存图片\n",
    "    plt.savefig(file)  # 保存图片\n",
    "\n",
    "\n",
    "def plot_value(y_true, y_pred):\n",
    "    balance_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision = sk.metrics.precision_score(y_true, y_pred)\n",
    "    recall = sk.metrics.recall_score(y_true, y_pred)\n",
    "    f1_value = sk.metrics.f1_score(y_true, y_pred)\n",
    "    return balance_accuracy, precision, recall, f1_value\n",
    "\n",
    "\n",
    "def autolabel(rects, ax):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "def plot_graphs(history_loc, type_loc):\n",
    "    plt.cla()\n",
    "\n",
    "    plt.plot(history_loc.history[type_loc])\n",
    "    plt.plot(history_loc.history['val_' + type_loc])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(type_loc)\n",
    "    plt.legend([type_loc, 'val_' + type_loc])\n",
    "    save_pic(plt, type_loc)\n",
    "    # plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def str_revert_to_list(methods_padding, max_len):\n",
    "    methods_padding_list = np.empty(shape=[0, max_len], dtype=int)\n",
    "\n",
    "    for methods_padding_one in methods_padding:\n",
    "        list1 = methods_padding_one.split(',')\n",
    "        list1 = list(map(int, list1))\n",
    "        methods_padding_list = np.append(methods_padding_list, [np.array(list1)], axis=0)  # 添加整行元素，axis=1添加整列元素\n",
    "    return methods_padding_list\n",
    "\n",
    "\n",
    "def get_object(save_absolute_path):\n",
    "    summer_load = None\n",
    "    with open(save_absolute_path, 'rb') as f:\n",
    "        summer_load = pickle.load(f)  # read file and build object\n",
    "    return summer_load\n",
    "\n",
    "\n",
    "def save_object(summer_save, save_absolute_path):\n",
    "    summer_save = pickle.dumps(summer_save)\n",
    "    with open(save_absolute_path, 'wb') as f:  # open file with write-mode\n",
    "        f.write(summer_save)  # serialize and save object\n",
    "\n",
    "\n",
    "def get_parent(vec_chair):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select parentId from data_model_2 where seq=\" + str(vec_chair[len(vec_chair) - 1]) + \";\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    if oneRow[0] == 0:\n",
    "        return vec_chair\n",
    "    else:\n",
    "        vec_chair.append(oneRow[0])\n",
    "        return get_parent(vec_chair)\n",
    "\n",
    "\n",
    "def get_chair_vec(vec_chair, i,maxlen):\n",
    "    cur = conn.cursor()\n",
    "    one_leaf_list = []\n",
    "    leaf = True\n",
    "    message_type = i\n",
    "    for one_leaf in vec_chair:\n",
    "        sqli = \"select a.vectorSemantic,REPLACE(REPLACE(vectorStruct,'[',''),']',''),logNum ,REPLACE(REPLACE(syntacticMessage,'[',''),']','') \" \\\n",
    "               \"from data_model_2 a  where seq=\" + str(one_leaf) + \";\"\n",
    "        result = cur.execute(sqli)\n",
    "        oneRow = cur.fetchone()\n",
    "        vectorSemantic = oneRow[0]\n",
    "        vectorStruct = oneRow[1]\n",
    "        logNum = oneRow[2]\n",
    "        syntacticMessage = oneRow[3]\n",
    "        if vectorSemantic != \"[]\":\n",
    "            if message_type == 1 or message_type == 3:\n",
    "                if vectorSemantic is not None :\n",
    "                    if len(vectorSemantic)==0:\n",
    "                        vectorSemantic='0'\n",
    "                    list_semantic = vectorSemantic.split(',')\n",
    "                    list_semantic = list(map(float, list_semantic))\n",
    "                    list_semantic_tmp = pad_sequences([list_semantic], maxlen=maxlen,\n",
    "                                                      padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "                    list_semantic = list_semantic_tmp[0]\n",
    "                    one_leaf_list.extend(list_semantic)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "            if message_type == 2:\n",
    "                if syntacticMessage is not None :\n",
    "                    if len(syntacticMessage)==0:\n",
    "                        syntacticMessage='0'\n",
    "                    list_syntactic = syntacticMessage.split(',')\n",
    "                    list_syntactic = list(map(float, list_syntactic))\n",
    "                    list_syntactic_tmp = pad_sequences([list_syntactic], maxlen=maxlen,\n",
    "                                                       padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "                    list_syntactic = list_syntactic_tmp[0]\n",
    "                    one_leaf_list.extend(list_syntactic)\n",
    "    cur.close()\n",
    "    return one_leaf_list\n",
    "\n",
    "\n",
    "def print_message(message):\n",
    "    t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(int(time.time())))\n",
    "    print('时间：' + t, end=\" \", flush=True)\n",
    "    print(message, flush=True)\n",
    "\n",
    "\n",
    "\n",
    "def repetition_list(a):\n",
    "    repetition = 6\n",
    "    c = []\n",
    "    for a_one in a:\n",
    "        b = []\n",
    "        for a_single in a_one:\n",
    "            for i in range(repetition):\n",
    "                b.append(a_single)\n",
    "        c.append(b)\n",
    "    return c\n",
    "\n",
    "\n",
    "def overlap_channel(a, b):\n",
    "    element_num = 0\n",
    "    height = 0\n",
    "    width = 0\n",
    "    if len(a.shape) == 2:\n",
    "        element_num = a.shape[0]\n",
    "        height = 1\n",
    "        width = a.shape[1]\n",
    "    elif len(a.shape) == 3:\n",
    "        element_num = a.shape[0]\n",
    "        height = a.shape[1]\n",
    "        width = a.shape[2]\n",
    "    else:\n",
    "        raise Exception(print_message('通道叠加 不支持这类维度:' + str(a.shape)))\n",
    "\n",
    "    a = a.reshape(element_num, 1, height, width)\n",
    "    b = b.reshape(element_num, 1, height, width)\n",
    "    a = a.transpose(1, 0, 2, 3)\n",
    "    b = b.transpose(1, 0, 2, 3)\n",
    "    c = np.vstack((a, b))\n",
    "    c = c.transpose(1, 2, 3, 0)\n",
    "    return c\n",
    "\n",
    "\n",
    "def check_list_exist(A, B):\n",
    "    exist = False\n",
    "    for row in A:\n",
    "        if (row == B).all():\n",
    "            exist = True\n",
    "    return exist\n",
    "\n",
    "\n",
    "def get_convert_data(A, B):\n",
    "    C = []\n",
    "    for A_one in A:\n",
    "        for B_one in B:\n",
    "            if (np.array(A_one) == np.array(B_one)).all():\n",
    "                C.append(A_one)\n",
    "    return C\n",
    "\n",
    "\n",
    "def cosDist(A, B):\n",
    "    return pdist(np.vstack([A, B]), 'cosine')  # np.sqrt(sum(np.power((A - B), 2)))\n",
    "\n",
    "\n",
    "def get_normal_data(data_point, data_lable):\n",
    "    neigh_num_a = 5\n",
    "    diff_rate_b = 1\n",
    "    can_stop = 0.99\n",
    "\n",
    "    # print_message(len(data_point))\n",
    "\n",
    "    abnormal_old = []\n",
    "    abnormal_new_ = []\n",
    "    for i in range(0, 100):\n",
    "        for one_point, one_lable in zip(data_point, data_lable):\n",
    "            neighbour_point = []\n",
    "            neighbour_lable = []\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(data_point, data_lable):\n",
    "                if check_list_exist(abnormal_new_, one_point_neighbour) or (one_point_neighbour == one_point).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    this_point_distance = cosDist(one_point_neighbour, one_point)\n",
    "                    if len(neighbour_point) >= neigh_num_a \\\n",
    "                            and this_point_distance < neighbour_point[neigh_num_a - 1] \\\n",
    "                            or len(neighbour_point) < neigh_num_a:\n",
    "                        if len(neighbour_point) < neigh_num_a:\n",
    "                            neighbour_point.append(this_point_distance)\n",
    "                            neighbour_lable.append(one_lable_neighbour)\n",
    "                        else:\n",
    "                            neighbour_point[neigh_num_a - 1] = (this_point_distance)\n",
    "                            neighbour_lable[neigh_num_a - 1] = (one_lable_neighbour)\n",
    "                        # 按照距离排序\n",
    "                        zipped = zip(neighbour_point, neighbour_lable)\n",
    "                        sort_zipped = sorted(zipped)\n",
    "                        result = zip(*sort_zipped)\n",
    "                        neighbour_point, neighbour_lable = [list(x) for x in result]\n",
    "\n",
    "            same_num = 0\n",
    "            diff_num = 0\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(neighbour_point[0:neigh_num_a],\n",
    "                                                                neighbour_lable[0:neigh_num_a]):\n",
    "                if one_lable_neighbour != one_lable:\n",
    "                    diff_num += 1\n",
    "                else:\n",
    "                    same_num += 1\n",
    "            diff_rate = diff_num / (same_num + diff_num)\n",
    "            if diff_rate >= diff_rate_b:\n",
    "                abnormal_new_.append([one_point])\n",
    "        if max(len(abnormal_new_), len(abnormal_old)) != 0 and \\\n",
    "                len(get_convert_data(abnormal_new_, abnormal_old)) / max(len(abnormal_new_),\n",
    "                                                                         len(abnormal_old)) >= can_stop:\n",
    "            break\n",
    "        else:\n",
    "            abnormal_old = abnormal_new_[:]\n",
    "        # print_message('单次noise数据量：' + str(len(abnormal_old)))\n",
    "\n",
    "    data_point_clear = []\n",
    "    data_lable_clear = []\n",
    "\n",
    "    for one_point, one_lable in zip(data_point, data_lable):\n",
    "        if (not check_list_exist(abnormal_old, one_point)):\n",
    "            data_point_clear.append(one_point)\n",
    "            data_lable_clear.append(one_lable)\n",
    "    return data_point_clear, data_lable_clear, abnormal_old\n",
    "\n",
    "\n",
    "##仅支持0 - 1 分类\n",
    "def change_abnormal(data_point, data_lable):\n",
    "    neigh_num_a = 10\n",
    "    diff_rate_b = 1\n",
    "    can_stop = 0.99\n",
    "\n",
    "    # print_message(len(data_point))\n",
    "\n",
    "    abnormal_old = []\n",
    "    abnormal_new_ = []\n",
    "    for i in range(0, 100):\n",
    "        for one_point, one_lable in zip(data_point, data_lable):\n",
    "            neighbour_point = []\n",
    "            neighbour_lable = []\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(data_point, data_lable):\n",
    "                if check_list_exist(abnormal_new_, one_point_neighbour) or (one_point_neighbour == one_point).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    this_point_distance = cosDist(one_point_neighbour, one_point)\n",
    "                    if len(neighbour_point) >= neigh_num_a \\\n",
    "                            and this_point_distance < neighbour_point[neigh_num_a - 1] \\\n",
    "                            or len(neighbour_point) < neigh_num_a:\n",
    "                        if len(neighbour_point) < neigh_num_a:\n",
    "                            neighbour_point.append(this_point_distance)\n",
    "                            neighbour_lable.append(one_lable_neighbour)\n",
    "                        else:\n",
    "                            neighbour_point[neigh_num_a - 1] = (this_point_distance)\n",
    "                            neighbour_lable[neigh_num_a - 1] = (one_lable_neighbour)\n",
    "                        # 按照距离排序\n",
    "                        zipped = zip(neighbour_point, neighbour_lable)\n",
    "                        sort_zipped = sorted(zipped)\n",
    "                        result = zip(*sort_zipped)\n",
    "                        neighbour_point, neighbour_lable = [list(x) for x in result]\n",
    "\n",
    "            same_num = 0\n",
    "            diff_num = 0\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(neighbour_point[0:neigh_num_a],\n",
    "                                                                neighbour_lable[0:neigh_num_a]):\n",
    "                if one_lable_neighbour != one_lable:\n",
    "                    diff_num += 1\n",
    "                else:\n",
    "                    same_num += 1\n",
    "            diff_rate = diff_num / (same_num + diff_num)\n",
    "            if diff_rate >= diff_rate_b:\n",
    "                abnormal_new_.append([one_point])\n",
    "        if max(len(abnormal_new_), len(abnormal_old)) != 0 and \\\n",
    "                len(get_convert_data(abnormal_new_, abnormal_old)) / max(len(abnormal_new_),\n",
    "                                                                         len(abnormal_old)) >= can_stop:\n",
    "            break\n",
    "        else:\n",
    "            abnormal_old = abnormal_new_[:]\n",
    "        # print_message('单次noise数据量：' + str(len(abnormal_old)))\n",
    "\n",
    "    data_point_clear = []\n",
    "    data_lable_clear = []\n",
    "\n",
    "    for one_point, one_lable in zip(data_point, data_lable):\n",
    "        if (not check_list_exist(abnormal_old, one_point)):\n",
    "            data_point_clear.append(one_point)\n",
    "            data_lable_clear.append(one_lable)\n",
    "        else:\n",
    "            ##修改noise标签\n",
    "            data_point_clear.append(one_point)\n",
    "            if one_lable == 0:\n",
    "                data_lable_clear.append(1)\n",
    "            else:\n",
    "                data_lable_clear.append(0)\n",
    "    return data_point_clear, data_lable_clear, abnormal_old\n",
    "\n",
    "\n",
    "def clear_data(train_vec_tmp, train_labels, zero_index):\n",
    "    group_num = 300\n",
    "    train_vec_tmp_clear = []\n",
    "    train_labels_clear = []\n",
    "    abnormal_num = 0\n",
    "    for b in range(int(len(train_vec_tmp) / group_num) + 1):\n",
    "        begin = b * group_num + zero_index\n",
    "        end = (b + 1) * group_num\n",
    "        if end > len(train_vec_tmp):\n",
    "            end = len(train_vec_tmp)\n",
    "        train_vec_group_one, train_labels_group_one, abnormal_old = change_abnormal(train_vec_tmp[begin:end],\n",
    "                                                                                    train_labels[begin:end])\n",
    "        train_vec_tmp_clear.extend(train_vec_group_one)\n",
    "        train_labels_clear.extend(train_labels_group_one)\n",
    "        abnormal_num = abnormal_num + len(abnormal_old)\n",
    "    print_message('noise数据量：' + str(abnormal_num))\n",
    "    print_message('训练总数据量：' + str(len(train_vec_tmp)))\n",
    "    train_vec_tmp = train_vec_tmp_clear\n",
    "    train_labels = train_labels_clear\n",
    "    return train_vec_tmp, train_labels\n",
    "\n",
    "\n",
    "def play_title_message():\n",
    "    playsound('D:/python/log_predict/file/结束提示.mp3')\n",
    "\n",
    "\n",
    "def get_tree_model_together(struct_vec_length_one,semantic_vec_length_one,syntatic_vec_length_one\n",
    "                            ,num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        keras.layers.InputLayer(struct_vec_length_one)\n",
    "    ])\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(semantic_vec_length_one, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(syntatic_vec_length_one, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model3.output, model2.output])\n",
    "    x = Dropout(0.4)(model_together)\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    final_output = Dense(24, activation='relu')(x)\n",
    "    model_together = Model(inputs=[model3.input, model2.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "\n",
    "\n",
    "def seperate_data(struct_leaf_vec, semantic_leaf_vec, syntatic_leaf_vec,\n",
    "                  lable, leaf,seed):\n",
    "    struct_train_vec, struct_test_vec, \\\n",
    "    semantic_train_vec, semantic_test_vec, \\\n",
    "    syntatic_train_vec, syntatic_test_vec, \\\n",
    "    struct_train_labels, struct_test_labels, \\\n",
    "    struct_train_leaf, struct_test_leaf = train_test_split(\n",
    "        struct_leaf_vec, semantic_leaf_vec, syntatic_leaf_vec,\n",
    "        lable, leaf,\n",
    "        test_size=0.2,\n",
    "        random_state=seed)\n",
    "    struct_verify_vec, struct_test_vec, \\\n",
    "    semantic_verify_vec, semantic_test_vec, \\\n",
    "    syntatic_verify_vec, syntatic_test_vec, \\\n",
    "    struct_verify_labels, struct_test_labels, \\\n",
    "    struct_verify_leaf, struct_test_leaf = train_test_split(\n",
    "        struct_test_vec, semantic_test_vec, syntatic_test_vec,\n",
    "        struct_test_labels,\n",
    "        struct_test_leaf,\n",
    "        test_size=0.5,\n",
    "        random_state=seed)\n",
    "    return struct_train_vec, struct_test_vec, \\\n",
    "           semantic_train_vec, semantic_test_vec, \\\n",
    "           syntatic_train_vec, syntatic_test_vec, \\\n",
    "           struct_train_labels, struct_test_labels, \\\n",
    "           struct_verify_vec, semantic_verify_vec, \\\n",
    "           syntatic_verify_vec, struct_verify_labels, struct_verify_leaf\n",
    "\n",
    "def get_lable(conn, seq):\n",
    "    seq_db = 0\n",
    "    leaf = ''\n",
    "    lable = 0\n",
    "    sqli = \"select seq,leaf,CASE WHEN logNum > 0 THEN  1  ELSE 0 END from data_model_2 where seq=\" + seq\n",
    "    cur = conn.cursor()\n",
    "    result = cur.execute(sqli)\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        seq_db = oneRow[0]\n",
    "        leaf = oneRow[1]\n",
    "        lable = oneRow[2]\n",
    "    cur.close()\n",
    "    return seq_db, leaf, lable\n",
    "\n",
    "\n",
    "\n",
    "def get_feature_from_file(file_name, node_all, node_vec):\n",
    "    with open(file_name, 'r') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            one_row = line.split(' ')\n",
    "            if len(one_row) > 2:\n",
    "                node_all.append(one_row[0])\n",
    "                one_row[len(one_row) - 1] = one_row[len(one_row) - 1].replace('\\n', '')\n",
    "                one_row = list(map(float, one_row))\n",
    "                node_vec.append(list(map(float, one_row[1:])))\n",
    "            line = file.readline()\n",
    "    return node_all, node_vec\n",
    "\n",
    "\n",
    "def convert_metric_to_list(project_name, feature_type, percentage, metric_type, tool_type):\n",
    "    one_row = []\n",
    "    one_row.append(project_name)\n",
    "    one_row.append(feature_type)\n",
    "    one_row.append(percentage)\n",
    "    one_row.append(metric_type)\n",
    "    one_row.append(tool_type)\n",
    "    return one_row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_call_graph(graph_leng, num_word, embedding_dim):\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Convolution1D(input_shape=(graph_leng, 1), filters=32, kernel_size=5, strides=1, padding='same',\n",
    "                      activation='relu', name='call-message'),\n",
    "        MaxPooling1D(pool_size=2, strides=2, padding='same', ),\n",
    "        Convolution1D(64, 5, strides=1, padding='same', activation='relu'),\n",
    "        MaxPooling1D(2, 2, 'same'),\n",
    "        Flatten(),\n",
    "        Dense(69, activation='relu'),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model3\n",
    "\n",
    "def get_method_name_model(method_name_x, method_name_y, num_word, embedding_dim):\n",
    "\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        keras.layers.Masking(mask_value=0, input_shape=(method_name_x, method_name_y)),\n",
    "        LSTM(units=8,activation='sigmoid'),\n",
    "        #         LSTM(units=32, activation='tanh'),\n",
    "        #         Flatten(),\n",
    "        Dense(53, activation='relu'),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model1\n",
    "\n",
    "def get_tree_model_together(semantic_vec_length_chain,syntatic_vec_length_chain,num_word,embedding_dim):\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(semantic_vec_length_chain, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(syntatic_vec_length_chain, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        #         LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        #         Flatten(),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model3.output, model2.output])\n",
    "    x = Dropout(0.2)(model_together)\n",
    "    final_output = Dense(43, activation='relu')(x)\n",
    "    model_together = Model(inputs=[model3.input, model2.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "\n",
    "def get_method_name_call_graph_model(method_dem_x,method_dem_y,call_graph_len,num_word,embedding_dim):\n",
    "    method_name_model = get_method_name_model(method_dem_x,method_dem_y,  num_word, embedding_dim)\n",
    "    call_graph_model = get_call_graph(call_graph_len,  num_word, embedding_dim)\n",
    "    m_j_merged_a = keras.layers.concatenate([method_name_model.output,call_graph_model.output],axis=-1)\n",
    "    dense1_a = Dense(4,activation='tanh')(m_j_merged_a)\n",
    "    final_output = Dropout(0.2)(dense1_a)\n",
    "    model_together_final = Model(inputs=[ method_name_model.input,call_graph_model.input],outputs=final_output)\n",
    "    return model_together_final\n",
    "\n",
    "\n",
    "def get_featrue_and_label(project_name):\n",
    "    # 1-语法 2-语义 3-结合(本树+调用关系)\n",
    "    # 4-语法（新）5-纯调用关系 6-结合(本树)\n",
    "    select_model = 6\n",
    "    semantic_vec_length = 50\n",
    "    tree_deep=1\n",
    "    semantic_vec_length_chain =  semantic_vec_length* tree_deep\n",
    "    syntatic_vec_length_chain = 30 * tree_deep\n",
    "    # 获取每个叶子节点\n",
    "    node = []\n",
    "    lable = []\n",
    "    semantic_leaf_one_node = []\n",
    "    semantic_leaf_vec = []\n",
    "    syntatic_leaf_vec = []\n",
    "    theme_leaf_vec = []\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    sqli = \"select seq,(select method_name_emb from data_model_1 where seq=a.methodSeq),\" \\\n",
    "           \"CASE WHEN logNum > 0 THEN  1  ELSE 0 END \" \\\n",
    "           \"from  data_model_2 a where  methodSeq in \" \\\n",
    "           \"(select seq from data_model_1 where projectName='\"+project_name+\"') \"\n",
    "    result = cur.execute(sqli)\n",
    "    #  and methodSeq<59900\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        node.append(oneRow[0])\n",
    "        theme_leaf_vec.append(eval(oneRow[1]))\n",
    "        lable.append(oneRow[2])\n",
    "    print_message(\"获取节点特征\")\n",
    "    for one_leaf in node:\n",
    "        vec_chain = []\n",
    "        for i in range(1, 3):\n",
    "            if i == 1:\n",
    "                maxlen = semantic_vec_length_chain\n",
    "                vec_chain = []\n",
    "                vec_chain.append(one_leaf)\n",
    "#                 vec_chain = get_parent(vec_chain)\n",
    "            elif i==2:\n",
    "                maxlen = syntatic_vec_length_chain\n",
    "                vec_chain = []\n",
    "                vec_chain.append(one_leaf)\n",
    "                #vec_chain = get_parent(vec_chain)\n",
    "\n",
    "            # 根据list中的记录查询节点记录，组合向量\n",
    "            one_leaf_vec = get_chair_vec(vec_chain, i,maxlen//tree_deep)\n",
    "            # 划分为等长向量\n",
    "            one_leaf_vec_padding_tmp = pad_sequences([one_leaf_vec], maxlen=maxlen,\n",
    "                                                     padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "            one_leaf_vec_padding_tmp = one_leaf_vec_padding_tmp[0]\n",
    "\n",
    "            if i == 1:\n",
    "                semantic_leaf_vec.append(one_leaf_vec_padding_tmp)\n",
    "            elif i==2:\n",
    "                syntatic_leaf_vec.append(one_leaf_vec_padding_tmp)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return node,semantic_leaf_vec,syntatic_leaf_vec,theme_leaf_vec,lable\n",
    "\n",
    "\n",
    "def get_feature_from_file(file_name, node_all, node_vec):\n",
    "    with open(file_name, 'r') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            one_row = line.split(' ')\n",
    "            if len(one_row) > 2:\n",
    "                node_all.append(one_row[0])\n",
    "                one_row[len(one_row) - 1] = one_row[len(one_row) - 1].replace('\\n', '')\n",
    "                one_row = list(map(float, one_row))\n",
    "                node_vec.append(list(map(float, one_row[1:])))\n",
    "            line = file.readline()\n",
    "    return node_all, node_vec\n",
    "\n",
    "\n",
    "\n",
    "def build_data(node,semantic_vec,edge,lable):\n",
    "    features_matrix_tmp=[]\n",
    "    for one_node,feature_value in zip(node,semantic_vec):\n",
    "        features_matrix_tmp.append(feature_value)\n",
    "\n",
    "    features_matrix=np.array(features_matrix_tmp)\n",
    "    a = {}\n",
    "    index=0\n",
    "    for i in features_matrix:\n",
    "        a[index]=[int(i) for i in i.tolist()]\n",
    "        index+=1\n",
    "\n",
    "\n",
    "    max_feature = np.max([v for v_list in a.values() for v in v_list])\n",
    "    features_matrix = np.zeros(shape = (len(list(a.keys())), max_feature+1))\n",
    "\n",
    "    i = 0\n",
    "    for k, vs in tqdm(a.items()):\n",
    "        for v in vs:\n",
    "            features_matrix[i, v] = 1\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "    class data:\n",
    "        x=[]\n",
    "        y=[]\n",
    "        edge_index=[]\n",
    "        train_mask=[]\n",
    "        val_mask=[]\n",
    "        test_mask=[]\n",
    "\n",
    "    data=data()\n",
    "    node=node\n",
    "    label=lable\n",
    "    edge=edge\n",
    "    feature=[]\n",
    "\n",
    "    feature = features_matrix\n",
    "    # for i in node:\n",
    "    #     feature.append(seq_feature[i])\n",
    "\n",
    "    train_rate=0.6\n",
    "    val_rate=0.2\n",
    "    test_rate=0.2\n",
    "    number=0\n",
    "    node_new={}\n",
    "    for i in node:\n",
    "        node_new[i]=number\n",
    "        number=number+1\n",
    "    data.x=torch.Tensor(feature)\n",
    "    edge_new=[[],[]]\n",
    "    for i in edge:\n",
    "        edge_new[0].append(node_new[i[0]])\n",
    "        edge_new[1].append(node_new[i[1]])\n",
    "\n",
    "    data.edge_index=torch.LongTensor(edge_new)\n",
    "\n",
    "    data.y=torch.LongTensor(label)\n",
    "    train_num=int(len(node)*train_rate)\n",
    "    train_mask=[ False for i in node]\n",
    "    while train_num>0:\n",
    "        index=0\n",
    "        for i in node:\n",
    "            if train_num>0 and random.random()>=0.5 and not train_mask[index]:\n",
    "                train_mask[index]=True\n",
    "                train_num=train_num-1\n",
    "            index=index+1\n",
    "    data.train_mask=train_mask\n",
    "    test_num=int(len(node)*test_rate)\n",
    "\n",
    "    test_mask=[ False for i in node]\n",
    "    while test_num>0:\n",
    "        index=0\n",
    "        for i,j in zip(test_mask,train_mask):\n",
    "            if test_num>0 and not i and not j:\n",
    "                if random.random()>=0.5:\n",
    "                    test_mask[index]=True\n",
    "                    test_num=test_num-1\n",
    "            index=index+1\n",
    "\n",
    "    data.test_mask=test_mask\n",
    "    val_num=int(len(node)*val_rate)\n",
    "\n",
    "    val_mask=[ False for i in node]\n",
    "    while val_num>0:\n",
    "        index=0\n",
    "        for i,j,z in zip(val_mask,train_mask,test_mask):\n",
    "            if val_num>0 and not i and not j and not z:\n",
    "                if random.random()>=0.5:\n",
    "                    val_mask[index]=True\n",
    "                    val_num=val_num-1\n",
    "            index=index+1\n",
    "    data.val_mask=val_mask\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_node_indices(G, ids):\n",
    "    # find the indices of the nodes\n",
    "    node_ids = np.asarray(ids)\n",
    "    flat_node_ids = node_ids.reshape(-1)\n",
    "\n",
    "    flat_node_indices = G.node_ids_to_ilocs(flat_node_ids) # in-built function makes it really easy\n",
    "    # back to the original shape\n",
    "    node_indices = flat_node_indices.reshape(1, len(node_ids)) # add 1 extra dimension\n",
    "\n",
    "    return node_indices\n",
    "\n",
    "def splite_data(vec_x_sem,vec_x_syn,vec_x_graph,vec_theme,\n",
    "                vec_y,mask):\n",
    "    vec_sem=[]\n",
    "    vec_syn=[]\n",
    "    vec_graph=[]\n",
    "    vec_method_theme=[]\n",
    "    lable=[]\n",
    "    for i,j,h,w,g,z  in zip(vec_x_sem,vec_x_syn,\n",
    "                              vec_x_graph,vec_theme,\n",
    "                              vec_y,\n",
    "                              mask):\n",
    "        if z:\n",
    "            vec_sem.append(i)\n",
    "            vec_syn.append(j)\n",
    "            vec_graph.append(h)\n",
    "            vec_method_theme.append(w)\n",
    "            lable.append(g)\n",
    "    return vec_sem,vec_syn,vec_graph,vec_method_theme,lable\n",
    "def get_method_name_emb(seq):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select method_name_emb from data_model_1 where seq=( select methodSeq from data_model_2 where seq=\" + str(seq) + \")\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    emb=oneRow[0]\n",
    "    cur.close()\n",
    "    return eval(emb)\n",
    "\n",
    "\n",
    "# 得到多分枝模型\n",
    "def get_graph_model_4(graph_leng,syntactic_len,method_name_x, method_name_y,\n",
    "                      num_word,embedding_dim):\n",
    "#     model1 = tf.keras.models.Sequential([\n",
    "#         Input(shape=(semantic_vec,1)),\n",
    "#         (Flatten()),\n",
    "#         (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "#         (keras.layers.GlobalMaxPool1D()),\n",
    "#     ])\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_leng,1)),\n",
    "        Flatten(),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model4 = tf.keras.models.Sequential([\n",
    "        keras.layers.Masking(mask_value=0, input_shape=(method_name_x, method_name_y)),\n",
    "        LSTM(units=8, activation='tanh', return_sequences=True),\n",
    "        keras.layers.GlobalMaxPool1D(),\n",
    "        \n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([ model2.output, model3.output,model4.output])\n",
    "    model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "    x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "    x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model2.input, model3.input,model4.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "def get_graph_model_3(graph_leng,syntactic_len,\n",
    "                      num_word,embedding_dim):\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_leng,1)),\n",
    "        Flatten(),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([ model2.output, model3.output])\n",
    "\n",
    "    model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "    x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "    x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model2.input, model3.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "def get_graph_model_2(semantic_vec,syntactic_len,\n",
    "                      num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(semantic_vec,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model1.output, model3.output])\n",
    "    model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "\n",
    "    x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "    x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model1.input,model3.input], outputs=final_output)\n",
    "    return model_together\n",
    "def get_graph_model_1(semantic_vec,\n",
    "                      num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(semantic_vec,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        (Dropout(0.2)),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model1\n",
    "def get_graph_model_gcn(graph_vec_len,\n",
    "                        num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_vec_len,1)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        (Dropout(0.2)),\n",
    "        Dense(2, activation='softmax'),\n",
    "\n",
    "    ])\n",
    "    return model1\n",
    "def save_performance(content,ac,pr,re,f1):\n",
    "    f = open(\"print-message.txt\", mode='a')\n",
    "    f.write(str(datetime.datetime.now())+'\\t'+ content+' \\t'+str(ac)+\n",
    "            '\\t'+str(pr)+ '\\t'+str(re)+'\\t'+ str(f1)+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def convert_2_hot_vec(list):\n",
    "    key = []\n",
    "    for i in list:\n",
    "        tmp = [int(i) for i in i[::2]]\n",
    "        for j in tmp:\n",
    "            if j not in key:\n",
    "                key.append(j)\n",
    "        # print(i[1::2])\n",
    "    # 得到one-hot表格\n",
    "    key = sort(key)\n",
    "    feature_hot = []\n",
    "    for i in list:\n",
    "        key_tmp = i[::2]\n",
    "        num_tmp = i[1::2]\n",
    "        key_tmp = [int(i) for i in key_tmp]\n",
    "        num_tmp = [int(i) for i in num_tmp]\n",
    "        \n",
    "        feature_tmp = []\n",
    "        for j in key:\n",
    "            if j in key_tmp:\n",
    "                feature_tmp.append(num_tmp[key_tmp.index(j)])\n",
    "            else:\n",
    "                feature_tmp.append(0)\n",
    "        feature_hot.append(feature_tmp)\n",
    "    return key,feature_hot\n",
    "def plot_value(y_true, y_pred):\n",
    "    import sklearn as sk\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    balance_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision = sk.metrics.precision_score(y_true, y_pred)\n",
    "    recall = sk.metrics.recall_score(y_true, y_pred)\n",
    "    f1_value = sk.metrics.f1_score(y_true, y_pred)\n",
    "    return balance_accuracy, precision, recall, f1_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca1d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epoch = 100  # 训练周期\n",
    "feature_length = \"10\"\n",
    "one_project = 'shardingsphere-elasticjob-master'\n",
    "num_word=50000\n",
    "embedding_dim=12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e51ed",
   "metadata": {},
   "source": [
    "# 获取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e8401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间：2024-08-19 09:54:04 获取节点特征\n"
     ]
    }
   ],
   "source": [
    "\n",
    "node, semantic_vec, \\\n",
    "syntatic_vec,\\\n",
    "leaf_method_name_vec,lable = get_featrue_and_label(one_project)\n",
    "\n",
    "randomValue=int(random.random()*100)\n",
    "node, semantic_vec, \\\n",
    "syntatic_vec,\\\n",
    "leaf_method_name_vec,lable=shuffle_data(node, semantic_vec,\n",
    "                                        syntatic_vec,\n",
    "                                        leaf_method_name_vec,lable,randomValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3890307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number=0\n",
    "node_new={}\n",
    "for i in node:\n",
    "    node_new[i]=number\n",
    "    number=number+1\n",
    "\n",
    "edge=[]#[[28,16],[16,85],[85,46],[28,16],[16,85],[85,46],[16,85],[85,46]]\n",
    "\n",
    "conn = get_connection()\n",
    "\n",
    "cur = conn.cursor()\n",
    "sqli = \"select parentId,seq \" \\\n",
    "       \"from data_model_2 \" \\\n",
    "       \"where methodSeq in \" \\\n",
    "       \"(select seq from data_model_1 where projectName='\"+one_project+\"') \" \\\n",
    "       \"and parentId<>0  \" \\\n",
    "       \"union \" \\\n",
    "       \"select callBlockSeq,calledBlockSeq \" \\\n",
    "       \"from call_graph_data \" \\\n",
    "       \"where callMethodSeq in \" \\\n",
    "       \"(select seq from data_model_1 where projectName='\"+one_project+\"')   \"\n",
    "result = cur.execute(sqli)\n",
    "#  and methodSeq<59900  and callMethodSeq<59900 and calledMethodSeq<59900\n",
    "for i in range(result):\n",
    "    oneRow = cur.fetchone()\n",
    "    if oneRow[0] in node_new.keys() and oneRow[1] in node_new.keys():\n",
    "        edge_tmp=[]\n",
    "        edge_tmp.append(node_new[oneRow[0]])\n",
    "        edge_tmp.append(node_new[oneRow[1]])\n",
    "        edge.append(edge_tmp)\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c848d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(node,one_project+'node.plk')\n",
    "save_object(semantic_vec,one_project+'semantic_vec.plk')\n",
    "save_object(syntatic_vec,one_project+'syntatic_vec.plk')\n",
    "save_object(leaf_method_name_vec,one_project+'leaf_method_name_vec.plk')\n",
    "save_object(lable,one_project+'lable.plk')\n",
    "save_object(node_new,one_project+'node_new.plk')\n",
    "save_object(edge,one_project+'edge.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ceafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6969ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node=get_object('数据集-缓存//'+one_project+'node.plk')\n",
    "semantic_vec=get_object('数据集-缓存//'+one_project+'semantic_vec.plk')\n",
    "syntatic_vec=get_object('数据集-缓存//'+one_project+'syntatic_vec.plk')\n",
    "leaf_method_name_vec=get_object('数据集-缓存//'+one_project+'leaf_method_name_vec.plk')\n",
    "lable=get_object('数据集-缓存//'+one_project+'lable.plk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfeb5a5",
   "metadata": {},
   "source": [
    "# 组装特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "beb35b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集构建成功\n"
     ]
    }
   ],
   "source": [
    "featrure_combined=[]\n",
    "label_all=lable\n",
    "for i,j,k in zip(semantic_vec,syntatic_vec,leaf_method_name_vec):\n",
    "    one_feature=[]\n",
    "    one_feature.extend(i)\n",
    "    one_feature.extend(j)\n",
    "    one_feature.extend(np.average(k, axis=0).tolist())\n",
    "    featrure_combined.append(one_feature)\n",
    "print('数据集构建成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1078499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec,test_vec,train_label,test_label = train_test_split(featrure_combined, \n",
    "                                                       label_all,test_size=0.2,\n",
    "                                                       random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25716b4",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b89db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ml = KNeighborsClassifier(n_neighbors=5, \n",
    "                          weights='uniform',\n",
    "                          algorithm='auto',\n",
    "                          leaf_size=80, p=9)    #实例化KNN模型     #放入训练数据进行训练\n",
    "random_state = np.random.RandomState(0)\n",
    "ml.fit(train_vec,train_label)\n",
    "predict_number=ml.predict(test_vec)\n",
    "verify_number=test_label\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance(one_project+'-KNN',balance_accuracy, precision, recall, f1_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8a9d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "su = SMOTE(random_state=42)\n",
    "featrure_combined, label_all = \\\n",
    "            su.fit_resample(featrure_combined, label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "befaed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec,test_vec,train_label,test_label = train_test_split(featrure_combined, \n",
    "                                                       label_all,test_size=0.2,\n",
    "                                                       random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333e06c",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fa17264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chang\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, shuffle=True,random_state=42)\n",
    "# SVM\n",
    "random_state = np.random.RandomState(0)\n",
    "ml = svm.SVC(kernel='linear', probability=True,\n",
    "             random_state=random_state,tol = 0.0001, max_iter = 100)\n",
    "ml.fit(train_vec,train_label)\n",
    "predict_number=ml.predict(test_vec)\n",
    "verify_number=test_label\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "\n",
    "save_performance(one_project+'-SVM',balance_accuracy, precision, recall, f1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904df905",
   "metadata": {},
   "source": [
    "# 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3912a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0)\n",
    "ml = RandomForestClassifier(n_estimators=100, max_depth=1, random_state=456)\n",
    "ml.fit(train_vec,train_label)\n",
    "predict_number=ml.predict(test_vec)\n",
    "verify_number=test_label\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance(one_project+'-随机森林',balance_accuracy, precision, recall, f1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaae109",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e848c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ml = GaussianNB() #alpha拉普拉斯平滑系数，防止概率为0出现\n",
    "random_state = np.random.RandomState(0)\n",
    "ml.fit(train_vec,train_label)\n",
    "predict_number=ml.predict(test_vec)\n",
    "verify_number=test_label\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance(one_project+'-朴素贝叶斯',balance_accuracy, precision, recall, f1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f736c228",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be02d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml= DecisionTreeClassifier(random_state=5,max_features = 3,max_leaf_nodes=3) # 初始化\n",
    "random_state = np.random.RandomState(0)\n",
    "ml.fit(train_vec,train_label)\n",
    "predict_number=ml.predict(test_vec)\n",
    "verify_number=test_label\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance(one_project+'-决策树',balance_accuracy, precision, recall, f1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0869634",
   "metadata": {},
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18d3c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chang\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "ml = LogisticRegression()\n",
    "random_state = np.random.RandomState(0)\n",
    "ml.fit(train_vec,train_label)\n",
    "predict_number=ml.predict(test_vec)\n",
    "verify_number=test_label\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(predict_number,verify_number)\n",
    "save_performance(one_project+'-逻辑回归',balance_accuracy, precision, recall, f1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba32359",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b4aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4d340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
