{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d5cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证gcn特征来源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f418ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作者：常晓松\n",
    "# 作用：日志等级预测\n",
    "# 时间： 2024/7/9 19:31\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import diags\n",
    "import scipy\n",
    "import stellargraph as sg\n",
    "from playsound import playsound\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import *\n",
    "from keras import Model, Input\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from random import random\n",
    "from stellargraph.layer.gcn import GraphConvolution, GatherIndices\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "import pymysql\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split  # 出留法\n",
    "from keras.layers import Convolution1D, MaxPooling1D, LSTM\n",
    "import random\n",
    "import numpy\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "def get_ave_and_standard_error(data_list):\n",
    "\n",
    "    SD_P = numpy.std(data_list, ddof=0)\n",
    "    n = len(data_list)\n",
    "\n",
    "    ave = mean(data_list)\n",
    "    standard_error = SD_P / math.sqrt(n)\n",
    "    return round(ave,3),round(standard_error,3)\n",
    "\n",
    "def get_connection():\n",
    "    global conn\n",
    "    host = \"127.0.0.1\"\n",
    "    user = \"root\"\n",
    "    password = 'chang123'\n",
    "    db = 'predict_log_final'\n",
    "    conn = pymysql.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        db=db,\n",
    "        charset='utf8',\n",
    "        # autocommit=True,    # 如果插入数据，， 是否自动提交? 和conn.commit()功能一致。\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "def shuffle_data(node, semantic_vec,semantic_vec_one_node, syntatic_vec,leaf_method_name_vec,lable, seed):\n",
    "    c = list(zip(node, semantic_vec,semantic_vec_one_node, syntatic_vec,leaf_method_name_vec,lable))  # 将a,b整体作为一个zip,每个元素一一对应后打乱\n",
    "    random.seed(seed)\n",
    "    random.shuffle(c)  # 打乱c\n",
    "    node[:], semantic_vec[:],semantic_vec_one_node[:], syntatic_vec[:],leaf_method_name_vec[:],lable[:] = zip(*c)  # 将打乱的c解开\n",
    "    return node, semantic_vec,semantic_vec_one_node, syntatic_vec,leaf_method_name_vec,lable\n",
    "\n",
    "\n",
    "\n",
    "def get_method_name_emb(seq):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select method_name_emb from data_model_1 where seq=( select methodSeq from data_model_2 where seq=\" + str(seq) + \")\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    emb=oneRow[0]\n",
    "    cur.close()\n",
    "    return eval(emb)\n",
    "\n",
    "\n",
    "\n",
    "def get_positive_data_from_db(vec_length):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select vectorStruct,logNum from data_model_2 where logNum<>0 and vectorStruct is not null;\"\n",
    "    result = cur.execute(sqli)\n",
    "    methods_padding = np.empty(shape=[0, vec_length], dtype=int)\n",
    "    labels = []\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        methods_padding = string_vector_to_int(methods_padding, oneRow)\n",
    "        labels.append(oneRow[1])\n",
    "    return methods_padding, labels\n",
    "\n",
    "\n",
    "\n",
    "def add_nagitive_data(methods_padding, lables):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select vectorStruct,logNum from data_model_2 where logNum=0 and vectorStruct is not null limit \" + str(\n",
    "        len(methods_padding)) + \";\"\n",
    "    result = cur.execute(sqli)\n",
    "    for i in range(1, len(methods_padding)):\n",
    "        oneRow = cur.fetchone()\n",
    "        methods_padding = string_vector_to_int(methods_padding, oneRow)\n",
    "        lables.append(oneRow[1])\n",
    "    return methods_padding, lables\n",
    "\n",
    "\n",
    "def string_vector_to_int(methods_padding, oneRow):\n",
    "    methods_padding = np.append(methods_padding, [list(map(int, oneRow[0][1:len(oneRow[0]) - 1].split(\",\")))], axis=0)\n",
    "    return methods_padding\n",
    "\n",
    "\n",
    "def float_revert_int(value_list):\n",
    "    revert_value_list = []\n",
    "    for i in value_list:\n",
    "        revert_value_list.append(int(np.round(i)))\n",
    "    value_list = revert_value_list\n",
    "    return revert_value_list\n",
    "\n",
    "\n",
    "def draw_confusion_matrix(y_true, y_pred, dic_lables):\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "    labels = []\n",
    "    for key in dic_lables:\n",
    "        labels.append(key)\n",
    "    print_message(type(labels))\n",
    "    # 小数转整数\n",
    "    y_pred_int = float_revert_int(y_pred)\n",
    "    sns.set()\n",
    "    f, ax = plt.subplots()\n",
    "    C2 = confusion_matrix(y_true,y_pred,  labels=labels)\n",
    "    print_message(C2)  # 打印出来看看\n",
    "    sns.heatmap(C2, annot=True, fmt='.20g', ax=ax, cmap=\"YlGnBu\")  # 画热力图\n",
    "    ax.set_title('confusion matrix')  # 标题\n",
    "    ax.set_xlabel('true')  # x轴\n",
    "    ax.set_ylabel('predict')  # y轴\n",
    "    save_pic(plt, '混淆矩阵')\n",
    "    # plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def save_pic(plt, file_name):\n",
    "    # 创建目录\n",
    "    dirs = 'C:\\\\Users\\\\chang\\\\Desktop\\\\日志工作空间\\\\实验图片\\\\'\n",
    "    t = time.strftime('%Y-%m-%d-%H', time.localtime(int(time.time())))\n",
    "    dirs = dirs + t\n",
    "    file = dirs + '\\\\' + file_name + '.png'\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    # 保存图片\n",
    "    plt.savefig(file)  # 保存图片\n",
    "\n",
    "\n",
    "def plot_value(y_true, y_pred):\n",
    "    balance_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision = sk.metrics.precision_score(y_true, y_pred)\n",
    "    recall = sk.metrics.recall_score(y_true, y_pred)\n",
    "    f1_value = sk.metrics.f1_score(y_true, y_pred)\n",
    "    return balance_accuracy, precision, recall, f1_value\n",
    "\n",
    "\n",
    "def autolabel(rects, ax):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "def plot_graphs(history_loc, type_loc):\n",
    "    plt.cla()\n",
    "\n",
    "    plt.plot(history_loc.history[type_loc])\n",
    "    plt.plot(history_loc.history['val_' + type_loc])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(type_loc)\n",
    "    plt.legend([type_loc, 'val_' + type_loc])\n",
    "    save_pic(plt, type_loc)\n",
    "    # plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def str_revert_to_list(methods_padding, max_len):\n",
    "    methods_padding_list = np.empty(shape=[0, max_len], dtype=int)\n",
    "\n",
    "    for methods_padding_one in methods_padding:\n",
    "        list1 = methods_padding_one.split(',')\n",
    "        list1 = list(map(int, list1))\n",
    "        methods_padding_list = np.append(methods_padding_list, [np.array(list1)], axis=0)  # 添加整行元素，axis=1添加整列元素\n",
    "    return methods_padding_list\n",
    "\n",
    "\n",
    "def get_object(save_absolute_path):\n",
    "    summer_load = None\n",
    "    with open(save_absolute_path, 'rb') as f:\n",
    "        summer_load = pickle.load(f)  # read file and build object\n",
    "    return summer_load\n",
    "\n",
    "\n",
    "def save_object(summer_save, save_absolute_path):\n",
    "    summer_save = pickle.dumps(summer_save)\n",
    "    with open(save_absolute_path, 'wb') as f:  # open file with write-mode\n",
    "        f.write(summer_save)  # serialize and save object\n",
    "\n",
    "\n",
    "def get_parent(vec_chair):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select parentId from data_model_2 where seq=\" + str(vec_chair[len(vec_chair) - 1]) + \";\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    if oneRow[0] == 0:\n",
    "        return vec_chair\n",
    "    else:\n",
    "        vec_chair.append(oneRow[0])\n",
    "        return get_parent(vec_chair)\n",
    "\n",
    "\n",
    "def get_chair_vec(vec_chair, i,maxlen):\n",
    "    cur = conn.cursor()\n",
    "    one_leaf_list = []\n",
    "    leaf = True\n",
    "    message_type = i\n",
    "    for one_leaf in vec_chair:\n",
    "        sqli = \"select a.vectorSemantic,REPLACE(REPLACE(vectorStruct,'[',''),']',''),logNum ,REPLACE(REPLACE(syntacticMessage,'[',''),']','') \" \\\n",
    "               \"from data_model_2 a  where seq=\" + str(one_leaf) + \";\"\n",
    "        result = cur.execute(sqli)\n",
    "        oneRow = cur.fetchone()\n",
    "        vectorSemantic = oneRow[0]\n",
    "        vectorStruct = oneRow[1]\n",
    "        logNum = oneRow[2]\n",
    "        syntacticMessage = oneRow[3]\n",
    "        if vectorSemantic != \"[]\":\n",
    "            if message_type == 1 or message_type == 3:\n",
    "                if vectorSemantic is not None :\n",
    "                    if len(vectorSemantic)==0:\n",
    "                        vectorSemantic='0'\n",
    "                    list_semantic = vectorSemantic.split(',')\n",
    "                    list_semantic = list(map(float, list_semantic))\n",
    "                    list_semantic_tmp = pad_sequences([list_semantic], maxlen=maxlen,\n",
    "                                                      padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "                    list_semantic = list_semantic_tmp[0]\n",
    "                    one_leaf_list.extend(list_semantic)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "            if message_type == 2:\n",
    "                if syntacticMessage is not None :\n",
    "                    if len(syntacticMessage)==0:\n",
    "                        syntacticMessage='0'\n",
    "                    list_syntactic = syntacticMessage.split(',')\n",
    "                    list_syntactic = list(map(float, list_syntactic))\n",
    "                    list_syntactic_tmp = pad_sequences([list_syntactic], maxlen=maxlen,\n",
    "                                                       padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "                    list_syntactic = list_syntactic_tmp[0]\n",
    "                    one_leaf_list.extend(list_syntactic)\n",
    "    cur.close()\n",
    "    return one_leaf_list\n",
    "\n",
    "\n",
    "def print_message(message):\n",
    "    t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(int(time.time())))\n",
    "    print('时间：' + t, end=\" \", flush=True)\n",
    "    print(message, flush=True)\n",
    "\n",
    "\n",
    "\n",
    "def repetition_list(a):\n",
    "    repetition = 6\n",
    "    c = []\n",
    "    for a_one in a:\n",
    "        b = []\n",
    "        for a_single in a_one:\n",
    "            for i in range(repetition):\n",
    "                b.append(a_single)\n",
    "        c.append(b)\n",
    "    return c\n",
    "\n",
    "\n",
    "def overlap_channel(a, b):\n",
    "    element_num = 0\n",
    "    height = 0\n",
    "    width = 0\n",
    "    if len(a.shape) == 2:\n",
    "        element_num = a.shape[0]\n",
    "        height = 1\n",
    "        width = a.shape[1]\n",
    "    elif len(a.shape) == 3:\n",
    "        element_num = a.shape[0]\n",
    "        height = a.shape[1]\n",
    "        width = a.shape[2]\n",
    "    else:\n",
    "        raise Exception(print_message('通道叠加 不支持这类维度:' + str(a.shape)))\n",
    "\n",
    "    a = a.reshape(element_num, 1, height, width)\n",
    "    b = b.reshape(element_num, 1, height, width)\n",
    "    a = a.transpose(1, 0, 2, 3)\n",
    "    b = b.transpose(1, 0, 2, 3)\n",
    "    c = np.vstack((a, b))\n",
    "    c = c.transpose(1, 2, 3, 0)\n",
    "    return c\n",
    "\n",
    "\n",
    "def check_list_exist(A, B):\n",
    "    exist = False\n",
    "    for row in A:\n",
    "        if (row == B).all():\n",
    "            exist = True\n",
    "    return exist\n",
    "\n",
    "\n",
    "def get_convert_data(A, B):\n",
    "    C = []\n",
    "    for A_one in A:\n",
    "        for B_one in B:\n",
    "            if (np.array(A_one) == np.array(B_one)).all():\n",
    "                C.append(A_one)\n",
    "    return C\n",
    "\n",
    "\n",
    "def cosDist(A, B):\n",
    "    return pdist(np.vstack([A, B]), 'cosine')  # np.sqrt(sum(np.power((A - B), 2)))\n",
    "\n",
    "\n",
    "def get_normal_data(data_point, data_lable):\n",
    "    neigh_num_a = 5\n",
    "    diff_rate_b = 1\n",
    "    can_stop = 0.99\n",
    "\n",
    "    # print_message(len(data_point))\n",
    "\n",
    "    abnormal_old = []\n",
    "    abnormal_new_ = []\n",
    "    for i in range(0, 100):\n",
    "        for one_point, one_lable in zip(data_point, data_lable):\n",
    "            neighbour_point = []\n",
    "            neighbour_lable = []\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(data_point, data_lable):\n",
    "                if check_list_exist(abnormal_new_, one_point_neighbour) or (one_point_neighbour == one_point).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    this_point_distance = cosDist(one_point_neighbour, one_point)\n",
    "                    if len(neighbour_point) >= neigh_num_a \\\n",
    "                            and this_point_distance < neighbour_point[neigh_num_a - 1] \\\n",
    "                            or len(neighbour_point) < neigh_num_a:\n",
    "                        if len(neighbour_point) < neigh_num_a:\n",
    "                            neighbour_point.append(this_point_distance)\n",
    "                            neighbour_lable.append(one_lable_neighbour)\n",
    "                        else:\n",
    "                            neighbour_point[neigh_num_a - 1] = (this_point_distance)\n",
    "                            neighbour_lable[neigh_num_a - 1] = (one_lable_neighbour)\n",
    "                        # 按照距离排序\n",
    "                        zipped = zip(neighbour_point, neighbour_lable)\n",
    "                        sort_zipped = sorted(zipped)\n",
    "                        result = zip(*sort_zipped)\n",
    "                        neighbour_point, neighbour_lable = [list(x) for x in result]\n",
    "\n",
    "            same_num = 0\n",
    "            diff_num = 0\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(neighbour_point[0:neigh_num_a],\n",
    "                                                                neighbour_lable[0:neigh_num_a]):\n",
    "                if one_lable_neighbour != one_lable:\n",
    "                    diff_num += 1\n",
    "                else:\n",
    "                    same_num += 1\n",
    "            diff_rate = diff_num / (same_num + diff_num)\n",
    "            if diff_rate >= diff_rate_b:\n",
    "                abnormal_new_.append([one_point])\n",
    "        if max(len(abnormal_new_), len(abnormal_old)) != 0 and \\\n",
    "                len(get_convert_data(abnormal_new_, abnormal_old)) / max(len(abnormal_new_),\n",
    "                                                                         len(abnormal_old)) >= can_stop:\n",
    "            break\n",
    "        else:\n",
    "            abnormal_old = abnormal_new_[:]\n",
    "        # print_message('单次noise数据量：' + str(len(abnormal_old)))\n",
    "\n",
    "    data_point_clear = []\n",
    "    data_lable_clear = []\n",
    "\n",
    "    for one_point, one_lable in zip(data_point, data_lable):\n",
    "        if (not check_list_exist(abnormal_old, one_point)):\n",
    "            data_point_clear.append(one_point)\n",
    "            data_lable_clear.append(one_lable)\n",
    "    return data_point_clear, data_lable_clear, abnormal_old\n",
    "\n",
    "\n",
    "##仅支持0 - 1 分类\n",
    "def change_abnormal(data_point, data_lable):\n",
    "    neigh_num_a = 10\n",
    "    diff_rate_b = 1\n",
    "    can_stop = 0.99\n",
    "\n",
    "    # print_message(len(data_point))\n",
    "\n",
    "    abnormal_old = []\n",
    "    abnormal_new_ = []\n",
    "    for i in range(0, 100):\n",
    "        for one_point, one_lable in zip(data_point, data_lable):\n",
    "            neighbour_point = []\n",
    "            neighbour_lable = []\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(data_point, data_lable):\n",
    "                if check_list_exist(abnormal_new_, one_point_neighbour) or (one_point_neighbour == one_point).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    this_point_distance = cosDist(one_point_neighbour, one_point)\n",
    "                    if len(neighbour_point) >= neigh_num_a \\\n",
    "                            and this_point_distance < neighbour_point[neigh_num_a - 1] \\\n",
    "                            or len(neighbour_point) < neigh_num_a:\n",
    "                        if len(neighbour_point) < neigh_num_a:\n",
    "                            neighbour_point.append(this_point_distance)\n",
    "                            neighbour_lable.append(one_lable_neighbour)\n",
    "                        else:\n",
    "                            neighbour_point[neigh_num_a - 1] = (this_point_distance)\n",
    "                            neighbour_lable[neigh_num_a - 1] = (one_lable_neighbour)\n",
    "                        # 按照距离排序\n",
    "                        zipped = zip(neighbour_point, neighbour_lable)\n",
    "                        sort_zipped = sorted(zipped)\n",
    "                        result = zip(*sort_zipped)\n",
    "                        neighbour_point, neighbour_lable = [list(x) for x in result]\n",
    "\n",
    "            same_num = 0\n",
    "            diff_num = 0\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(neighbour_point[0:neigh_num_a],\n",
    "                                                                neighbour_lable[0:neigh_num_a]):\n",
    "                if one_lable_neighbour != one_lable:\n",
    "                    diff_num += 1\n",
    "                else:\n",
    "                    same_num += 1\n",
    "            diff_rate = diff_num / (same_num + diff_num)\n",
    "            if diff_rate >= diff_rate_b:\n",
    "                abnormal_new_.append([one_point])\n",
    "        if max(len(abnormal_new_), len(abnormal_old)) != 0 and \\\n",
    "                len(get_convert_data(abnormal_new_, abnormal_old)) / max(len(abnormal_new_),\n",
    "                                                                         len(abnormal_old)) >= can_stop:\n",
    "            break\n",
    "        else:\n",
    "            abnormal_old = abnormal_new_[:]\n",
    "        # print_message('单次noise数据量：' + str(len(abnormal_old)))\n",
    "\n",
    "    data_point_clear = []\n",
    "    data_lable_clear = []\n",
    "\n",
    "    for one_point, one_lable in zip(data_point, data_lable):\n",
    "        if (not check_list_exist(abnormal_old, one_point)):\n",
    "            data_point_clear.append(one_point)\n",
    "            data_lable_clear.append(one_lable)\n",
    "        else:\n",
    "            ##修改noise标签\n",
    "            data_point_clear.append(one_point)\n",
    "            if one_lable == 0:\n",
    "                data_lable_clear.append(1)\n",
    "            else:\n",
    "                data_lable_clear.append(0)\n",
    "    return data_point_clear, data_lable_clear, abnormal_old\n",
    "\n",
    "\n",
    "def clear_data(train_vec_tmp, train_labels, zero_index):\n",
    "    group_num = 300\n",
    "    train_vec_tmp_clear = []\n",
    "    train_labels_clear = []\n",
    "    abnormal_num = 0\n",
    "    for b in range(int(len(train_vec_tmp) / group_num) + 1):\n",
    "        begin = b * group_num + zero_index\n",
    "        end = (b + 1) * group_num\n",
    "        if end > len(train_vec_tmp):\n",
    "            end = len(train_vec_tmp)\n",
    "        train_vec_group_one, train_labels_group_one, abnormal_old = change_abnormal(train_vec_tmp[begin:end],\n",
    "                                                                                    train_labels[begin:end])\n",
    "        train_vec_tmp_clear.extend(train_vec_group_one)\n",
    "        train_labels_clear.extend(train_labels_group_one)\n",
    "        abnormal_num = abnormal_num + len(abnormal_old)\n",
    "    print_message('noise数据量：' + str(abnormal_num))\n",
    "    print_message('训练总数据量：' + str(len(train_vec_tmp)))\n",
    "    train_vec_tmp = train_vec_tmp_clear\n",
    "    train_labels = train_labels_clear\n",
    "    return train_vec_tmp, train_labels\n",
    "\n",
    "\n",
    "def play_title_message():\n",
    "    playsound('D:/python/log_predict/file/结束提示.mp3')\n",
    "\n",
    "\n",
    "def get_tree_model_together(struct_vec_length_one,semantic_vec_length_one,syntatic_vec_length_one\n",
    "                            ,num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        keras.layers.InputLayer(struct_vec_length_one)\n",
    "    ])\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(semantic_vec_length_one, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(syntatic_vec_length_one, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model3.output, model2.output])\n",
    "    x = Dropout(0.4)(model_together)\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    final_output = Dense(24, activation='relu')(x)\n",
    "    model_together = Model(inputs=[model3.input, model2.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "\n",
    "\n",
    "def seperate_data(struct_leaf_vec, semantic_leaf_vec, syntatic_leaf_vec,\n",
    "                  lable, leaf,seed):\n",
    "    struct_train_vec, struct_test_vec, \\\n",
    "    semantic_train_vec, semantic_test_vec, \\\n",
    "    syntatic_train_vec, syntatic_test_vec, \\\n",
    "    struct_train_labels, struct_test_labels, \\\n",
    "    struct_train_leaf, struct_test_leaf = train_test_split(\n",
    "        struct_leaf_vec, semantic_leaf_vec, syntatic_leaf_vec,\n",
    "        lable, leaf,\n",
    "        test_size=0.2,\n",
    "        random_state=seed)\n",
    "    struct_verify_vec, struct_test_vec, \\\n",
    "    semantic_verify_vec, semantic_test_vec, \\\n",
    "    syntatic_verify_vec, syntatic_test_vec, \\\n",
    "    struct_verify_labels, struct_test_labels, \\\n",
    "    struct_verify_leaf, struct_test_leaf = train_test_split(\n",
    "        struct_test_vec, semantic_test_vec, syntatic_test_vec,\n",
    "        struct_test_labels,\n",
    "        struct_test_leaf,\n",
    "        test_size=0.5,\n",
    "        random_state=seed)\n",
    "    return struct_train_vec, struct_test_vec, \\\n",
    "           semantic_train_vec, semantic_test_vec, \\\n",
    "           syntatic_train_vec, syntatic_test_vec, \\\n",
    "           struct_train_labels, struct_test_labels, \\\n",
    "           struct_verify_vec, semantic_verify_vec, \\\n",
    "           syntatic_verify_vec, struct_verify_labels, struct_verify_leaf\n",
    "\n",
    "def get_lable(conn, seq):\n",
    "    seq_db = 0\n",
    "    leaf = ''\n",
    "    lable = 0\n",
    "    sqli = \"select seq,leaf,CASE WHEN logNum > 0 THEN  1  ELSE 0 END from data_model_2 where seq=\" + seq\n",
    "    cur = conn.cursor()\n",
    "    result = cur.execute(sqli)\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        seq_db = oneRow[0]\n",
    "        leaf = oneRow[1]\n",
    "        lable = oneRow[2]\n",
    "    cur.close()\n",
    "    return seq_db, leaf, lable\n",
    "\n",
    "\n",
    "\n",
    "def get_feature_from_file(file_name, node_all, node_vec):\n",
    "    with open(file_name, 'r') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            one_row = line.split(' ')\n",
    "            if len(one_row) > 2:\n",
    "                node_all.append(one_row[0])\n",
    "                one_row[len(one_row) - 1] = one_row[len(one_row) - 1].replace('\\n', '')\n",
    "                one_row = list(map(float, one_row))\n",
    "                node_vec.append(list(map(float, one_row[1:])))\n",
    "            line = file.readline()\n",
    "    return node_all, node_vec\n",
    "\n",
    "\n",
    "def convert_metric_to_list(project_name, feature_type, percentage, metric_type, tool_type):\n",
    "    one_row = []\n",
    "    one_row.append(project_name)\n",
    "    one_row.append(feature_type)\n",
    "    one_row.append(percentage)\n",
    "    one_row.append(metric_type)\n",
    "    one_row.append(tool_type)\n",
    "    return one_row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_call_graph(graph_leng, num_word, embedding_dim):\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Convolution1D(input_shape=(graph_leng, 1), filters=32, kernel_size=5, strides=1, padding='same',\n",
    "                      activation='relu', name='call-message'),\n",
    "        MaxPooling1D(pool_size=2, strides=2, padding='same', ),\n",
    "        Convolution1D(64, 5, strides=1, padding='same', activation='relu'),\n",
    "        MaxPooling1D(2, 2, 'same'),\n",
    "        Flatten(),\n",
    "        Dense(69, activation='relu'),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model3\n",
    "\n",
    "def get_method_name_model(method_name_x, method_name_y, num_word, embedding_dim):\n",
    "\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        keras.layers.Masking(mask_value=0, input_shape=(method_name_x, method_name_y)),\n",
    "        LSTM(units=8,activation='sigmoid'),\n",
    "        #         LSTM(units=32, activation='tanh'),\n",
    "        #         Flatten(),\n",
    "        Dense(53, activation='relu'),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model1\n",
    "\n",
    "def get_tree_model_together(semantic_vec_length_chain,syntatic_vec_length_chain,num_word,embedding_dim):\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(semantic_vec_length_chain, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(syntatic_vec_length_chain, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        #         LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        #         Flatten(),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model3.output, model2.output])\n",
    "    x = Dropout(0.2)(model_together)\n",
    "    final_output = Dense(43, activation='relu')(x)\n",
    "    model_together = Model(inputs=[model3.input, model2.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "\n",
    "def get_method_name_call_graph_model(method_dem_x,method_dem_y,call_graph_len,num_word,embedding_dim):\n",
    "    method_name_model = get_method_name_model(method_dem_x,method_dem_y,  num_word, embedding_dim)\n",
    "    call_graph_model = get_call_graph(call_graph_len,  num_word, embedding_dim)\n",
    "    m_j_merged_a = keras.layers.concatenate([method_name_model.output,call_graph_model.output],axis=-1)\n",
    "    dense1_a = Dense(4,activation='tanh')(m_j_merged_a)\n",
    "    final_output = Dropout(0.2)(dense1_a)\n",
    "    model_together_final = Model(inputs=[ method_name_model.input,call_graph_model.input],outputs=final_output)\n",
    "    return model_together_final\n",
    "\n",
    "\n",
    "def get_featrue_and_label(project_name):\n",
    "    # 1-语法 2-语义 3-结合(本树+调用关系)\n",
    "    # 4-语法（新）5-纯调用关系 6-结合(本树)\n",
    "    select_model = 6\n",
    "    semantic_vec_length = 50\n",
    "    tree_deep=4\n",
    "    semantic_vec_length_chain =  semantic_vec_length* tree_deep\n",
    "    syntatic_vec_length_chain = 30 * tree_deep\n",
    "    bro_vec_length_semantic = 0  # 304\n",
    "    # 获取每个叶子节点\n",
    "    print_message(\"获取节点\")\n",
    "    node = []\n",
    "    lable = []\n",
    "    semantic_leaf_one_node = []\n",
    "    semantic_leaf_vec = []\n",
    "    syntatic_leaf_vec = []\n",
    "    theme_leaf_vec = []\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    sqli = \"select seq,(select method_name_emb from data_model_1 where seq=a.methodSeq),\" \\\n",
    "           \"CASE WHEN logNum > 0 THEN  1  ELSE 0 END \" \\\n",
    "           \"from  data_model_2 a where  methodSeq in \" \\\n",
    "           \"(select seq from data_model_1 where projectName='\"+project_name+\"') \"\n",
    "    result = cur.execute(sqli)\n",
    "    # and methodSeq<59537\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        node.append(oneRow[0])\n",
    "        theme_leaf_vec.append(eval(oneRow[1]))\n",
    "        lable.append(oneRow[2])\n",
    "    print_message(\"获取节点特征\")\n",
    "    for one_leaf in node:\n",
    "        vec_chain = []\n",
    "        for i in range(1, 4):\n",
    "            if i == 1:\n",
    "                maxlen = semantic_vec_length_chain\n",
    "                vec_chain = []\n",
    "                vec_chain.append(one_leaf)\n",
    "                vec_chain = get_parent(vec_chain)\n",
    "            elif i==2:\n",
    "                maxlen = syntatic_vec_length_chain\n",
    "                vec_chain = []\n",
    "                vec_chain.append(one_leaf)\n",
    "                vec_chain = get_parent(vec_chain)\n",
    "            elif i==3:\n",
    "                maxlen=semantic_vec_length\n",
    "                vec_chain = []\n",
    "                vec_chain.append(one_leaf)\n",
    "\n",
    "            # 根据list中的记录查询节点记录，组合向量\n",
    "            one_leaf_vec = get_chair_vec(vec_chain, i,maxlen//tree_deep)\n",
    "            # 划分为等长向量\n",
    "            one_leaf_vec_padding_tmp = pad_sequences([one_leaf_vec], maxlen=maxlen,\n",
    "                                                     padding=\"post\", truncating=\"post\", dtype='float32')\n",
    "            one_leaf_vec_padding_tmp = one_leaf_vec_padding_tmp[0]\n",
    "\n",
    "            if i == 1:\n",
    "                semantic_leaf_vec.append(one_leaf_vec_padding_tmp)\n",
    "            elif i==2:\n",
    "                syntatic_leaf_vec.append(one_leaf_vec_padding_tmp)\n",
    "            elif i==3:\n",
    "                semantic_leaf_one_node.append(one_leaf_vec_padding_tmp)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print_message(\"完成\")\n",
    "    return node,semantic_leaf_vec,semantic_leaf_one_node,syntatic_leaf_vec,theme_leaf_vec,lable\n",
    "\n",
    "\n",
    "def get_feature_from_file(file_name, node_all, node_vec):\n",
    "    with open(file_name, 'r') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            one_row = line.split(' ')\n",
    "            if len(one_row) > 2:\n",
    "                node_all.append(one_row[0])\n",
    "                one_row[len(one_row) - 1] = one_row[len(one_row) - 1].replace('\\n', '')\n",
    "                one_row = list(map(float, one_row))\n",
    "                node_vec.append(list(map(float, one_row[1:])))\n",
    "            line = file.readline()\n",
    "    return node_all, node_vec\n",
    "\n",
    "\n",
    "\n",
    "def build_data(node,semantic_vec,edge,lable):\n",
    "    features_matrix_tmp=[]\n",
    "    for one_node,feature_value in zip(node,semantic_vec):\n",
    "        features_matrix_tmp.append(feature_value)\n",
    "\n",
    "    features_matrix=np.array(features_matrix_tmp)\n",
    "    a = {}\n",
    "    index=0\n",
    "    for i in features_matrix:\n",
    "        a[index]=[int(i) for i in i.tolist()]\n",
    "        index+=1\n",
    "\n",
    "\n",
    "    max_feature = np.max([v for v_list in a.values() for v in v_list])\n",
    "    features_matrix = np.zeros(shape = (len(list(a.keys())), max_feature+1))\n",
    "\n",
    "    i = 0\n",
    "    for k, vs in tqdm(a.items()):\n",
    "        for v in vs:\n",
    "            features_matrix[i, v] = 1\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "    class data:\n",
    "        x=[]\n",
    "        y=[]\n",
    "        edge_index=[]\n",
    "        train_mask=[]\n",
    "        val_mask=[]\n",
    "        test_mask=[]\n",
    "\n",
    "    data=data()\n",
    "    node=node\n",
    "    label=lable\n",
    "    edge=edge\n",
    "    feature=[]\n",
    "\n",
    "    feature = features_matrix\n",
    "    # for i in node:\n",
    "    #     feature.append(seq_feature[i])\n",
    "\n",
    "    train_rate=0.6\n",
    "    val_rate=0.2\n",
    "    test_rate=0.2\n",
    "    number=0\n",
    "    node_new={}\n",
    "    for i in node:\n",
    "        node_new[i]=number\n",
    "        number=number+1\n",
    "    data.x=torch.Tensor(feature)\n",
    "    edge_new=[[],[]]\n",
    "    for i in edge:\n",
    "        edge_new[0].append(node_new[i[0]])\n",
    "        edge_new[1].append(node_new[i[1]])\n",
    "\n",
    "    data.edge_index=torch.LongTensor(edge_new)\n",
    "\n",
    "    data.y=torch.LongTensor(label)\n",
    "    train_num=int(len(node)*train_rate)\n",
    "    train_mask=[ False for i in node]\n",
    "    while train_num>0:\n",
    "        index=0\n",
    "        for i in node:\n",
    "            if train_num>0 and random.random()>=0.5 and not train_mask[index]:\n",
    "                train_mask[index]=True\n",
    "                train_num=train_num-1\n",
    "            index=index+1\n",
    "    data.train_mask=train_mask\n",
    "    test_num=int(len(node)*test_rate)\n",
    "\n",
    "    test_mask=[ False for i in node]\n",
    "    while test_num>0:\n",
    "        index=0\n",
    "        for i,j in zip(test_mask,train_mask):\n",
    "            if test_num>0 and not i and not j:\n",
    "                if random.random()>=0.5:\n",
    "                    test_mask[index]=True\n",
    "                    test_num=test_num-1\n",
    "            index=index+1\n",
    "\n",
    "    data.test_mask=test_mask\n",
    "    val_num=int(len(node)*val_rate)\n",
    "\n",
    "    val_mask=[ False for i in node]\n",
    "    while val_num>0:\n",
    "        index=0\n",
    "        for i,j,z in zip(val_mask,train_mask,test_mask):\n",
    "            if val_num>0 and not i and not j and not z:\n",
    "                if random.random()>=0.5:\n",
    "                    val_mask[index]=True\n",
    "                    val_num=val_num-1\n",
    "            index=index+1\n",
    "    data.val_mask=val_mask\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_node_indices(G, ids):\n",
    "    # find the indices of the nodes\n",
    "    node_ids = np.asarray(ids)\n",
    "    flat_node_ids = node_ids.reshape(-1)\n",
    "\n",
    "    flat_node_indices = G.node_ids_to_ilocs(flat_node_ids) # in-built function makes it really easy\n",
    "    # back to the original shape\n",
    "    node_indices = flat_node_indices.reshape(1, len(node_ids)) # add 1 extra dimension\n",
    "\n",
    "    return node_indices\n",
    "\n",
    "def splite_data(vec_x_sem,vec_x_syn,vec_x_graph,vec_theme,struc_vec,\n",
    "                vec_y,mask):\n",
    "    vec_sem=[]\n",
    "    vec_syn=[]\n",
    "    vec_graph=[]\n",
    "    vec_method_theme=[]\n",
    "    vec_struc=[]\n",
    "    lable=[]\n",
    "    for i,j,h,w,a,g,z  in zip(vec_x_sem,vec_x_syn,\n",
    "                              vec_x_graph,vec_theme,\n",
    "                              struc_vec,vec_y,\n",
    "                              mask):\n",
    "        if z:\n",
    "            vec_sem.append(i)\n",
    "            vec_syn.append(j)\n",
    "            vec_graph.append(h)\n",
    "            vec_method_theme.append(w)\n",
    "            vec_struc.append(a)\n",
    "            lable.append(g)\n",
    "    return vec_sem,vec_syn,vec_graph,vec_method_theme,vec_struc,lable\n",
    "def get_method_name_emb(seq):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select method_name_emb from data_model_1 where seq=( select methodSeq from data_model_2 where seq=\" + str(seq) + \")\"\n",
    "    result = cur.execute(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    emb=oneRow[0]\n",
    "    cur.close()\n",
    "    return eval(emb)\n",
    "\n",
    "\n",
    "# 得到多分枝模型\n",
    "def get_graph_model_4(semantic_vec,graph_leng,syntactic_len,method_name_x, method_name_y,\n",
    "                      num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(semantic_vec,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_leng,1)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        Flatten(),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model4 = tf.keras.models.Sequential([\n",
    "        keras.layers.Masking(mask_value=0, input_shape=(method_name_x, method_name_y)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    x = keras.layers.concatenate([model1.output, model2.output, model3.output,model4.output])\n",
    "#     model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "#     x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "#     x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model1.input, model2.input, model3.input,model4.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "def get_graph_model_3(semantic_vec,graph_leng,syntactic_len,\n",
    "                      num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(semantic_vec,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_leng,1)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        Flatten(),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    x = keras.layers.concatenate([model1.output, model2.output, model3.output])\n",
    "\n",
    "#     model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "#     x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "#     x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model1.input, model2.input, model3.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "def get_graph_model_2(semantic_vec,syntactic_len,\n",
    "                      num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(semantic_vec,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        Input(shape=(syntactic_len,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    x = keras.layers.concatenate([model1.output, model3.output])\n",
    "#     model_together=Lambda(lambda x:keras.backend.expand_dims(x,axis=1))(model_together)\n",
    "\n",
    "#     x = LSTM(units=32, activation='tanh', return_sequences=True)(model_together)\n",
    "#     x=keras.layers.GlobalMaxPool1D()(x)\n",
    "    final_output = Dropout(0.2)(x)\n",
    "    final_output = Dense(2, activation='softmax')(final_output)\n",
    "    model_together = Model(inputs=[model1.input,model3.input], outputs=final_output)\n",
    "    return model_together\n",
    "def get_graph_model_1(semantic_vec,\n",
    "                      num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(semantic_vec,1)),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        (Dropout(0.2)),\n",
    "        Dense(2, activation='softmax'),\n",
    "    ])\n",
    "    return model1\n",
    "def get_graph_model_gcn(graph_vec_len,\n",
    "                        num_word,embedding_dim):\n",
    "    model1 = tf.keras.models.Sequential([\n",
    "        Input(shape=(graph_vec_len,1)),\n",
    "        LSTM(units=32, activation='tanh', return_sequences=True),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "        (Dropout(0.2)),\n",
    "        Dense(2, activation='softmax'),\n",
    "\n",
    "    ])\n",
    "    return model1\n",
    "def save_performance(content,ac,pr,re,f1):\n",
    "    f = open(\"print-message.txt\", mode='a')\n",
    "    f.write(str(datetime.datetime.now())+'\\t'+ content+' \\t'+str(ac)+\n",
    "            '\\t'+str(pr)+ '\\t'+str(re)+'\\t'+ str(f1)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719c1c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间：2024-08-15 19:03:32 获取节点\n",
      "时间：2024-08-15 19:28:25 获取节点特征\n",
      "时间：2024-08-15 19:33:30 完成\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epoch = 100  # 训练周期\n",
    "feature_length = \"10\"\n",
    "one_project = 'zeppelin-master'\n",
    "node_all = []\n",
    "node_vec = []\n",
    "\n",
    "node, semantic_vec,semantic_vec_one_node, syntatic_vec,leaf_method_name_vec,lable = get_featrue_and_label(one_project)\n",
    "\n",
    "\n",
    "semantic_vec_one_node_tmp=[]\n",
    "for one in semantic_vec_one_node:\n",
    "    tmp=[ int(i) for i in one if i!=0]\n",
    "    semantic_vec_one_node_tmp.append(tmp)\n",
    "semantic_vec_one_node=semantic_vec_one_node_tmp\n",
    "\n",
    "randomValue=int(random.random()*100)\n",
    "node, semantic_vec,semantic_vec_one_node, syntatic_vec,leaf_method_name_vec,lable=shuffle_data(node, semantic_vec,semantic_vec_one_node, syntatic_vec,leaf_method_name_vec,lable,randomValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee23624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"D:\\\\研究生\\\\研究内容\\\\研究生开题\\\\\" \\\n",
    "            \"工作空间\\\\最终版实验程序和数据备份\\\\\" \\\n",
    "            \"dataset\\\\struc2vec-vec\\\\barbell-\" + feature_length + \"-\" + one_project + \".emb\"\n",
    "node_all, node_vec = get_feature_from_file(file_name, node_all, node_vec)\n",
    "tmp={}\n",
    "for one ,vec in zip(node_all,node_vec):\n",
    "    tmp[int(one)]=vec\n",
    "struc_vec=[]\n",
    "struc_vec_dict={}\n",
    "for i,j in zip(node_all,node_vec):\n",
    "    struc_vec_dict[int(i)]=j\n",
    "for i in node:\n",
    "    struc_vec.append(struc_vec_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc12dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_vec_tmp=np.array(semantic_vec).astype(int)\n",
    "syntatic_vec_tmp=np.array(syntatic_vec).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7eb7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_all=[]\n",
    "for i, j in zip(semantic_vec_tmp,syntatic_vec_tmp):\n",
    "    tmp=[]\n",
    "    tmp.extend(i)\n",
    "    tmp.extend(j)\n",
    "    feature_all.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec298030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 57818/57818 [00:06<00:00, 8938.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 57818, Edges: 65082\n",
      "\n",
      " Node types:\n",
      "  default: [57818]\n",
      "    Features: float32 vector, length 6948\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [65082]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n",
      "(34690, 3) (11564, 3) (11564, 3)\n"
     ]
    }
   ],
   "source": [
    "# 得到图\n",
    "number=0\n",
    "node_new={}\n",
    "for i in node:\n",
    "    node_new[i]=number\n",
    "    number=number+1\n",
    "\n",
    "edge=[]#[[28,16],[16,85],[85,46],[28,16],[16,85],[85,46],[16,85],[85,46]]\n",
    "\n",
    "conn = get_connection()\n",
    "\n",
    "cur = conn.cursor()\n",
    "sqli = \"select parentId,seq \" \\\n",
    "       \"from data_model_2 \" \\\n",
    "       \"where methodSeq in \" \\\n",
    "       \"(select seq from data_model_1 where projectName='\"+one_project+\"') \" \\\n",
    "       \"and parentId<>0   \" \\\n",
    "       \"union \" \\\n",
    "       \"select callBlockSeq,calledBlockSeq \" \\\n",
    "       \"from call_graph_data \" \\\n",
    "       \"where callMethodSeq in \" \\\n",
    "    \"(select seq from data_model_1 where projectName='\"+one_project+\"')   \"\n",
    "result = cur.execute(sqli)\n",
    "#\n",
    "#and callMethodSeq<59537 and calledMethodSeq<59537\n",
    "for i in range(result):\n",
    "    oneRow = cur.fetchone()\n",
    "    edge_tmp=[]\n",
    "    edge_tmp.append(node_new[oneRow[0]])\n",
    "    edge_tmp.append(node_new[oneRow[1]])\n",
    "    edge.append(edge_tmp)\n",
    "cur.close()\n",
    "conn.close()\n",
    "#构建图 单语义节点\n",
    "features = {}\n",
    "index=0\n",
    "index_all=[]\n",
    "for feature_one in feature_all:\n",
    "    features[index] = feature_one\n",
    "    index_all.append(index)\n",
    "    index+=1\n",
    "max_feature = np.max([v for v_list in features.values() for v in v_list])\n",
    "features_matrix = np.zeros(shape = (len(list(features.keys())), max_feature+1))\n",
    "\n",
    "i = 0\n",
    "for k, vs in tqdm(features.items()):\n",
    "    for v in vs:\n",
    "        features_matrix[i, v] = 1\n",
    "    i+=1\n",
    "\n",
    "node_features = pd.DataFrame(features_matrix, index = features.keys()) # into dataframe for StellarGraph\n",
    "\n",
    "edge = pd.DataFrame(edge,columns=['source', 'target']) # 将第一维度数据转为为行，第二维度数据转化为列，即 3 行 2 列，并设置列标签\n",
    "edge.columns = ['source', 'target']\n",
    "\n",
    "\n",
    "G = sg.StellarGraph(node_features, edge)\n",
    "print(G.info())\n",
    "targets = pd.DataFrame(data=None,columns=['id','name','ml_target']) # into dataframe for StellarGraph\n",
    "import pandas as pd\n",
    "\n",
    "index=0\n",
    "for key,label_tmp in zip(features.keys(),lable):\n",
    "    targets.loc[index] = [key,'-',label_tmp] \n",
    "    index=index+1\n",
    "# targets.columns = ['name', 'ml_target']\n",
    "targets.index = targets.id\n",
    "targets = targets.loc[features.keys(), :]\n",
    "targets.index = targets.id.astype(str)\n",
    "train_pages, test_pages = train_test_split(targets, test_size=0.4,random_state=73)\n",
    "val_pages, test_pages = train_test_split(test_pages, test_size=0.5,random_state=73)\n",
    "print(train_pages.shape, val_pages.shape, test_pages.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "target_encoding = LabelBinarizer()\n",
    "train_targets = target_encoding.fit_transform(np.int64(train_pages['ml_target']))\n",
    "val_targets = target_encoding.transform(np.int64(val_pages['ml_target']))\n",
    "test_targets = target_encoding.transform(np.int64(test_pages['ml_target']))\n",
    "\n",
    "# 得到gcn特征\n",
    "\n",
    "# Get the adjacency matrix\n",
    "A = G.to_adjacency_matrix(weighted=False)\n",
    "# Add self-connections\n",
    "A_t = A + scipy.sparse.diags(np.ones(A.shape[0]) - A.diagonal())\n",
    "# Degree matrix to the power of -1/2\n",
    "D_t = scipy.sparse.diags(np.power(np.array(A.sum(1)), -0.5).flatten(), 0)\n",
    "# Normalise the Adjacency matrix\n",
    "A_norm = A.dot(D_t).transpose().dot(D_t).todense()\n",
    "A_input = np.expand_dims(A_norm, 0)\n",
    "\n",
    "\n",
    "# Define the function to get these indices\n",
    "def get_node_indices(G, ids):\n",
    "    # find the indices of the nodes\n",
    "    node_ids = np.asarray(ids)\n",
    "    flat_node_ids = node_ids.reshape(-1)\n",
    "\n",
    "    flat_node_indices = G.node_ids_to_ilocs(flat_node_ids) # in-built function makes it really easy\n",
    "    # back to the original shape\n",
    "    node_indices = flat_node_indices.reshape(1, len(node_ids)) # add 1 extra dimension\n",
    "    \n",
    "    return node_indices\n",
    "\n",
    "# Get indices\n",
    "train_indices = get_node_indices(G, np.int64(train_pages.index))\n",
    "val_indices = get_node_indices(G, np.int64(val_pages.index))\n",
    "test_indices = get_node_indices(G, np.int64(test_pages.index))\n",
    "\n",
    "# Expand dimensions\n",
    "features_input = np.expand_dims(features_matrix, 0)\n",
    "A_input = np.expand_dims(A_norm, 0)\n",
    "\n",
    "y_train = np.expand_dims(train_targets, 0)\n",
    "y_val = np.expand_dims(val_targets, 0)\n",
    "y_test = np.expand_dims(test_targets, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1602984",
   "metadata": {},
   "source": [
    "# 旧工作 deeplog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ce4202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集切分正常\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 数据集切分\n",
    "print('数据集切分正常')\n",
    "all_indices = get_node_indices(G, np.int64(targets.index))\n",
    "\n",
    "vec_x_graph=[]\n",
    "for i in semantic_vec:\n",
    "    vec_x_graph.append(0)\n",
    "train_mask=[]\n",
    "val_mask=[]\n",
    "test_mask=[]\n",
    "for i in all_indices[0]:\n",
    "    if i in train_indices[0]:\n",
    "        train_mask.append(True)\n",
    "    else:\n",
    "        train_mask.append(False)\n",
    "    if i in val_indices[0]:\n",
    "        val_mask.append(True)\n",
    "    else:\n",
    "        val_mask.append(False)\n",
    "    if i in test_indices[0]:\n",
    "        test_mask.append(True)\n",
    "    else:\n",
    "        test_mask.append(False)\n",
    "train_vec_sem,train_vec_syn, \\\n",
    "train_vec_graph,train_vec_theme,train_vec_struct,train_lable=splite_data \\\n",
    "    (semantic_vec,syntatic_vec,vec_x_graph,leaf_method_name_vec,struc_vec,\n",
    "     lable,train_mask)\n",
    "val_vec_sem,val_vec_syn, \\\n",
    "val_vec_graph,val_vec_theme,val_vec_struct,val_lable=splite_data \\\n",
    "    (semantic_vec,syntatic_vec,vec_x_graph,leaf_method_name_vec,struc_vec,\n",
    "     lable,val_mask)\n",
    "test_vec_sem,test_vec_syn, \\\n",
    "test_vec_graph,test_vec_theme,test_vec_struct,test_lable=splite_data \\\n",
    "    (semantic_vec,syntatic_vec,vec_x_graph,leaf_method_name_vec,struc_vec,\n",
    "     lable,test_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d942f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5999861634785015\n",
      "0.1123378495243586\n"
     ]
    }
   ],
   "source": [
    "num_tmp=0\n",
    "for i in train_mask:\n",
    "    if i:\n",
    "        num_tmp+=1\n",
    "print(num_tmp/len(train_mask))\n",
    "num_tmp=0\n",
    "for i in train_lable:\n",
    "    if i == 1:\n",
    "        num_tmp+=1\n",
    "print(num_tmp/len(train_lable))\n",
    "train_lable_one_hot=tf.keras.utils.to_categorical(train_lable, num_classes=2)\n",
    "val_lable_one_hot=tf.keras.utils.to_categorical(val_lable, num_classes=2)\n",
    "test_lable_one_hot=tf.keras.utils.to_categorical(test_lable, num_classes=2)\n",
    "# y_train.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbd78e",
   "metadata": {},
   "source": [
    "# DeepLog1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85299217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1d_1/kernel:0', 'conv1d_1/bias:0', 'conv1d/kernel:0', 'conv1d/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1d_1/kernel:0', 'conv1d_1/bias:0', 'conv1d/kernel:0', 'conv1d/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1085/1085 - 27s - loss: 0.2161 - accuracy: 0.9073 - val_loss: 0.1767 - val_accuracy: 0.9167 - 27s/epoch - 24ms/step\n",
      "Epoch 2/100\n",
      "1085/1085 - 24s - loss: 0.1800 - accuracy: 0.9212 - val_loss: 0.1658 - val_accuracy: 0.9238 - 24s/epoch - 22ms/step\n",
      "Epoch 3/100\n",
      "1085/1085 - 24s - loss: 0.1695 - accuracy: 0.9243 - val_loss: 0.1657 - val_accuracy: 0.9278 - 24s/epoch - 22ms/step\n",
      "Epoch 4/100\n",
      "1085/1085 - 24s - loss: 0.1620 - accuracy: 0.9274 - val_loss: 0.1540 - val_accuracy: 0.9250 - 24s/epoch - 22ms/step\n",
      "Epoch 5/100\n",
      "1085/1085 - 24s - loss: 0.1561 - accuracy: 0.9290 - val_loss: 0.1553 - val_accuracy: 0.9308 - 24s/epoch - 22ms/step\n",
      "Epoch 6/100\n",
      "1085/1085 - 24s - loss: 0.1526 - accuracy: 0.9319 - val_loss: 0.1491 - val_accuracy: 0.9305 - 24s/epoch - 22ms/step\n",
      "Epoch 7/100\n",
      "1085/1085 - 24s - loss: 0.1475 - accuracy: 0.9318 - val_loss: 0.1430 - val_accuracy: 0.9306 - 24s/epoch - 22ms/step\n",
      "Epoch 8/100\n",
      "1085/1085 - 24s - loss: 0.1456 - accuracy: 0.9335 - val_loss: 0.1423 - val_accuracy: 0.9330 - 24s/epoch - 22ms/step\n",
      "Epoch 9/100\n",
      "1085/1085 - 24s - loss: 0.1429 - accuracy: 0.9357 - val_loss: 0.1425 - val_accuracy: 0.9339 - 24s/epoch - 22ms/step\n",
      "Epoch 10/100\n",
      "1085/1085 - 24s - loss: 0.1398 - accuracy: 0.9366 - val_loss: 0.1375 - val_accuracy: 0.9354 - 24s/epoch - 22ms/step\n",
      "Epoch 11/100\n",
      "1085/1085 - 24s - loss: 0.1385 - accuracy: 0.9372 - val_loss: 0.1364 - val_accuracy: 0.9351 - 24s/epoch - 22ms/step\n",
      "Epoch 12/100\n",
      "1085/1085 - 24s - loss: 0.1365 - accuracy: 0.9386 - val_loss: 0.1451 - val_accuracy: 0.9364 - 24s/epoch - 22ms/step\n",
      "Epoch 13/100\n",
      "1085/1085 - 24s - loss: 0.1347 - accuracy: 0.9379 - val_loss: 0.1315 - val_accuracy: 0.9383 - 24s/epoch - 22ms/step\n",
      "Epoch 14/100\n",
      "1085/1085 - 24s - loss: 0.1345 - accuracy: 0.9393 - val_loss: 0.1341 - val_accuracy: 0.9407 - 24s/epoch - 22ms/step\n",
      "Epoch 15/100\n",
      "1085/1085 - 24s - loss: 0.1309 - accuracy: 0.9395 - val_loss: 0.1312 - val_accuracy: 0.9406 - 24s/epoch - 22ms/step\n",
      "Epoch 16/100\n",
      "1085/1085 - 24s - loss: 0.1301 - accuracy: 0.9405 - val_loss: 0.1315 - val_accuracy: 0.9408 - 24s/epoch - 22ms/step\n",
      "Epoch 17/100\n",
      "1085/1085 - 24s - loss: 0.1277 - accuracy: 0.9413 - val_loss: 0.1309 - val_accuracy: 0.9388 - 24s/epoch - 22ms/step\n",
      "Epoch 18/100\n",
      "1085/1085 - 24s - loss: 0.1289 - accuracy: 0.9404 - val_loss: 0.1280 - val_accuracy: 0.9395 - 24s/epoch - 22ms/step\n",
      "Epoch 19/100\n",
      "1085/1085 - 24s - loss: 0.1278 - accuracy: 0.9424 - val_loss: 0.1289 - val_accuracy: 0.9373 - 24s/epoch - 22ms/step\n",
      "Epoch 20/100\n",
      "1085/1085 - 24s - loss: 0.1238 - accuracy: 0.9419 - val_loss: 0.1309 - val_accuracy: 0.9376 - 24s/epoch - 22ms/step\n",
      "Epoch 21/100\n",
      "1085/1085 - 24s - loss: 0.1226 - accuracy: 0.9436 - val_loss: 0.1280 - val_accuracy: 0.9416 - 24s/epoch - 22ms/step\n",
      "Epoch 22/100\n",
      "1085/1085 - 24s - loss: 0.1249 - accuracy: 0.9433 - val_loss: 0.1258 - val_accuracy: 0.9417 - 24s/epoch - 22ms/step\n",
      "Epoch 23/100\n",
      "1085/1085 - 24s - loss: 0.1231 - accuracy: 0.9443 - val_loss: 0.1259 - val_accuracy: 0.9402 - 24s/epoch - 22ms/step\n",
      "Epoch 24/100\n",
      "1085/1085 - 24s - loss: 0.1210 - accuracy: 0.9453 - val_loss: 0.1315 - val_accuracy: 0.9406 - 24s/epoch - 22ms/step\n",
      "Epoch 25/100\n",
      "1085/1085 - 24s - loss: 0.1237 - accuracy: 0.9451 - val_loss: 0.1251 - val_accuracy: 0.9424 - 24s/epoch - 22ms/step\n",
      "Epoch 26/100\n",
      "1085/1085 - 25s - loss: 0.1214 - accuracy: 0.9444 - val_loss: 0.1271 - val_accuracy: 0.9416 - 25s/epoch - 23ms/step\n",
      "Epoch 27/100\n",
      "1085/1085 - 24s - loss: 0.1198 - accuracy: 0.9450 - val_loss: 0.1268 - val_accuracy: 0.9436 - 24s/epoch - 22ms/step\n",
      "Epoch 28/100\n",
      "1085/1085 - 24s - loss: 0.1208 - accuracy: 0.9459 - val_loss: 0.1272 - val_accuracy: 0.9436 - 24s/epoch - 22ms/step\n",
      "Epoch 29/100\n",
      "1085/1085 - 24s - loss: 0.1213 - accuracy: 0.9452 - val_loss: 0.1272 - val_accuracy: 0.9429 - 24s/epoch - 22ms/step\n",
      "Epoch 30/100\n",
      "1085/1085 - 24s - loss: 0.1216 - accuracy: 0.9456 - val_loss: 0.1415 - val_accuracy: 0.9351 - 24s/epoch - 22ms/step\n",
      "Epoch 31/100\n",
      "1085/1085 - 24s - loss: 0.1190 - accuracy: 0.9463 - val_loss: 0.1252 - val_accuracy: 0.9413 - 24s/epoch - 22ms/step\n",
      "Epoch 32/100\n",
      "1085/1085 - 24s - loss: 0.1171 - accuracy: 0.9479 - val_loss: 0.1250 - val_accuracy: 0.9435 - 24s/epoch - 22ms/step\n",
      "Epoch 33/100\n",
      "1085/1085 - 24s - loss: 0.1178 - accuracy: 0.9456 - val_loss: 0.1240 - val_accuracy: 0.9437 - 24s/epoch - 22ms/step\n",
      "Epoch 34/100\n",
      "1085/1085 - 25s - loss: 0.1193 - accuracy: 0.9463 - val_loss: 0.1232 - val_accuracy: 0.9450 - 25s/epoch - 23ms/step\n",
      "Epoch 35/100\n",
      "1085/1085 - 24s - loss: 0.1180 - accuracy: 0.9469 - val_loss: 0.1255 - val_accuracy: 0.9437 - 24s/epoch - 22ms/step\n",
      "Epoch 36/100\n",
      "1085/1085 - 24s - loss: 0.1184 - accuracy: 0.9452 - val_loss: 0.1229 - val_accuracy: 0.9452 - 24s/epoch - 22ms/step\n",
      "Epoch 37/100\n",
      "1085/1085 - 24s - loss: 0.1176 - accuracy: 0.9464 - val_loss: 0.1210 - val_accuracy: 0.9439 - 24s/epoch - 22ms/step\n",
      "Epoch 38/100\n",
      "1085/1085 - 24s - loss: 0.1160 - accuracy: 0.9463 - val_loss: 0.1300 - val_accuracy: 0.9434 - 24s/epoch - 22ms/step\n",
      "Epoch 39/100\n",
      "1085/1085 - 24s - loss: 0.1156 - accuracy: 0.9486 - val_loss: 0.1216 - val_accuracy: 0.9442 - 24s/epoch - 22ms/step\n",
      "Epoch 40/100\n",
      "1085/1085 - 25s - loss: 0.1166 - accuracy: 0.9473 - val_loss: 0.1221 - val_accuracy: 0.9428 - 25s/epoch - 23ms/step\n",
      "Epoch 41/100\n",
      "1085/1085 - 24s - loss: 0.1139 - accuracy: 0.9476 - val_loss: 0.1229 - val_accuracy: 0.9453 - 24s/epoch - 22ms/step\n",
      "Epoch 42/100\n",
      "1085/1085 - 24s - loss: 0.1152 - accuracy: 0.9476 - val_loss: 0.1249 - val_accuracy: 0.9434 - 24s/epoch - 22ms/step\n",
      "Epoch 43/100\n",
      "1085/1085 - 24s - loss: 0.1129 - accuracy: 0.9494 - val_loss: 0.1230 - val_accuracy: 0.9441 - 24s/epoch - 22ms/step\n",
      "Epoch 44/100\n",
      "1085/1085 - 24s - loss: 0.1130 - accuracy: 0.9486 - val_loss: 0.1213 - val_accuracy: 0.9452 - 24s/epoch - 22ms/step\n",
      "Epoch 45/100\n",
      "1085/1085 - 24s - loss: 0.1136 - accuracy: 0.9481 - val_loss: 0.1262 - val_accuracy: 0.9447 - 24s/epoch - 22ms/step\n",
      "Epoch 46/100\n",
      "1085/1085 - 24s - loss: 0.1107 - accuracy: 0.9487 - val_loss: 0.1261 - val_accuracy: 0.9459 - 24s/epoch - 22ms/step\n",
      "Epoch 47/100\n",
      "1085/1085 - 24s - loss: 0.1103 - accuracy: 0.9493 - val_loss: 0.1230 - val_accuracy: 0.9434 - 24s/epoch - 22ms/step\n",
      "Epoch 48/100\n",
      "1085/1085 - 24s - loss: 0.1126 - accuracy: 0.9490 - val_loss: 0.1260 - val_accuracy: 0.9466 - 24s/epoch - 22ms/step\n",
      "Epoch 49/100\n",
      "1085/1085 - 24s - loss: 0.1144 - accuracy: 0.9493 - val_loss: 0.1233 - val_accuracy: 0.9428 - 24s/epoch - 22ms/step\n",
      "Epoch 50/100\n",
      "1085/1085 - 24s - loss: 0.1159 - accuracy: 0.9491 - val_loss: 0.1220 - val_accuracy: 0.9409 - 24s/epoch - 22ms/step\n",
      "Epoch 51/100\n",
      "1085/1085 - 24s - loss: 0.1130 - accuracy: 0.9485 - val_loss: 0.1184 - val_accuracy: 0.9460 - 24s/epoch - 22ms/step\n",
      "Epoch 52/100\n",
      "1085/1085 - 24s - loss: 0.1109 - accuracy: 0.9496 - val_loss: 0.1203 - val_accuracy: 0.9460 - 24s/epoch - 22ms/step\n",
      "Epoch 53/100\n",
      "1085/1085 - 24s - loss: 0.1117 - accuracy: 0.9491 - val_loss: 0.1235 - val_accuracy: 0.9432 - 24s/epoch - 22ms/step\n",
      "Epoch 54/100\n",
      "1085/1085 - 24s - loss: 0.1121 - accuracy: 0.9487 - val_loss: 0.1194 - val_accuracy: 0.9457 - 24s/epoch - 22ms/step\n",
      "Epoch 55/100\n",
      "1085/1085 - 24s - loss: 0.1106 - accuracy: 0.9506 - val_loss: 0.1215 - val_accuracy: 0.9442 - 24s/epoch - 22ms/step\n",
      "Epoch 56/100\n",
      "1085/1085 - 25s - loss: 0.1134 - accuracy: 0.9501 - val_loss: 0.1237 - val_accuracy: 0.9447 - 25s/epoch - 23ms/step\n",
      "Epoch 57/100\n",
      "1085/1085 - 24s - loss: 0.1104 - accuracy: 0.9505 - val_loss: 0.1257 - val_accuracy: 0.9429 - 24s/epoch - 22ms/step\n",
      "Epoch 58/100\n",
      "1085/1085 - 24s - loss: 0.1109 - accuracy: 0.9501 - val_loss: 0.1206 - val_accuracy: 0.9465 - 24s/epoch - 22ms/step\n",
      "Epoch 59/100\n",
      "1085/1085 - 24s - loss: 0.1072 - accuracy: 0.9513 - val_loss: 0.1187 - val_accuracy: 0.9463 - 24s/epoch - 22ms/step\n",
      "Epoch 60/100\n",
      "1085/1085 - 24s - loss: 0.1086 - accuracy: 0.9512 - val_loss: 0.1207 - val_accuracy: 0.9460 - 24s/epoch - 22ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "1085/1085 - 24s - loss: 0.1116 - accuracy: 0.9494 - val_loss: 0.1240 - val_accuracy: 0.9441 - 24s/epoch - 22ms/step\n",
      "Epoch 62/100\n",
      "1085/1085 - 24s - loss: 0.1090 - accuracy: 0.9500 - val_loss: 0.1267 - val_accuracy: 0.9446 - 24s/epoch - 22ms/step\n",
      "Epoch 63/100\n",
      "1085/1085 - 24s - loss: 0.1088 - accuracy: 0.9511 - val_loss: 0.1243 - val_accuracy: 0.9460 - 24s/epoch - 22ms/step\n",
      "Epoch 64/100\n",
      "1085/1085 - 24s - loss: 0.1108 - accuracy: 0.9494 - val_loss: 0.1226 - val_accuracy: 0.9457 - 24s/epoch - 22ms/step\n",
      "Epoch 65/100\n",
      "1085/1085 - 24s - loss: 0.1100 - accuracy: 0.9495 - val_loss: 0.1268 - val_accuracy: 0.9405 - 24s/epoch - 22ms/step\n",
      "Epoch 66/100\n",
      "1085/1085 - 25s - loss: 0.1078 - accuracy: 0.9503 - val_loss: 0.1264 - val_accuracy: 0.9446 - 25s/epoch - 23ms/step\n",
      "Epoch 67/100\n",
      "1085/1085 - 24s - loss: 0.1089 - accuracy: 0.9502 - val_loss: 0.1206 - val_accuracy: 0.9467 - 24s/epoch - 22ms/step\n",
      "Epoch 68/100\n",
      "1085/1085 - 24s - loss: 0.1095 - accuracy: 0.9513 - val_loss: 0.1227 - val_accuracy: 0.9472 - 24s/epoch - 22ms/step\n",
      "Epoch 69/100\n",
      "1085/1085 - 25s - loss: 0.1103 - accuracy: 0.9514 - val_loss: 0.1349 - val_accuracy: 0.9432 - 25s/epoch - 23ms/step\n",
      "Epoch 70/100\n",
      "1085/1085 - 24s - loss: 0.1088 - accuracy: 0.9504 - val_loss: 0.1218 - val_accuracy: 0.9451 - 24s/epoch - 22ms/step\n",
      "Epoch 71/100\n",
      "1085/1085 - 24s - loss: 0.1107 - accuracy: 0.9503 - val_loss: 0.1248 - val_accuracy: 0.9453 - 24s/epoch - 22ms/step\n",
      "Epoch 72/100\n",
      "1085/1085 - 24s - loss: 0.1085 - accuracy: 0.9514 - val_loss: 0.1255 - val_accuracy: 0.9459 - 24s/epoch - 22ms/step\n",
      "Epoch 73/100\n",
      "1085/1085 - 24s - loss: 0.1146 - accuracy: 0.9498 - val_loss: 0.1251 - val_accuracy: 0.9463 - 24s/epoch - 22ms/step\n",
      "Epoch 74/100\n",
      "1085/1085 - 24s - loss: 0.1076 - accuracy: 0.9505 - val_loss: 0.1222 - val_accuracy: 0.9460 - 24s/epoch - 22ms/step\n",
      "Epoch 75/100\n",
      "1085/1085 - 26s - loss: 0.1064 - accuracy: 0.9508 - val_loss: 0.1195 - val_accuracy: 0.9471 - 26s/epoch - 24ms/step\n",
      "Epoch 76/100\n",
      "1085/1085 - 27s - loss: 0.1091 - accuracy: 0.9516 - val_loss: 0.1237 - val_accuracy: 0.9456 - 27s/epoch - 25ms/step\n",
      "Epoch 77/100\n",
      "1085/1085 - 25s - loss: 0.1089 - accuracy: 0.9494 - val_loss: 0.1203 - val_accuracy: 0.9466 - 25s/epoch - 23ms/step\n",
      "Epoch 78/100\n",
      "1085/1085 - 24s - loss: 0.1091 - accuracy: 0.9506 - val_loss: 0.1250 - val_accuracy: 0.9469 - 24s/epoch - 22ms/step\n",
      "Epoch 79/100\n",
      "1085/1085 - 24s - loss: 0.1104 - accuracy: 0.9510 - val_loss: 0.1198 - val_accuracy: 0.9458 - 24s/epoch - 22ms/step\n",
      "Epoch 80/100\n",
      "1085/1085 - 24s - loss: 0.1052 - accuracy: 0.9522 - val_loss: 0.1234 - val_accuracy: 0.9471 - 24s/epoch - 22ms/step\n",
      "Epoch 81/100\n",
      "1085/1085 - 24s - loss: 0.1103 - accuracy: 0.9516 - val_loss: 0.1262 - val_accuracy: 0.9481 - 24s/epoch - 22ms/step\n",
      "Epoch 82/100\n",
      "1085/1085 - 24s - loss: 0.1122 - accuracy: 0.9511 - val_loss: 0.1241 - val_accuracy: 0.9434 - 24s/epoch - 22ms/step\n",
      "Epoch 83/100\n",
      "1085/1085 - 24s - loss: 0.1120 - accuracy: 0.9518 - val_loss: 0.1219 - val_accuracy: 0.9447 - 24s/epoch - 22ms/step\n",
      "Epoch 84/100\n",
      "1085/1085 - 24s - loss: 0.1061 - accuracy: 0.9521 - val_loss: 0.1225 - val_accuracy: 0.9417 - 24s/epoch - 22ms/step\n",
      "Epoch 85/100\n",
      "1085/1085 - 24s - loss: 0.1075 - accuracy: 0.9509 - val_loss: 0.1259 - val_accuracy: 0.9465 - 24s/epoch - 22ms/step\n",
      "Epoch 86/100\n",
      "1085/1085 - 24s - loss: 0.1063 - accuracy: 0.9504 - val_loss: 0.1225 - val_accuracy: 0.9450 - 24s/epoch - 22ms/step\n",
      "Epoch 87/100\n",
      "1085/1085 - 24s - loss: 0.1093 - accuracy: 0.9506 - val_loss: 0.1277 - val_accuracy: 0.9454 - 24s/epoch - 22ms/step\n",
      "Epoch 88/100\n",
      "1085/1085 - 24s - loss: 0.1069 - accuracy: 0.9512 - val_loss: 0.1238 - val_accuracy: 0.9431 - 24s/epoch - 22ms/step\n",
      "Epoch 89/100\n",
      "1085/1085 - 24s - loss: 0.1107 - accuracy: 0.9504 - val_loss: 0.1227 - val_accuracy: 0.9447 - 24s/epoch - 22ms/step\n",
      "Epoch 90/100\n",
      "1085/1085 - 25s - loss: 0.1069 - accuracy: 0.9508 - val_loss: 0.1205 - val_accuracy: 0.9450 - 25s/epoch - 23ms/step\n",
      "Epoch 91/100\n",
      "1085/1085 - 24s - loss: 0.1059 - accuracy: 0.9521 - val_loss: 0.1223 - val_accuracy: 0.9453 - 24s/epoch - 22ms/step\n",
      "Epoch 92/100\n",
      "1085/1085 - 24s - loss: 0.1048 - accuracy: 0.9512 - val_loss: 0.1221 - val_accuracy: 0.9441 - 24s/epoch - 22ms/step\n",
      "Epoch 93/100\n",
      "1085/1085 - 24s - loss: 0.1051 - accuracy: 0.9526 - val_loss: 0.1228 - val_accuracy: 0.9463 - 24s/epoch - 22ms/step\n",
      "Epoch 94/100\n",
      "1085/1085 - 24s - loss: 0.1036 - accuracy: 0.9533 - val_loss: 0.1285 - val_accuracy: 0.9389 - 24s/epoch - 22ms/step\n",
      "Epoch 95/100\n",
      "1085/1085 - 24s - loss: 0.1159 - accuracy: 0.9494 - val_loss: 0.1300 - val_accuracy: 0.9353 - 24s/epoch - 22ms/step\n",
      "Epoch 96/100\n",
      "1085/1085 - 24s - loss: 0.1054 - accuracy: 0.9536 - val_loss: 0.1255 - val_accuracy: 0.9402 - 24s/epoch - 22ms/step\n",
      "Epoch 97/100\n",
      "1085/1085 - 24s - loss: 0.1058 - accuracy: 0.9537 - val_loss: 0.1223 - val_accuracy: 0.9468 - 24s/epoch - 22ms/step\n",
      "Epoch 98/100\n",
      "1085/1085 - 24s - loss: 0.1068 - accuracy: 0.9518 - val_loss: 0.1224 - val_accuracy: 0.9466 - 24s/epoch - 22ms/step\n",
      "Epoch 99/100\n",
      "1085/1085 - 24s - loss: 0.1112 - accuracy: 0.9510 - val_loss: 0.1189 - val_accuracy: 0.9452 - 24s/epoch - 22ms/step\n",
      "Epoch 100/100\n",
      "1085/1085 - 24s - loss: 0.1075 - accuracy: 0.9528 - val_loss: 0.1222 - val_accuracy: 0.9470 - 24s/epoch - 22ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_tree_model_together(semantic_len,syntatic_len,num_word, embedding_dim):\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(semantic_len, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "        (Convolution1D(input_shape=(syntatic_len, 1), filters=1,\n",
    "                       kernel_size=1, strides=1, padding='same',\n",
    "                       activation='relu', kernel_initializer=keras.initializers.Ones(),\n",
    "                       bias_initializer='zeros')),\n",
    "        (MaxPooling1D(pool_size=2, strides=2, padding='same', )),\n",
    "        (Flatten()),\n",
    "        (keras.layers.Embedding(num_word, embedding_dim)),\n",
    "        (keras.layers.GlobalMaxPool1D()),\n",
    "    ])\n",
    "    model_together = keras.layers.concatenate([model3.output, model2.output])\n",
    "    x = Dropout(0.4)(model_together)\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    final_output=Dense(1, activation='sigmoid')(x)\n",
    "    model_together = Model(inputs=[model3.input, model2.input], outputs=final_output)\n",
    "    return model_together\n",
    "\n",
    "model_old = get_tree_model_together(len(train_vec_sem[0]),len(train_vec_syn[0]),50000,16)\n",
    "\n",
    "model_old.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=losses.binary_crossentropy,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "history = model_old.fit([np.array(train_vec_syn), \n",
    "                                    np.array(train_vec_sem)],\n",
    "                 np.array(train_lable),\n",
    "                 validation_data=([np.array(val_vec_syn),\n",
    "                                   np.array(val_vec_sem)],\n",
    "                                  np.array(val_lable)),\n",
    "                 epochs=num_epoch, verbose=2)\n",
    "\n",
    "\n",
    "test_predict = model_old.predict([np.array(test_vec_syn),\n",
    "                                np.array(test_vec_sem)])\n",
    " \n",
    "predict_label=[i[0] for i in np.array(test_predict)]\n",
    "bin_pred = [1 if p > 0.5 else 0 for p in predict_label]\n",
    "test_true = test_lable\n",
    "correct = 0\n",
    "\n",
    "y_pred=[]\n",
    "y_true=[]\n",
    "for i in range(len(bin_pred)):\n",
    "    y_pred.append(bin_pred[i])\n",
    "    y_true.append(test_true[i])\n",
    "    if bin_pred[i] == test_true[i]:\n",
    "        correct += 1\n",
    "balance_accuracy, precision, recall, f1_value = plot_value(y_pred,y_true)\n",
    "\n",
    "save_performance('DeepLog1.0', balance_accuracy, precision, recall, f1_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644f912",
   "metadata": {},
   "source": [
    "# ASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'zeppelin-master' 1\n",
      "时间：2024-08-15 23:13:14 获取节点\n",
      "时间：2024-08-15 23:13:16 打乱数据\n",
      "时间：2024-08-15 23:13:16 按比例切分数据集\n",
      "时间：2024-08-15 23:13:16 获取特征\n",
      "时间：2024-08-15 23:14:40 tokenizer\n",
      "时间：2024-08-15 23:15:44 word2vec\n",
      "时间：2024-08-15 23:15:45  构建模型skip-gram\n",
      "时间：2024-08-15 23:16:48  构建模型skip-gram\n",
      "时间：2024-08-15 23:17:00  构建模型skip-gram\n",
      "时间：2024-08-15 23:18:24 组建模型\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 80, 128)           117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 80, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10240)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 10241     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 127,489\n",
      "Trainable params: 127,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "时间：2024-08-15 23:18:45 训练\n",
      "Epoch 1/100\n",
      "1446/1446 - 105s - loss: 0.2249 - accuracy: 0.9074 - val_loss: 0.1911 - val_accuracy: 0.9217 - lr: 0.0010 - 105s/epoch - 73ms/step\n",
      "Epoch 2/100\n",
      "1446/1446 - 96s - loss: 0.1825 - accuracy: 0.9253 - val_loss: 0.1876 - val_accuracy: 0.9258 - lr: 0.0010 - 96s/epoch - 67ms/step\n",
      "Epoch 3/100\n",
      "1446/1446 - 96s - loss: 0.1681 - accuracy: 0.9311 - val_loss: 0.1612 - val_accuracy: 0.9314 - lr: 0.0010 - 96s/epoch - 66ms/step\n",
      "Epoch 4/100\n",
      "1446/1446 - 96s - loss: 0.1543 - accuracy: 0.9346 - val_loss: 0.1544 - val_accuracy: 0.9338 - lr: 0.0010 - 96s/epoch - 66ms/step\n",
      "Epoch 5/100\n",
      "1446/1446 - 98s - loss: 0.1475 - accuracy: 0.9385 - val_loss: 0.1567 - val_accuracy: 0.9370 - lr: 0.0010 - 98s/epoch - 68ms/step\n",
      "Epoch 6/100\n",
      "1446/1446 - 96s - loss: 0.1393 - accuracy: 0.9417 - val_loss: 0.1577 - val_accuracy: 0.9379 - lr: 0.0010 - 96s/epoch - 66ms/step\n",
      "Epoch 7/100\n",
      "1446/1446 - 98s - loss: 0.1348 - accuracy: 0.9423 - val_loss: 0.1482 - val_accuracy: 0.9398 - lr: 0.0010 - 98s/epoch - 68ms/step\n",
      "Epoch 8/100\n",
      "1446/1446 - 96s - loss: 0.1320 - accuracy: 0.9443 - val_loss: 0.1496 - val_accuracy: 0.9342 - lr: 0.0010 - 96s/epoch - 66ms/step\n",
      "Epoch 9/100\n",
      "1446/1446 - 96s - loss: 0.1249 - accuracy: 0.9468 - val_loss: 0.1419 - val_accuracy: 0.9360 - lr: 0.0010 - 96s/epoch - 66ms/step\n",
      "Epoch 10/100\n",
      "1446/1446 - 96s - loss: 0.1223 - accuracy: 0.9485 - val_loss: 0.1389 - val_accuracy: 0.9410 - lr: 0.0010 - 96s/epoch - 67ms/step\n",
      "Epoch 11/100\n",
      "1446/1446 - 96s - loss: 0.1191 - accuracy: 0.9499 - val_loss: 0.1468 - val_accuracy: 0.9368 - lr: 0.0010 - 96s/epoch - 66ms/step\n",
      "Epoch 12/100\n",
      "1446/1446 - 95s - loss: 0.1135 - accuracy: 0.9519 - val_loss: 0.1406 - val_accuracy: 0.9416 - lr: 0.0010 - 95s/epoch - 66ms/step\n",
      "Epoch 13/100\n",
      "1446/1446 - 96s - loss: 0.1144 - accuracy: 0.9520 - val_loss: 0.1406 - val_accuracy: 0.9418 - lr: 0.0010 - 96s/epoch - 66ms/step\n",
      "Epoch 14/100\n",
      "1446/1446 - 98s - loss: 0.0982 - accuracy: 0.9576 - val_loss: 0.1290 - val_accuracy: 0.9467 - lr: 5.0000e-04 - 98s/epoch - 67ms/step\n",
      "Epoch 15/100\n",
      "1446/1446 - 109s - loss: 0.0955 - accuracy: 0.9586 - val_loss: 0.1320 - val_accuracy: 0.9463 - lr: 5.0000e-04 - 109s/epoch - 76ms/step\n",
      "Epoch 16/100\n",
      "1446/1446 - 97s - loss: 0.0932 - accuracy: 0.9595 - val_loss: 0.1312 - val_accuracy: 0.9466 - lr: 5.0000e-04 - 97s/epoch - 67ms/step\n",
      "Epoch 17/100\n",
      "1446/1446 - 97s - loss: 0.0914 - accuracy: 0.9596 - val_loss: 0.1295 - val_accuracy: 0.9474 - lr: 5.0000e-04 - 97s/epoch - 67ms/step\n",
      "Epoch 18/100\n",
      "1446/1446 - 96s - loss: 0.0830 - accuracy: 0.9635 - val_loss: 0.1313 - val_accuracy: 0.9483 - lr: 2.5000e-04 - 96s/epoch - 66ms/step\n",
      "Epoch 19/100\n",
      "1446/1446 - 95s - loss: 0.0808 - accuracy: 0.9630 - val_loss: 0.1347 - val_accuracy: 0.9496 - lr: 2.5000e-04 - 95s/epoch - 66ms/step\n",
      "Epoch 20/100\n",
      "1446/1446 - 95s - loss: 0.0799 - accuracy: 0.9645 - val_loss: 0.1433 - val_accuracy: 0.9479 - lr: 2.5000e-04 - 95s/epoch - 66ms/step\n",
      "Epoch 21/100\n",
      "1446/1446 - 96s - loss: 0.0762 - accuracy: 0.9652 - val_loss: 0.1348 - val_accuracy: 0.9481 - lr: 1.2500e-04 - 96s/epoch - 66ms/step\n",
      "Epoch 22/100\n",
      "1446/1446 - 95s - loss: 0.0743 - accuracy: 0.9657 - val_loss: 0.1355 - val_accuracy: 0.9466 - lr: 1.2500e-04 - 95s/epoch - 66ms/step\n",
      "Epoch 23/100\n",
      "1446/1446 - 95s - loss: 0.0741 - accuracy: 0.9665 - val_loss: 0.1343 - val_accuracy: 0.9474 - lr: 1.2500e-04 - 95s/epoch - 66ms/step\n",
      "Epoch 24/100\n",
      "1446/1446 - 95s - loss: 0.0713 - accuracy: 0.9669 - val_loss: 0.1400 - val_accuracy: 0.9474 - lr: 6.2500e-05 - 95s/epoch - 66ms/step\n",
      "Epoch 25/100\n",
      "1446/1446 - 96s - loss: 0.0707 - accuracy: 0.9670 - val_loss: 0.1408 - val_accuracy: 0.9478 - lr: 6.2500e-05 - 96s/epoch - 66ms/step\n",
      "Epoch 26/100\n",
      "1446/1446 - 95s - loss: 0.0704 - accuracy: 0.9671 - val_loss: 0.1390 - val_accuracy: 0.9468 - lr: 6.2500e-05 - 95s/epoch - 66ms/step\n",
      "Epoch 27/100\n",
      "1446/1446 - 96s - loss: 0.0690 - accuracy: 0.9677 - val_loss: 0.1443 - val_accuracy: 0.9473 - lr: 3.1250e-05 - 96s/epoch - 66ms/step\n",
      "Epoch 28/100\n",
      "1446/1446 - 95s - loss: 0.0685 - accuracy: 0.9683 - val_loss: 0.1488 - val_accuracy: 0.9475 - lr: 3.1250e-05 - 95s/epoch - 66ms/step\n",
      "Epoch 29/100\n",
      "1446/1446 - 95s - loss: 0.0683 - accuracy: 0.9680 - val_loss: 0.1518 - val_accuracy: 0.9475 - lr: 3.1250e-05 - 95s/epoch - 66ms/step\n",
      "Epoch 30/100\n",
      "1446/1446 - 95s - loss: 0.0674 - accuracy: 0.9685 - val_loss: 0.1518 - val_accuracy: 0.9471 - lr: 1.5625e-05 - 95s/epoch - 66ms/step\n",
      "Epoch 31/100\n",
      "1446/1446 - 96s - loss: 0.0672 - accuracy: 0.9684 - val_loss: 0.1544 - val_accuracy: 0.9475 - lr: 1.5625e-05 - 96s/epoch - 66ms/step\n",
      "Epoch 32/100\n",
      "1446/1446 - 95s - loss: 0.0673 - accuracy: 0.9682 - val_loss: 0.1542 - val_accuracy: 0.9472 - lr: 1.5625e-05 - 95s/epoch - 66ms/step\n",
      "Epoch 33/100\n",
      "1446/1446 - 95s - loss: 0.0667 - accuracy: 0.9689 - val_loss: 0.1538 - val_accuracy: 0.9470 - lr: 7.8125e-06 - 95s/epoch - 66ms/step\n",
      "Epoch 34/100\n",
      "1446/1446 - 102s - loss: 0.0666 - accuracy: 0.9689 - val_loss: 0.1543 - val_accuracy: 0.9472 - lr: 7.8125e-06 - 102s/epoch - 70ms/step\n",
      "Epoch 35/100\n",
      "1446/1446 - 101s - loss: 0.0665 - accuracy: 0.9688 - val_loss: 0.1549 - val_accuracy: 0.9472 - lr: 7.8125e-06 - 101s/epoch - 70ms/step\n",
      "Epoch 36/100\n",
      "1446/1446 - 100s - loss: 0.0662 - accuracy: 0.9690 - val_loss: 0.1551 - val_accuracy: 0.9471 - lr: 3.9063e-06 - 100s/epoch - 69ms/step\n",
      "Epoch 37/100\n",
      "1446/1446 - 99s - loss: 0.0662 - accuracy: 0.9691 - val_loss: 0.1554 - val_accuracy: 0.9470 - lr: 3.9063e-06 - 99s/epoch - 69ms/step\n",
      "Epoch 38/100\n",
      "1446/1446 - 99s - loss: 0.0663 - accuracy: 0.9687 - val_loss: 0.1558 - val_accuracy: 0.9470 - lr: 3.9063e-06 - 99s/epoch - 69ms/step\n",
      "Epoch 39/100\n",
      "1446/1446 - 99s - loss: 0.0661 - accuracy: 0.9693 - val_loss: 0.1560 - val_accuracy: 0.9470 - lr: 1.9531e-06 - 99s/epoch - 68ms/step\n",
      "Epoch 40/100\n",
      "1446/1446 - 100s - loss: 0.0660 - accuracy: 0.9688 - val_loss: 0.1561 - val_accuracy: 0.9470 - lr: 1.9531e-06 - 100s/epoch - 69ms/step\n",
      "Epoch 41/100\n",
      "1446/1446 - 99s - loss: 0.0660 - accuracy: 0.9688 - val_loss: 0.1562 - val_accuracy: 0.9470 - lr: 1.9531e-06 - 99s/epoch - 68ms/step\n",
      "Epoch 42/100\n",
      "1446/1446 - 99s - loss: 0.0660 - accuracy: 0.9688 - val_loss: 0.1563 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 99s/epoch - 69ms/step\n",
      "Epoch 43/100\n",
      "1446/1446 - 100s - loss: 0.0659 - accuracy: 0.9692 - val_loss: 0.1564 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 100s/epoch - 69ms/step\n",
      "Epoch 44/100\n",
      "1446/1446 - 99s - loss: 0.0658 - accuracy: 0.9689 - val_loss: 0.1565 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 99s/epoch - 68ms/step\n",
      "Epoch 45/100\n",
      "1446/1446 - 99s - loss: 0.0659 - accuracy: 0.9695 - val_loss: 0.1565 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 99s/epoch - 69ms/step\n",
      "Epoch 46/100\n",
      "1446/1446 - 99s - loss: 0.0658 - accuracy: 0.9691 - val_loss: 0.1566 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 99s/epoch - 68ms/step\n",
      "Epoch 47/100\n",
      "1446/1446 - 99s - loss: 0.0660 - accuracy: 0.9693 - val_loss: 0.1567 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 99s/epoch - 68ms/step\n",
      "Epoch 48/100\n",
      "1446/1446 - 99s - loss: 0.0658 - accuracy: 0.9690 - val_loss: 0.1568 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 99s/epoch - 68ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "1446/1446 - 100s - loss: 0.0657 - accuracy: 0.9689 - val_loss: 0.1568 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 100s/epoch - 69ms/step\n",
      "Epoch 50/100\n",
      "1446/1446 - 100s - loss: 0.0661 - accuracy: 0.9689 - val_loss: 0.1569 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 100s/epoch - 69ms/step\n",
      "Epoch 51/100\n",
      "1446/1446 - 107s - loss: 0.0659 - accuracy: 0.9692 - val_loss: 0.1570 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 107s/epoch - 74ms/step\n",
      "Epoch 52/100\n",
      "1446/1446 - 96s - loss: 0.0657 - accuracy: 0.9690 - val_loss: 0.1570 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 96s/epoch - 67ms/step\n",
      "Epoch 53/100\n",
      "1446/1446 - 96s - loss: 0.0659 - accuracy: 0.9692 - val_loss: 0.1571 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 96s/epoch - 66ms/step\n",
      "Epoch 54/100\n",
      "1446/1446 - 96s - loss: 0.0658 - accuracy: 0.9690 - val_loss: 0.1571 - val_accuracy: 0.9469 - lr: 1.0000e-06 - 96s/epoch - 67ms/step\n",
      "Epoch 55/100\n",
      "1446/1446 - 96s - loss: 0.0659 - accuracy: 0.9691 - val_loss: 0.1571 - val_accuracy: 0.9467 - lr: 1.0000e-06 - 96s/epoch - 67ms/step\n",
      "Epoch 56/100\n",
      "1446/1446 - 96s - loss: 0.0658 - accuracy: 0.9689 - val_loss: 0.1572 - val_accuracy: 0.9467 - lr: 1.0000e-06 - 96s/epoch - 66ms/step\n",
      "Epoch 57/100\n"
     ]
    }
   ],
   "source": [
    "#ase 项目集合 项目内\n",
    "# 作者：常晓松\n",
    "# 作用：日志等级预测\n",
    "# 时间： 2022/3/23 14:35\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "import pymysql\n",
    "from keras.layers import LSTM, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from keras import Model, Input\n",
    "import random\n",
    "import sklearn as sk\n",
    "\n",
    "def stratified_shuffle_split(data_vec, data_lable, test_size, seed):\n",
    "    train_data = []\n",
    "    train_lable = []\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    test_data = []\n",
    "    test_lable = []\n",
    "    for train_index, test_index in split.split(data_vec, data_lable):\n",
    "        for i in train_index:\n",
    "            train_data.append(data_vec[i])\n",
    "            train_lable.append(data_lable[i])\n",
    "        for i in test_index:\n",
    "            test_data.append(data_vec[i])\n",
    "            test_lable.append(data_lable[i])\n",
    "    return train_data, train_lable, test_data, test_lable\n",
    "\n",
    "\n",
    "def shuffle_data(leaf, lable, seed):\n",
    "    import random\n",
    "    c = list(zip(leaf, lable))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(c)  # 打乱c\n",
    "    leaf[:], lable[:] = zip(*c)  # 将打乱的c解开\n",
    "    return leaf, lable\n",
    "\n",
    "\n",
    "def get_positive_data_from_db(vec_length):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select vectorfused,lable_ASE from data_model_2 where lable_ASE<>0 and vectorfused is not null;\"\n",
    "    result = cur.execute(sqli)\n",
    "    methods_padding = np.empty(shape=[0, vec_length], dtype=int)\n",
    "    labels = []\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        methods_padding = string_vector_to_int(methods_padding, oneRow)\n",
    "        labels.append(oneRow[1])\n",
    "    return methods_padding, labels\n",
    "\n",
    "\n",
    "def splite_data(methods_padding, labels):\n",
    "    group_size_one = int(len(methods_padding) / 10)\n",
    "    methods_train = methods_padding[0:group_size_one * 6]\n",
    "    labels_train = labels[0:group_size_one * 6]\n",
    "    methods_test = methods_padding[group_size_one * 6:group_size_one * 8]\n",
    "    labels_test = labels[group_size_one * 6:group_size_one * 8]\n",
    "    methods_verify = methods_padding[group_size_one * 8:]\n",
    "    labels_verify = labels[group_size_one * 8:]\n",
    "    return methods_train, labels_train, methods_test, labels_test, methods_verify, labels_verify\n",
    "\n",
    "\n",
    "def add_nagitive_data(methods_padding, lables):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select vectorfused,lable_ASE from data_model_2 where lable_ASE=0 and vectorfused is not null limit \" + str(\n",
    "        len(methods_padding)) + \";\"\n",
    "    result = cur.execute(sqli)\n",
    "    for i in range(1, len(methods_padding)):\n",
    "        oneRow = cur.fetchone()\n",
    "        methods_padding = string_vector_to_int(methods_padding, oneRow)\n",
    "        lables.append(oneRow[1])\n",
    "    return methods_padding, lables\n",
    "\n",
    "\n",
    "def string_vector_to_int(methods_padding, oneRow):\n",
    "    methods_padding = np.append(methods_padding, [list(map(int, oneRow[0][1:len(oneRow[0]) - 1].split(\",\")))], axis=0)\n",
    "    return methods_padding\n",
    "\n",
    "\n",
    "def float_revert_int(value_list):\n",
    "    revert_value_list = []\n",
    "    for i in value_list:\n",
    "        revert_value_list.append(int(np.round(i)))\n",
    "    value_list = revert_value_list\n",
    "    return revert_value_list\n",
    "\n",
    "\n",
    "def draw_confusion_matrix(y_true, y_pred, dic_lables):\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.cla()\n",
    "\n",
    "    labels = []\n",
    "    for key in dic_lables:\n",
    "        labels.append(key)\n",
    "    print_message(type(labels))\n",
    "    # 小数转整数\n",
    "    y_pred_int = float_revert_int(y_pred)\n",
    "    sns.set()\n",
    "    f, ax = plt.subplots()\n",
    "    C2 = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    print_message(C2)  # 打印出来看看\n",
    "    sns.heatmap(C2, annot=True, fmt='.20g', ax=ax, cmap=\"YlGnBu\")  # 画热力图\n",
    "    ax.set_title('confusion matrix')  # 标题\n",
    "    ax.set_xlabel('true')  # x轴\n",
    "    ax.set_ylabel('predict')  # y轴\n",
    "    save_pic(plt, '混淆矩阵')\n",
    "    # plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def save_pic(plt, file_name):\n",
    "    # 创建目录\n",
    "    import os, time\n",
    "    dirs = 'C:\\\\Users\\\\chang\\\\Desktop\\\\日志工作空间\\\\实验图片\\\\'\n",
    "    t = time.strftime('%Y-%m-%d-%H', time.localtime(int(time.time())))\n",
    "    dirs = dirs + t\n",
    "    file = dirs + '\\\\' + file_name + '.png'\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    # 保存图片\n",
    "    plt.savefig(file)  # 保存图片\n",
    "\n",
    "\n",
    "def plot_value(y_true, y_pred):\n",
    "    \n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    balance_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision = sk.metrics.precision_score(y_true, y_pred)\n",
    "    recall = sk.metrics.recall_score(y_true, y_pred)\n",
    "    f1_value = sk.metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "    return balance_accuracy, precision, recall, f1_value\n",
    "\n",
    "\n",
    "def autolabel(rects, ax):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "def plot_graphs(history_loc, type_loc):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.cla()\n",
    "\n",
    "    plt.plot(history_loc.history[type_loc])\n",
    "    plt.plot(history_loc.history['val_' + type_loc])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(type_loc)\n",
    "    plt.legend([type_loc, 'val_' + type_loc])\n",
    "    save_pic(plt, type_loc)\n",
    "    # plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def str_revert_to_list(methods_padding, max_len):\n",
    "    methods_padding_list = np.empty(shape=[0, max_len], dtype=int)\n",
    "\n",
    "    for methods_padding_one in methods_padding:\n",
    "        list1 = methods_padding_one.split(',')\n",
    "        list1 = list(map(int, list1))\n",
    "        methods_padding_list = np.append(methods_padding_list, [np.array(list1)], axis=0)  # 添加整行元素，axis=1添加整列元素\n",
    "    return methods_padding_list\n",
    "\n",
    "\n",
    "def get_object(save_absolute_path):\n",
    "    summer_load = None\n",
    "    with open(save_absolute_path, 'rb') as f:\n",
    "        summer_load = pickle.load(f)  # read file and build object\n",
    "    return summer_load\n",
    "\n",
    "\n",
    "def save_object(summer_save, save_absolute_path):\n",
    "    summer_save = pickle.dumps(summer_save)\n",
    "    with open(save_absolute_path, 'wb') as f:  # open file with write-mode\n",
    "        f.write(summer_save)  # serialize and save object\n",
    "\n",
    "\n",
    "def get_parent(vec_chair):\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select parentId from data_model_2 where seq=\" + str(vec_chair[len(vec_chair) - 1]) + \";\"\n",
    "    try:\n",
    "        result = cur.execute(sqli)\n",
    "    except:\n",
    "        print(sqli)\n",
    "    oneRow = cur.fetchone()\n",
    "    if oneRow[0] == 0:\n",
    "        return vec_chair\n",
    "    else:\n",
    "        vec_chair.append(oneRow[0])\n",
    "        return get_parent(vec_chair)\n",
    "\n",
    "\n",
    "def get_chair_vec_int(vec_chair, i):\n",
    "    vectorfused = None\n",
    "    one_leaf_list = []\n",
    "    vectorSemantic = None\n",
    "    leaf = True\n",
    "    message_type = i\n",
    "    cur = conn.cursor()\n",
    "    for one_leaf in vec_chair:\n",
    "        sqli = \"select REPLACE(REPLACE(a.vectorSemanticRawVec,'[',''),']',''),\" \\\n",
    "               \"REPLACE(REPLACE(fusedMessageRawVec,'[',''),']',''),\" \\\n",
    "               \"lable_ASE ,REPLACE(REPLACE(syntacticMessageRawVec,'[',''),']','') \" \\\n",
    "               \"from data_model_2 a  where seq=\" + str(one_leaf) + \";\"\n",
    "        result = cur.execute(sqli)\n",
    "        oneRow = cur.fetchone()\n",
    "        vectorSemantic = oneRow[0]\n",
    "        vectorfused = oneRow[1]\n",
    "        lable_ASE = oneRow[2]\n",
    "        syntacticMessage = oneRow[3]\n",
    "        if vectorSemantic != \"[]\":\n",
    "            if message_type == 1:\n",
    "                if vectorfused is not None:\n",
    "                    list_fused = vectorfused.split(',')\n",
    "                    list_fused = list(map(float, list_fused))\n",
    "                    one_leaf_list.extend(list_fused)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "            if message_type == 2:\n",
    "                if vectorSemantic is not None and len(vectorSemantic) > 0:\n",
    "                    list_semantic = vectorSemantic.split(',')\n",
    "                    list_semantic = list(map(float, list_semantic))\n",
    "                    one_leaf_list.extend(list_semantic)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "            if message_type == 3:\n",
    "                if syntacticMessage is not None and len(syntacticMessage) > 0:\n",
    "                    list_syntactic = syntacticMessage.split(',')\n",
    "                    list_syntactic = list(map(float, list_syntactic))\n",
    "                    one_leaf_list.extend(list_syntactic)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "    cur.close()\n",
    "    return one_leaf_list\n",
    "\n",
    "\n",
    "def get_chair_vec_raw(vec_chair, i):\n",
    "    vectorfused = None\n",
    "    one_leaf_list = []\n",
    "    vectorSemantic = None\n",
    "    leaf = True\n",
    "    message_type = i\n",
    "    cur = conn.cursor()\n",
    "    for one_leaf in vec_chair:\n",
    "        sqli = \"select REPLACE(REPLACE(vectorSemanticRaw,'[',''),']',''),\" \\\n",
    "               \"REPLACE(REPLACE(fusedMessageRaw,'[',''),']',''),\" \\\n",
    "               \"lable_ASE ,REPLACE(REPLACE(syntacticMessageRaw,'[',''),']','') \" \\\n",
    "               \"from data_model_2 a  where seq=\" + str(one_leaf) + \";\"\n",
    "        result = cur.execute(sqli)\n",
    "        oneRow = cur.fetchone()\n",
    "        vectorSemantic = oneRow[0]\n",
    "        vectorfused = oneRow[1]\n",
    "        syntacticMessage = oneRow[3]\n",
    "        lable_ASE = oneRow[2]\n",
    "        if vectorSemantic != \"[]\":\n",
    "            if message_type == 1:\n",
    "                if vectorfused is not None:\n",
    "                    list_fused = vectorfused.split(',')\n",
    "                    list_fused = list(map(str, list_fused))\n",
    "                    one_leaf_list.extend(list_fused)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "            if message_type == 2:\n",
    "                if vectorSemantic is not None and len(vectorSemantic) > 0:\n",
    "                    list_semantic = vectorSemantic.split(',')\n",
    "                    list_semantic = list(map(str, list_semantic))\n",
    "                    one_leaf_list.extend(list_semantic)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "            if message_type == 3:\n",
    "                if syntacticMessage is not None and len(syntacticMessage) > 0:\n",
    "                    list_syntactic = syntacticMessage.split(',')\n",
    "                    list_syntactic = list(map(str, list_syntactic))\n",
    "                    one_leaf_list.extend(list_syntactic)\n",
    "                    # one_leaf_list.extend([0,0,0,0])\n",
    "    cur.close()\n",
    "    return one_leaf_list\n",
    "\n",
    "\n",
    "def print_message(message):\n",
    "    t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(int(time.time())))\n",
    "    print('时间：' + t, end=\" \", flush=True)\n",
    "    print(message, flush=True)\n",
    "\n",
    "\n",
    "def repetition_list(a):\n",
    "    repetition = 6\n",
    "    c = []\n",
    "    for a_one in a:\n",
    "        b = []\n",
    "        for a_single in a_one:\n",
    "            for i in range(repetition):\n",
    "                b.append(a_single)\n",
    "        c.append(b)\n",
    "    return c\n",
    "\n",
    "\n",
    "def overlap_channel(a, b):\n",
    "    element_num = 0\n",
    "    height = 0\n",
    "    width = 0\n",
    "    if len(a.shape) == 2:\n",
    "        element_num = a.shape[0]\n",
    "        height = 1\n",
    "        width = a.shape[1]\n",
    "    elif len(a.shape) == 3:\n",
    "        element_num = a.shape[0]\n",
    "        height = a.shape[1]\n",
    "        width = a.shape[2]\n",
    "    else:\n",
    "        raise Exception(print_message('通道叠加 不支持这类维度:' + str(a.shape)))\n",
    "\n",
    "    a = a.reshape(element_num, 1, height, width)\n",
    "    b = b.reshape(element_num, 1, height, width)\n",
    "    a = a.transpose(1, 0, 2, 3)\n",
    "    b = b.transpose(1, 0, 2, 3)\n",
    "    c = np.vstack((a, b))\n",
    "    c = c.transpose(1, 2, 3, 0)\n",
    "    return c\n",
    "\n",
    "\n",
    "def check_list_exist(A, B):\n",
    "    exist = False\n",
    "    for row in A:\n",
    "        if (row == B).all():\n",
    "            exist = True\n",
    "    return exist\n",
    "\n",
    "\n",
    "def get_convert_data(A, B):\n",
    "    C = []\n",
    "    for A_one in A:\n",
    "        for B_one in B:\n",
    "            if (np.array(A_one) == np.array(B_one)).all():\n",
    "                C.append(A_one)\n",
    "    return C\n",
    "\n",
    "\n",
    "def cosDist(A, B):\n",
    "    from scipy.spatial.distance import pdist\n",
    "    return pdist(np.vstack([A, B]), 'cosine')  # np.sqrt(sum(np.power((A - B), 2)))\n",
    "\n",
    "\n",
    "def get_normal_data(data_point, data_lable):\n",
    "    neigh_num_a = 5\n",
    "    diff_rate_b = 1\n",
    "    can_stop = 0.99\n",
    "\n",
    "    # print_message(len(data_point))\n",
    "\n",
    "    abnormal_old = []\n",
    "    abnormal_new_ = []\n",
    "    for i in range(0, 100):\n",
    "        for one_point, one_lable in zip(data_point, data_lable):\n",
    "            neighbour_point = []\n",
    "            neighbour_lable = []\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(data_point, data_lable):\n",
    "                if check_list_exist(abnormal_new_, one_point_neighbour) or (one_point_neighbour == one_point).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    this_point_distance = cosDist(one_point_neighbour, one_point)\n",
    "                    if len(neighbour_point) >= neigh_num_a \\\n",
    "                            and this_point_distance < neighbour_point[neigh_num_a - 1] \\\n",
    "                            or len(neighbour_point) < neigh_num_a:\n",
    "                        if len(neighbour_point) < neigh_num_a:\n",
    "                            neighbour_point.append(this_point_distance)\n",
    "                            neighbour_lable.append(one_lable_neighbour)\n",
    "                        else:\n",
    "                            neighbour_point[neigh_num_a - 1] = (this_point_distance)\n",
    "                            neighbour_lable[neigh_num_a - 1] = (one_lable_neighbour)\n",
    "                        # 按照距离排序\n",
    "                        zipped = zip(neighbour_point, neighbour_lable)\n",
    "                        sort_zipped = sorted(zipped)\n",
    "                        result = zip(*sort_zipped)\n",
    "                        neighbour_point, neighbour_lable = [list(x) for x in result]\n",
    "\n",
    "            same_num = 0\n",
    "            diff_num = 0\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(neighbour_point[0:neigh_num_a],\n",
    "                                                                neighbour_lable[0:neigh_num_a]):\n",
    "                if one_lable_neighbour != one_lable:\n",
    "                    diff_num += 1\n",
    "                else:\n",
    "                    same_num += 1\n",
    "            diff_rate = diff_num / (same_num + diff_num)\n",
    "            if diff_rate >= diff_rate_b:\n",
    "                abnormal_new_.append([one_point])\n",
    "        if max(len(abnormal_new_), len(abnormal_old)) != 0 and \\\n",
    "                len(get_convert_data(abnormal_new_, abnormal_old)) / max(len(abnormal_new_),\n",
    "                                                                         len(abnormal_old)) >= can_stop:\n",
    "            break\n",
    "        else:\n",
    "            abnormal_old = abnormal_new_[:]\n",
    "        # print_message('单次noise数据量：' + str(len(abnormal_old)))\n",
    "\n",
    "    data_point_clear = []\n",
    "    data_lable_clear = []\n",
    "\n",
    "    for one_point, one_lable in zip(data_point, data_lable):\n",
    "        if (not check_list_exist(abnormal_old, one_point)):\n",
    "            data_point_clear.append(one_point)\n",
    "            data_lable_clear.append(one_lable)\n",
    "    return data_point_clear, data_lable_clear, abnormal_old\n",
    "\n",
    "\n",
    "##仅支持0 - 1 分类\n",
    "def change_abnormal(data_point, data_lable):\n",
    "    neigh_num_a = 10\n",
    "    diff_rate_b = 1\n",
    "    can_stop = 0.99\n",
    "\n",
    "    # print_message(len(data_point))\n",
    "\n",
    "    abnormal_old = []\n",
    "    abnormal_new_ = []\n",
    "    for i in range(0, 100):\n",
    "        for one_point, one_lable in zip(data_point, data_lable):\n",
    "            neighbour_point = []\n",
    "            neighbour_lable = []\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(data_point, data_lable):\n",
    "                if check_list_exist(abnormal_new_, one_point_neighbour) or (one_point_neighbour == one_point).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    this_point_distance = cosDist(one_point_neighbour, one_point)\n",
    "                    if len(neighbour_point) >= neigh_num_a \\\n",
    "                            and this_point_distance < neighbour_point[neigh_num_a - 1] \\\n",
    "                            or len(neighbour_point) < neigh_num_a:\n",
    "                        if len(neighbour_point) < neigh_num_a:\n",
    "                            neighbour_point.append(this_point_distance)\n",
    "                            neighbour_lable.append(one_lable_neighbour)\n",
    "                        else:\n",
    "                            neighbour_point[neigh_num_a - 1] = (this_point_distance)\n",
    "                            neighbour_lable[neigh_num_a - 1] = (one_lable_neighbour)\n",
    "                        # 按照距离排序\n",
    "                        zipped = zip(neighbour_point, neighbour_lable)\n",
    "                        sort_zipped = sorted(zipped)\n",
    "                        result = zip(*sort_zipped)\n",
    "                        neighbour_point, neighbour_lable = [list(x) for x in result]\n",
    "\n",
    "            same_num = 0\n",
    "            diff_num = 0\n",
    "            for one_point_neighbour, one_lable_neighbour in zip(neighbour_point[0:neigh_num_a],\n",
    "                                                                neighbour_lable[0:neigh_num_a]):\n",
    "                if one_lable_neighbour != one_lable:\n",
    "                    diff_num += 1\n",
    "                else:\n",
    "                    same_num += 1\n",
    "            diff_rate = diff_num / (same_num + diff_num)\n",
    "            if diff_rate >= diff_rate_b:\n",
    "                abnormal_new_.append([one_point])\n",
    "        if max(len(abnormal_new_), len(abnormal_old)) != 0 and \\\n",
    "                len(get_convert_data(abnormal_new_, abnormal_old)) / max(len(abnormal_new_),\n",
    "                                                                         len(abnormal_old)) >= can_stop:\n",
    "            break\n",
    "        else:\n",
    "            abnormal_old = abnormal_new_[:]\n",
    "        # print_message('单次noise数据量：' + str(len(abnormal_old)))\n",
    "\n",
    "    data_point_clear = []\n",
    "    data_lable_clear = []\n",
    "\n",
    "    for one_point, one_lable in zip(data_point, data_lable):\n",
    "        if (not check_list_exist(abnormal_old, one_point)):\n",
    "            data_point_clear.append(one_point)\n",
    "            data_lable_clear.append(one_lable)\n",
    "        else:\n",
    "            ##修改noise标签\n",
    "            data_point_clear.append(one_point)\n",
    "            if one_lable == 0:\n",
    "                data_lable_clear.append(1)\n",
    "            else:\n",
    "                data_lable_clear.append(0)\n",
    "    return data_point_clear, data_lable_clear, abnormal_old\n",
    "\n",
    "\n",
    "def clear_data(train_vec_tmp, train_labels, zero_index):\n",
    "    group_num = 300\n",
    "    train_vec_tmp_clear = []\n",
    "    train_labels_clear = []\n",
    "    abnormal_num = 0\n",
    "    for b in range(int(len(train_vec_tmp) / group_num) + 1):\n",
    "        begin = b * group_num + zero_index\n",
    "        end = (b + 1) * group_num\n",
    "        if end > len(train_vec_tmp):\n",
    "            end = len(train_vec_tmp)\n",
    "        train_vec_group_one, train_labels_group_one, abnormal_old = change_abnormal(train_vec_tmp[begin:end],\n",
    "                                                                                    train_labels[begin:end])\n",
    "        train_vec_tmp_clear.extend(train_vec_group_one)\n",
    "        train_labels_clear.extend(train_labels_group_one)\n",
    "        abnormal_num = abnormal_num + len(abnormal_old)\n",
    "    print_message('noise数据量：' + str(abnormal_num))\n",
    "    print_message('训练总数据量：' + str(len(train_vec_tmp)))\n",
    "    train_vec_tmp = train_vec_tmp_clear\n",
    "    train_labels = train_labels_clear\n",
    "    return train_vec_tmp, train_labels\n",
    "\n",
    "\n",
    "def play_title_message():\n",
    "    from playsound import playsound\n",
    "    playsound('D:/python/log_predict/file/结束提示.mp3')\n",
    "\n",
    "\n",
    "def get_vector(leaf, lable):\n",
    "    fused_leaf_vec = []\n",
    "    semantic_leaf_vec = []\n",
    "    syntatic_leaf_vec = []\n",
    "\n",
    "    for one_leaf in leaf:\n",
    "        vec_chain = []\n",
    "        # 叶子节点放入 list 中\n",
    "        vec_chain.append(one_leaf)\n",
    "        for i in range(1, 4):\n",
    "            # 根据list中的记录查询节点记录，组合向量\n",
    "            one_leaf_vec = get_chair_vec_raw(vec_chain, i)\n",
    "            if i == 1:\n",
    "                fused_leaf_vec.append(one_leaf_vec)\n",
    "            elif i == 2:\n",
    "                semantic_leaf_vec.append(one_leaf_vec)\n",
    "            else:\n",
    "                syntatic_leaf_vec.append(one_leaf_vec)\n",
    "\n",
    "    # 独热编码\n",
    "    #     lable = tf.keras.utils.to_categorical(lable, num_classes=2)\n",
    "    return fused_leaf_vec, semantic_leaf_vec, syntatic_leaf_vec, leaf, lable\n",
    "\n",
    "\n",
    "def str_to_vec(semantic_tokenizer, semantic_train_vec, semantic_max_len):\n",
    "    num_word = 50000  # 词典的词数\n",
    "    voo_token = 'NONE'  # 指代缺失词\n",
    "\n",
    "    trunc_type = 'post'\n",
    "    semantic_method_sequence = semantic_tokenizer.texts_to_sequences(semantic_train_vec)\n",
    "    semantic_method_padded = pad_sequences(semantic_method_sequence, maxlen=semantic_max_len,\n",
    "                                           padding=trunc_type, truncating=trunc_type)\n",
    "    return semantic_method_padded\n",
    "\n",
    "\n",
    "def tokenizer_vec_use_tokenizer(tokenizer, train_vec_raw, verify_vec_raw, test_vec_raw):\n",
    "    num_word = 500000\n",
    "    voo_token = 'NONE'  # 指代缺失词\n",
    "    train_vec_raw_convert = []\n",
    "    verify_vec_raw_convert = []\n",
    "    test_vec_raw_convert = []\n",
    "    for i in train_vec_raw:\n",
    "        train_vec_raw_convert.append(' '.join(i))\n",
    "\n",
    "    for i in verify_vec_raw:\n",
    "        verify_vec_raw_convert.append(' '.join(i))\n",
    "\n",
    "    for i in test_vec_raw:\n",
    "        test_vec_raw_convert.append(' '.join(i))\n",
    "    train_vec_raw = tokenizer.texts_to_sequences(train_vec_raw_convert)\n",
    "    verify_vec_raw = tokenizer.texts_to_sequences(verify_vec_raw_convert)\n",
    "    test_vec_raw = tokenizer.texts_to_sequences(test_vec_raw_convert)\n",
    "    return train_vec_raw, verify_vec_raw, test_vec_raw\n",
    "\n",
    "\n",
    "def pad_vec(train_vec, verify_vec, test_vec, vec_length):\n",
    "    trunc_type = 'post'\n",
    "    train_vec = pad_sequences(train_vec,\n",
    "                              maxlen=vec_length,\n",
    "                              padding=trunc_type,\n",
    "                              truncating=trunc_type)\n",
    "    verify_vec = pad_sequences(verify_vec,\n",
    "                               maxlen=vec_length,\n",
    "                               padding=trunc_type,\n",
    "                               truncating=trunc_type)\n",
    "    test_vec = pad_sequences(test_vec,\n",
    "                             maxlen=vec_length,\n",
    "                             padding=trunc_type,\n",
    "                             truncating=trunc_type)\n",
    "    return train_vec, verify_vec, test_vec\n",
    "\n",
    "\n",
    "def tokenizer_vec(vec_length, train_vec_raw, verify_vec_raw, test_vec_raw):\n",
    "    num_word = 500000\n",
    "    voo_token = 'NONE'  # 指代缺失词\n",
    "    trunc_type = 'post'\n",
    "    feature_all = []\n",
    "    for i in train_vec_raw:\n",
    "        feature_all.append(' '.join(i))\n",
    "    for i in verify_vec_raw:\n",
    "        feature_all.append(' '.join(i))\n",
    "    for i in test_vec_raw:\n",
    "        feature_all.append(' '.join(i))\n",
    "    tokenizer = \\\n",
    "        Tokenizer(num_words=num_word, oov_token=voo_token)\n",
    "    tokenizer.fit_on_texts(feature_all)\n",
    "    train_vec_raw = tokenizer.texts_to_sequences(train_vec_raw)\n",
    "    train_vec_raw = pad_sequences(train_vec_raw,\n",
    "                                  maxlen=vec_length,\n",
    "                                  padding=trunc_type,\n",
    "                                  truncating=trunc_type)\n",
    "    verify_vec_raw = tokenizer.texts_to_sequences(verify_vec_raw)\n",
    "    verify_vec_raw = pad_sequences(verify_vec_raw,\n",
    "                                   maxlen=vec_length,\n",
    "                                   padding=trunc_type,\n",
    "                                   truncating=trunc_type)\n",
    "    test_vec_raw = tokenizer.texts_to_sequences(test_vec_raw)\n",
    "    test_vec_raw = pad_sequences(test_vec_raw,\n",
    "                                 maxlen=vec_length,\n",
    "                                 padding=trunc_type,\n",
    "                                 truncating=trunc_type)\n",
    "    return train_vec_raw, verify_vec_raw, test_vec_raw\n",
    "\n",
    "\n",
    "def get_tokenizer():\n",
    "    cur = conn.cursor()\n",
    "    sql = \"select replace (vectorSemanticRaw,',',' '),logNum,seq ,replace (syntacticMessageRaw,',',' ') ,replace (fusedMessageRaw,',',' ') \" \\\n",
    "          \"from data_model_2 where \" \\\n",
    "          \"   fusedMessageRaw is not null ;\"\n",
    "    result = cur.execute(sql)\n",
    "    semantic_methods = []\n",
    "    syntatic_methods = []\n",
    "    index = []\n",
    "\n",
    "    fused_methods = []\n",
    "\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        semantic_methods.append(oneRow[0])\n",
    "        syntatic_methods.append(oneRow[3])\n",
    "        fused_methods.append(oneRow[4])\n",
    "    cur.close()\n",
    "    # tokenize\n",
    "    num_word = 50000  # 词典的词数\n",
    "    voo_token = 'NONE'  # 指代缺失词\n",
    "    trunc_type = 'post'\n",
    "    semantic_tokenizer = Tokenizer(num_words=num_word, oov_token=voo_token)\n",
    "    semantic_tokenizer.fit_on_texts(semantic_methods)\n",
    "\n",
    "    # 处理语法信息\n",
    "    syntatic_tokenizer = Tokenizer(num_words=num_word, oov_token=voo_token)\n",
    "    syntatic_tokenizer.fit_on_texts(syntatic_methods)\n",
    "    # 处理混合信息\n",
    "    fused_tokenizer = Tokenizer(num_words=num_word, oov_token=voo_token)\n",
    "    fused_tokenizer.fit_on_texts(fused_methods)\n",
    "    return semantic_tokenizer, syntatic_tokenizer, fused_tokenizer\n",
    "\n",
    "\n",
    "def get_index(sentence, word_index):\n",
    "    sequence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sequence.append(word_index[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def word2vec(train_vec, verify_vec, test_vec, sentence_words_num):\n",
    "    words_vec_num = 100\n",
    "    vec_all = []\n",
    "    for i in train_vec:\n",
    "        vec_all.append([str(j) for j in i])\n",
    "    for i in verify_vec:\n",
    "        vec_all.append([str(j) for j in i])\n",
    "    for i in test_vec:\n",
    "        vec_all.append([str(j) for j in i])\n",
    "\n",
    "    print_message(\" 构建模型skip-gram\")\n",
    "    model = gensim.models.Word2Vec(vec_all, min_count=1, workers=8, sg=1, vector_size=words_vec_num)\n",
    "    w2v_model = model\n",
    "\n",
    "    for i in range(10):\n",
    "        w2v_model.train(vec_all, total_examples=len(vec_all), epochs=1)\n",
    "\n",
    "    ######\n",
    "    train_vec = convert_vec_word2vec(train_vec, w2v_model, words_vec_num, sentence_words_num)\n",
    "    verify_vec = convert_vec_word2vec(verify_vec, w2v_model, words_vec_num, sentence_words_num)\n",
    "    test_vec = convert_vec_word2vec(test_vec, w2v_model, words_vec_num, sentence_words_num)\n",
    "    #####\n",
    "\n",
    "    return w2v_model, train_vec, verify_vec, test_vec\n",
    "\n",
    "\n",
    "def convert_vec_word2vec(train_vec, w2v_model, words_vec_num, sentence_words_num):\n",
    "    train_vec_new = []\n",
    "    zero_list = []\n",
    "    for i in range(words_vec_num):\n",
    "        zero_list.append(0)\n",
    "    for one_sentence in train_vec:\n",
    "        one_sentence_vec = []\n",
    "        for token in one_sentence:\n",
    "            if len(one_sentence_vec) < sentence_words_num:\n",
    "                embedding = w2v_model.wv[str(token)]\n",
    "                one_sentence_vec.append(embedding)\n",
    "        # padding词矩阵\n",
    "        for i in range(sentence_words_num - len(one_sentence_vec)):\n",
    "            one_sentence_vec.append(zero_list)\n",
    "        train_vec_new.append(one_sentence_vec)\n",
    "    return train_vec_new\n",
    "\n",
    "\n",
    "def print_message(message):\n",
    "    t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(int(time.time())))\n",
    "    print('时间：' + t, end=\" \", flush=True)\n",
    "    print(message, flush=True)\n",
    "\n",
    "\n",
    "def run_one_project(project_name, select_model):\n",
    "    print(project_name, select_model)\n",
    "    semantic_vec_length = 50\n",
    "    syntatic_vec_length = 30\n",
    "    fused_vec_length = 80\n",
    "    seed = 1234  # random.randint(1, 1000)\n",
    "    # 获取每个叶子节点\n",
    "    print_message(\"获取节点\")\n",
    "    leaf = []\n",
    "    lable = []\n",
    "\n",
    "    # 获取数据集\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    sqli = \"select seq,CASE WHEN logNum > 0 THEN  1  ELSE 0 END \" \\\n",
    "           \"from  data_model_2 a where   \" \\\n",
    "           \"methodSeq in \" \\\n",
    "           \"(select seq from data_model_1 \" \\\n",
    "           \"where projectName in(\" + project_name + \")) \"\n",
    "\n",
    "    result = cur.execute(sqli)\n",
    "    for i in range(result):\n",
    "        oneRow = cur.fetchone()\n",
    "        leaf.append(oneRow[0])\n",
    "        lable.append(oneRow[1])\n",
    "    cur.close()\n",
    "    # 打乱数据\n",
    "    print_message('打乱数据')\n",
    "    leaf, lable = shuffle_data(leaf, lable, seed)\n",
    "    # 按比例切分数据集\n",
    "    print_message('按比例切分数据集')\n",
    "    leaf_train, lable_train, leaf_test, lable_test = stratified_shuffle_split(\n",
    "        leaf, lable, 0.4, seed)\n",
    "    leaf_test, lable_test, leaf_verify, lable_verify = stratified_shuffle_split(\n",
    "        leaf_test, lable_test, 0.5, seed)\n",
    "\n",
    "    print_message('获取特征')\n",
    "    # 获取训练集合的特征\n",
    "    fused_train_vec, \\\n",
    "    semantic_train_vec, \\\n",
    "    syntatic_train_vec, \\\n",
    "    train_leaf, \\\n",
    "    train_labels = get_vector(leaf_train, lable_train)\n",
    "    # 获取验证集的特征\n",
    "    fused_verify_vec, \\\n",
    "    semantic_verify_vec, \\\n",
    "    syntatic_verify_vec, \\\n",
    "    verify_leaf, \\\n",
    "    verify_labels = get_vector(leaf_verify, lable_verify)\n",
    "    # 获取测试集的特征\n",
    "    fused_test_vec, \\\n",
    "    semantic_test_vec, \\\n",
    "    syntatic_test_vec, \\\n",
    "    test_leaf, \\\n",
    "    test_labels = get_vector(leaf_test, lable_test)\n",
    "\n",
    "    # tokenizer\n",
    "    print_message('tokenizer')\n",
    "    semantic_tokenizer, syntatic_tokenizer, fused_tokenizer = get_tokenizer()\n",
    "    semantic_train_vec, semantic_verify_vec, semantic_test_vec \\\n",
    "        = tokenizer_vec_use_tokenizer(semantic_tokenizer,\n",
    "                                      semantic_train_vec, semantic_verify_vec, semantic_test_vec)\n",
    "    syntatic_train_vec, syntatic_verify_vec, syntatic_test_vec \\\n",
    "        = tokenizer_vec_use_tokenizer(syntatic_tokenizer,\n",
    "                                      syntatic_train_vec, syntatic_verify_vec, syntatic_test_vec)\n",
    "    fused_train_vec, fused_verify_vec, fused_test_vec \\\n",
    "        = tokenizer_vec_use_tokenizer(fused_tokenizer,\n",
    "                                      fused_train_vec, fused_verify_vec, fused_test_vec)\n",
    "    conn.close()\n",
    "    print_message('word2vec')\n",
    "    word2vec_semantic, semantic_train_vec, semantic_verify_vec, semantic_test_vec = \\\n",
    "        word2vec(semantic_train_vec, semantic_verify_vec, semantic_test_vec, semantic_vec_length)\n",
    "    word2vec_syntatic, syntatic_train_vec, syntatic_verify_vec, syntatic_test_vec = \\\n",
    "        word2vec(syntatic_train_vec, syntatic_verify_vec, syntatic_test_vec, syntatic_vec_length)\n",
    "    word2vec_fused, fused_train_vec, fused_verify_vec, fused_test_vec = \\\n",
    "        word2vec(fused_train_vec, fused_verify_vec, fused_test_vec, fused_vec_length)\n",
    "\n",
    "    # SMOTE上采样\n",
    "#     print_message('SMOTE上采样')\n",
    "\n",
    "#     def combine_dim(list_1):\n",
    "#         list_2 = list_1.reshape(list_1.shape[0], list_1.shape[1] * list_1.shape[2])\n",
    "#         return list_2\n",
    "#     def splite_dim(list_2, y, z):\n",
    "#         return list_2.reshape(list_2.shape[0], y, z)\n",
    "#     from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#     su = SMOTE(random_state=42)\n",
    "\n",
    "\n",
    "#     if select_model == 1:\n",
    "#         fused_train_vec = np.array(fused_train_vec)\n",
    "#         y = fused_train_vec.shape[1]\n",
    "#         z = fused_train_vec.shape[2]\n",
    "#         fused_train_vec = combine_dim(fused_train_vec)\n",
    "#         fused_train_vec, train_labels = \\\n",
    "#             su.fit_resample(fused_train_vec, train_labels)\n",
    "#         fused_train_vec = splite_dim(fused_train_vec, y, z)\n",
    "#     elif select_model == 2:\n",
    "#         semantic_train_vec = np.array(semantic_train_vec)\n",
    "#         y = semantic_train_vec.shape[1]\n",
    "#         z = semantic_train_vec.shape[2]\n",
    "#         semantic_train_vec = combine_dim(semantic_train_vec)\n",
    "#         semantic_train_vec, train_labels = \\\n",
    "#             su.fit_resample(semantic_train_vec, train_labels)\n",
    "#         semantic_train_vec = splite_dim(semantic_train_vec, y, z)\n",
    "#     elif select_model == 4:\n",
    "#         syntatic_train_vec = np.array(syntatic_train_vec)\n",
    "#         y = syntatic_train_vec.shape[1]\n",
    "#         z = syntatic_train_vec.shape[2]\n",
    "#         syntatic_train_vec = combine_dim(syntatic_train_vec)\n",
    "#         syntatic_train_vec, train_labels = \\\n",
    "#             su.fit_resample(syntatic_train_vec, train_labels)\n",
    "#         syntatic_train_vec=splite_dim(syntatic_train_vec,y,z)\n",
    "    from keras.layers import Convolution1D, MaxPooling1D\n",
    "\n",
    "    print_message(\"组建模型\")\n",
    "    inp = None\n",
    "    if select_model == 1:\n",
    "        inp = Input(shape=np.array(fused_train_vec).shape[1:])\n",
    "    elif select_model == 2:\n",
    "        inp = Input(shape=np.array(semantic_train_vec).shape[1:])\n",
    "    elif select_model == 4:\n",
    "        inp = Input(shape=np.array(syntatic_train_vec).shape[1:])\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(inp)\n",
    "#\n",
    "    model.add(tf.keras.layers.LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),  # 'categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    # 训练\n",
    "    print_message(\"训练\")\n",
    "    history = []\n",
    "    lr_reduce = keras.callbacks.ReduceLROnPlateau('val_loss', patience=3, factor=0.5, min_lr=0.000001)\n",
    "    if select_model == 1:\n",
    "        history = model.fit(np.array(fused_train_vec),\n",
    "                            np.array(train_labels),\n",
    "                            validation_data=(np.array(fused_verify_vec),\n",
    "                                             np.array(verify_labels)),\n",
    "                            batch_size=24,\n",
    "                            callbacks=[lr_reduce],\n",
    "                            epochs=num_epoch, verbose=2, shuffle=True)\n",
    "    elif select_model == 2:\n",
    "        history = model.fit(np.array(semantic_train_vec),\n",
    "                            np.array(train_labels),\n",
    "                            validation_data=(np.array(semantic_verify_vec),\n",
    "                                             np.array(verify_labels)),\n",
    "                            batch_size=24,\n",
    "                            callbacks=[lr_reduce],\n",
    "                            epochs=num_epoch, verbose=2, shuffle=True)\n",
    "\n",
    "    elif select_model == 4:\n",
    "        history = model.fit(np.array(syntatic_train_vec), np.array(train_labels),\n",
    "                            validation_data=(np.array(syntatic_verify_vec),\n",
    "                                             np.array(verify_labels)),\n",
    "                            batch_size=24,\n",
    "                            callbacks=[lr_reduce],\n",
    "                            epochs=num_epoch, verbose=2, shuffle=True)\n",
    "\n",
    "    # 单输出\n",
    "    # 输出结果\n",
    "    print_message(\"开始输出结果\")\n",
    "    predict_number = []\n",
    "    if select_model == 1:\n",
    "        predict_one_hot = model.predict(np.array(fused_test_vec))\n",
    "    elif select_model == 2:\n",
    "        predict_one_hot = model.predict(np.array(semantic_test_vec))\n",
    "\n",
    "    elif select_model == 4:\n",
    "        predict_one_hot = model.predict(np.array(syntatic_test_vec))\n",
    "\n",
    "    # predict_number = [np.argmax(one_hot) for one_hot in predict_one_hot]\n",
    "    # verify_number = [np.argmax(one_hot) for one_hot in test_labels]\n",
    "    threshold = 0.5\n",
    "    predict_one_hot = np.int64(predict_one_hot >= threshold)\n",
    "\n",
    "    predict_number = predict_one_hot\n",
    "    verify_number = test_labels\n",
    "\n",
    "    y_true = verify_number\n",
    "    y_pred = predict_number\n",
    "\n",
    "    balance_accuracy_ase_all, precision_ase_all, recall_ase_all, f1_value_ase_all = plot_value(y_true, y_pred)\n",
    "    feature_type = \"\"\n",
    "    # 1-fused 2-语义\n",
    "    # 4-语法（新）\n",
    "    if select_model == 1:\n",
    "        feature_type = \"fused\"\n",
    "    elif select_model == 2:\n",
    "        feature_type = \"semantic\"\n",
    "    elif select_model == 4:\n",
    "        feature_type = \"syntatic\"\n",
    "    # plot_graphs(history, 'accuracy')\n",
    "    # plot_graphs(history, 'loss')\n",
    "    print_message(\"结束\")\n",
    "    save_performance('ASE'+feature_type, balance_accuracy_ase_all, precision_ase_all, recall_ase_all, f1_value_ase_all)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    project_name = \"'\"+one_project+\"'\"\n",
    "#     project_name = \"'zookeeper-master'\"\n",
    "    # 1-fused 2-语义\n",
    "    # 4-语法（新）\n",
    "    model_type = [1]#,2,4\n",
    "\n",
    "    for select_model in model_type:\n",
    "        run_one_project(project_name, select_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe50db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100  # 训练周期\n",
    "feature_length = \"10\"\n",
    "one_project = 'zeppelin-master'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f1907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
